{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import *\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import tensorflow.keras.backend as K\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.data_process import parse_function, load_data\n",
    "# from losses.face_losses import arcface_loss\n",
    "# from nets.MobileFaceNet import inference\n",
    "from verification import evaluate\n",
    "from scipy.optimize import brentq\n",
    "from scipy import interpolate\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dropout,PReLU,Layer\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization, add, Reshape,DepthwiseConv2D\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import math\n",
    "\n",
    "weight_decay = 5e-5  # l2正则化decay常量\n",
    "\n",
    "\n",
    "batch_norm_params = {\n",
    "    'center': True,\n",
    "    'scale': True,\n",
    "    'momentum': 0.995,\n",
    "    'epsilon': 2e-5,\n",
    "}\n",
    "\n",
    "MODEL_FILE = 'MobileFaceNet.h5'\n",
    "LITE_FILE  = 'MobileFaceNet.tflite'\n",
    "\n",
    "NUM_PICTURES=490623\n",
    "NUM_CLASSES=10572\n",
    "BATCH_SIZE=90\n",
    "TARGET_SIZE=(112,112)\n",
    "TFRECORD_PATH='/workspace/dataset/faces_webface_112x112/tfrecords/tran.tfrecords'\n",
    "\n",
    "train_batch_size = 90\n",
    "test_batch_size = 100\n",
    "eval_datasets = ['lfw']\n",
    "eval_db_path = '/workspace/dataset/faces_webface_112x112/'\n",
    "eval_nrof_folds = 10\n",
    "tfrecords_file_path = '/workspace/dataset/faces_webface_112x112/tfrecords/'\n",
    "summary_path = '/workspace/output/summary'\n",
    "ckpt_path = '/workspace/output/ckpt'\n",
    "pretrained_model = False\n",
    "log_file_path = '/workspace/output/logs'\n",
    "ckpt_best_path = '/workspace/output/ckpt_best'\n",
    "saver_maxkeep = 50\n",
    "summary_interval = 400\n",
    "ckpt_interval = 200\n",
    "validate_interval = 500\n",
    "show_info_interval = 50\n",
    "log_device_mapping = False\n",
    "log_histograms = False\n",
    "prelogits_norm_loss_factor = 2e-5\n",
    "prelogits_norm_p = 1.0\n",
    "max_epoch = 12\n",
    "image_size = [112, 112]\n",
    "embedding_size = 128\n",
    "\n",
    "# prepare validate datasets\n",
    "# ver_list = []\n",
    "# ver_name_list = []\n",
    "# for db in eval_datasets:\n",
    "#     print('begin db %s convert.' % db)\n",
    "#     data_set = load_data(db, image_size, eval_db_path)\n",
    "#     ver_list.append(data_set)\n",
    "#     ver_name_list.append(db)\n",
    "\n",
    "# # output file path\n",
    "# if not os.path.exists(log_file_path):\n",
    "#     os.makedirs(log_file_path)\n",
    "# if not os.path.exists(ckpt_best_path):\n",
    "#     os.makedirs(ckpt_best_path)\n",
    "# if not os.path.exists(ckpt_path):\n",
    "#     os.makedirs(ckpt_path)\n",
    "# # create log dir\n",
    "# subdir = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "# log_dir = os.path.join(os.path.expanduser(log_file_path), subdir)\n",
    "# if not os.path.isdir(log_dir):  # Create the log directory if it doesn't exist\n",
    "#     os.makedirs(log_dir)\n",
    "\n",
    "\n",
    "def my_generator(tfrecord_path=TFRECORD_PATH,batch_size=BATCH_SIZE,out_num=NUM_CLASSES):\n",
    "    \"\"\"自定义generator\n",
    "    \n",
    "    # Argument\n",
    "        tfrecord_path:\n",
    "        batch_size\n",
    "        out_num: 类别数量，用于生成onehot\n",
    "        \n",
    "    # Return\n",
    "    \n",
    "    \"\"\"\n",
    "    def parse_function(example_proto):\n",
    "        features = {'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                    'label': tf.FixedLenFeature([], tf.int64)}\n",
    "        features = tf.parse_single_example(example_proto, features)\n",
    "        # You can do more image distortion here for training data\n",
    "        img = tf.image.decode_jpeg(features['image_raw'])\n",
    "        img = tf.reshape(img, shape=(112, 112, 3))\n",
    "\n",
    "        #img = tf.py_func(random_rotate_image, [img], tf.uint8)\n",
    "        img = tf.cast(img, dtype=tf.float32)\n",
    "        img = tf.subtract(img, 127.5)\n",
    "        img = tf.multiply(img,  0.0078125)\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        label = tf.cast(features['label'], tf.int64)\n",
    "#         label = tf.one_hot(label,out_num)\n",
    "#         label = tf.reshape(label,(-1,))\n",
    "#         one_hot = tf.one_hot(label,out_num)\n",
    "        return (img, label)\n",
    "    \n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess = tf.Session(config=config)\n",
    "#     sess = K.get_session()\n",
    "    # training datasets api config\n",
    "    tfrecords_f = os.path.join(tfrecord_path)\n",
    "    dataset = tf.data.TFRecordDataset(tfrecords_f)\n",
    "    dataset = dataset.map(parse_function)\n",
    "#     dataset = dataset.shuffle(buffer_size=5000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    return iterator, next_element, sess\n",
    "    # begin iteration\n",
    "#     while(True):\n",
    "#         sess.run(iterator.initializer)\n",
    "#         while True:\n",
    "#             try:\n",
    "#                 images, labels = sess.run(next_element)\n",
    "# #                 for i in range(len(images)):\n",
    "# #                     images[i,...] = cv2.cvtColor(images[i, ...], cv2.COLOR_RGB2BGR)\n",
    "#                 yield images,labels\n",
    "#             except tf.errors.OutOfRangeError:\n",
    "# #                 print(\"End of dataset\")\n",
    "#                 break\n",
    "\n",
    "\n",
    "def my_generator_wrapper():\n",
    "    for image,label in my_generator():\n",
    "        yield ([image,label],label)\n",
    "#         yield image,label\n",
    "        \n",
    "def test_generator():\n",
    "    a = my_generator_wrapper()\n",
    "    for i in tqdm(range(50000)):\n",
    "        a.__next__()\n",
    "        \n",
    "def learning_rate_schedule(epoch, lr, boundaries, values):\n",
    "    \"\"\"\n",
    "    # Argument:\n",
    "        epoch: now epoch\n",
    "        lr: learning rate to schedule\n",
    "        boundaries: Number of epochs for learning rate piecewise.\n",
    "        values: target value of learning rate\n",
    "    \"\"\"\n",
    "    if epoch <= boundaries[0]:\n",
    "        t = values[0]\n",
    "    for low, high, v in zip(boundaries[:-1],boundaries[1:],values[1:]):\n",
    "        if low < epoch <= high:\n",
    "            t = v\n",
    "    if epoch > boundaries[-1]:\n",
    "        t = values[-1]\n",
    "        \n",
    "    K.get_session().run(lr.assign(t))\n",
    "    return epoch,t\n",
    "\n",
    "class ExponentialMovingAverage:\n",
    "    \"\"\"对模型权重进行指数滑动平均。\n",
    "    用法：在model.compile之后、第一次训练之前使用；\n",
    "    先初始化对象，然后执行inject方法。\n",
    "    \"\"\"\n",
    "    def __init__(self, model, momentum=0.9999):\n",
    "        self.momentum = momentum\n",
    "        self.model = model\n",
    "        self.ema_weights = [K.zeros(K.shape(w)) for w in model.weights]\n",
    "    def inject(self):\n",
    "        \"\"\"添加更新算子到model.metrics_updates。\n",
    "        \"\"\"\n",
    "        self.initialize()\n",
    "        for w1, w2 in zip(self.ema_weights, self.model.weights):\n",
    "            op = K.moving_average_update(w1, w2, self.momentum)\n",
    "            self.model.metrics_updates.append(op)\n",
    "    def initialize(self):\n",
    "        \"\"\"ema_weights初始化跟原模型初始化一致。\n",
    "        \"\"\"\n",
    "        self.old_weights = K.batch_get_value(self.model.weights)\n",
    "        K.batch_set_value(zip(self.ema_weights, self.old_weights))\n",
    "    def apply_ema_weights(self):\n",
    "        \"\"\"备份原模型权重，然后将平均权重应用到模型上去。\n",
    "        \"\"\"\n",
    "        self.old_weights = K.batch_get_value(self.model.weights)\n",
    "        ema_weights = K.batch_get_value(self.ema_weights)\n",
    "        K.batch_set_value(zip(self.model.weights, ema_weights))\n",
    "    def reset_old_weights(self):\n",
    "        \"\"\"恢复模型到旧权重。\n",
    "        \"\"\"\n",
    "        K.batch_set_value(zip(self.model.weights, self.old_weights))\n",
    "\n",
    "\n",
    "\n",
    "def flow_wrapper(flow):\n",
    "    \"\"\"自定义wrapper，将(x,y)变成([x,y],y)\"\"\"\n",
    "    while True:\n",
    "        x,y = flow.next()\n",
    "        yield ([x,y],y)\n",
    "\n",
    "def prelu(input, name=''):\n",
    "    \"\"\"自定义prelu\"\"\"\n",
    "    alphas = K.variable(K.constant(0.25,dtype=tf.float32,shape=[input.get_shape()[-1]]),name=name + 'prelu_alphas')\n",
    "    pos = K.relu(input)\n",
    "    neg = alphas * (input - K.abs(input)) * 0.5\n",
    "    return pos + neg\n",
    "\n",
    "cval = Constant(0.25)  # prelu α 初始常量\n",
    "\n",
    "def _conv_block(inputs, filters, kernel, strides):\n",
    "    \"\"\"Convolution Block\n",
    "    This function defines a 2D convolution operation with BN and relu6.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        strides: An integer or tuple/list of 2 integers,\n",
    "            specifying the strides of the convolution along the width and height.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    x = Conv2D(filters, kernel, padding='same', strides=strides, kernel_initializer='glorot_normal',kernel_regularizer=l2(weight_decay))(inputs)\n",
    "    x = BatchNormalization(axis=channel_axis,**batch_norm_params)(x)\n",
    "    x = PReLU(cval)(x)\n",
    "#     x = Activation(relu)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _bottleneck(inputs, filters, kernel, t, s, r=False):\n",
    "    \"\"\"Bottleneck\n",
    "    This function defines a basic bottleneck structure.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        r: Boolean, Whether to use the residuals.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    tchannel = K.int_shape(inputs)[channel_axis] * t\n",
    "\n",
    "    x = _conv_block(inputs, tchannel, (1, 1), (1, 1))\n",
    "\n",
    "    x = DepthwiseConv2D(kernel, strides=(s, s), depth_multiplier=1, padding='same', kernel_initializer='glorot_normal')(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = PReLU(cval)(x)\n",
    "#     x = Activation(relu)(x)\n",
    "\n",
    "    x = Conv2D(filters, (1, 1), strides=(1, 1), padding='same', kernel_initializer='glorot_normal',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = BatchNormalization(axis=channel_axis,**batch_norm_params)(x)\n",
    "\n",
    "    if r:\n",
    "        x = add([x, inputs])\n",
    "    return x\n",
    "\n",
    "\n",
    "def _inverted_residual_block(inputs, filters, kernel, t, strides, n):\n",
    "    \"\"\"Inverted Residual Block\n",
    "    This function defines a sequence of 1 or more identical layers.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        n: Integer, layer repeat times.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    x = _bottleneck(inputs, filters, kernel, t, strides)\n",
    "\n",
    "    for i in range(1, n):\n",
    "        x = _bottleneck(x, filters, kernel, t, 1, True)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class ArcFace(Layer):\n",
    "    \"\"\"改进的softmax，得出的结果再与真是结果之间求交叉熵\"\"\"\n",
    "    def __init__(self, n_classes=10, s=64.0, m=0.50, regularizer=None, **kwargs):\n",
    "        super(ArcFace, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print(input_shape)\n",
    "        print('build')\n",
    "        super(ArcFace, self).build(input_shape[0])\n",
    "        print('build2')\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                shape=(input_shape[0][-1].value, self.n_classes),\n",
    "                                initializer='glorot_uniform',\n",
    "                                trainable=True,\n",
    "                                regularizer=self.regularizer)\n",
    "        print('build3')\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x, y = inputs # x为embeddings，y为labels\n",
    "#         c = K.shape(x)[-1]  # 特征维度\n",
    "#         # 1. normalize feature\n",
    "#         x = tf.nn.l2_normalize(x, axis=1)\n",
    "#         # 2. normalize weights\n",
    "#         W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "#         # dot product\n",
    "#         # 全连接层，x的结构为（None，128）w的结构为（128，n_classes）。logits的结构为(None,n_classes)\n",
    "#         # (np.random.randn(5,128) @ np.random.randn(128,10)).shape # (5, 10)\n",
    "#         # 3. 计算xW得到预测向量y\n",
    "#         logits = x @ W\n",
    "#         # add margin\n",
    "#         # clip logits to prevent zero division when backward\n",
    "#         theta = tf.acos(K.clip(logits, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n",
    "#         target_logits = tf.cos(theta + self.m)\n",
    "#         # sin = tf.sqrt(1 - logits**2)\n",
    "#         # cos_m = tf.cos(logits)\n",
    "#         # sin_m = tf.sin(logits)\n",
    "#         # target_logits = logits * cos_m - sin * sin_m\n",
    "#         logits = logits * (1 - y) + target_logits * y\n",
    "#         # feature re-scale\n",
    "#         # 9. 对所有值乘上固定值s\n",
    "#         logits *= self.s\n",
    "#         out = tf.nn.softmax(logits)\n",
    "#         print(out)\n",
    "#         return out\n",
    "\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        embedding, labels = inputs\n",
    "        labels = tf.reshape(labels,shape=(-1,))\n",
    "        print('labels:',labels)\n",
    "        \n",
    "        out_num = self.n_classes\n",
    "        w_init=None\n",
    "        s=64.\n",
    "        m=0.5\n",
    "        \n",
    "        cos_m = tf.cos(m)\n",
    "        sin_m = tf.sin(m)\n",
    "        mm = sin_m * m  # issue 1\n",
    "        threshold = tf.cos(math.pi - m)\n",
    "        with tf.compat.v1.variable_scope('arcface_loss'):\n",
    "            # inputs and weights norm\n",
    "            embedding_norm = tf.norm(tensor=embedding, axis=1, keepdims=True)\n",
    "            embedding = tf.compat.v1.div(embedding, embedding_norm, name='norm_embedding')\n",
    "#             weights = tf.get_variable(name='embedding_weights', shape=(embedding.get_shape().as_list()[-1], out_num),\n",
    "#                                       initializer=w_init, dtype=tf.float32)\n",
    "            weights = self.W\n",
    "            weights_norm = tf.norm(tensor=weights, axis=0, keepdims=True)\n",
    "            weights = tf.compat.v1.div(weights, weights_norm, name='norm_weights')\n",
    "            # cos(theta+m)\n",
    "            cos_t = tf.matmul(embedding, weights, name='cos_t')\n",
    "            cos_t2 = tf.square(cos_t, name='cos_2')\n",
    "            sin_t2 = tf.subtract(1., cos_t2, name='sin_2')\n",
    "            sin_t = tf.sqrt(sin_t2, name='sin_t')\n",
    "            cos_mt = s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')\n",
    "\n",
    "            # this condition controls the theta+m should in range [0, pi]\n",
    "            #      0<=theta+m<=pi\n",
    "            #     -m<=theta<=pi-m\n",
    "            cond_v = cos_t - threshold\n",
    "            cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)\n",
    "\n",
    "            keep_val = s*(cos_t - mm)\n",
    "            cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n",
    "            print('labels:',labels,'out_num',out_num)\n",
    "            mask = tf.one_hot(labels, depth=out_num, name='one_hot_mask')\n",
    "#             mask = tf.reshape(mask,(-1,))\n",
    "#             mask = labels\n",
    "            print(mask)\n",
    "            # mask = tf.squeeze(mask, 1)\n",
    "            inv_mask = tf.subtract(1., mask, name='inverse_mask')\n",
    "            print(inv_mask)\n",
    "            s_cos_t = tf.multiply(s, cos_t, name='scalar_cos_t')\n",
    "            print(s_cos_t)\n",
    "            logit = tf.add(tf.multiply(s_cos_t, inv_mask), tf.multiply(cos_mt_temp, mask), name='arcface_loss_output')\n",
    "            print(logit)\n",
    "#             inference_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=labels))\n",
    "            inference_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=labels)\n",
    "            print(inference_loss)\n",
    "#             inference_loss = tf.nn.softmax(logit)\n",
    "            print(inference_loss)\n",
    "        return inference_loss\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.n_classes)\n",
    "\n",
    "def MobileFaceNets(input_shape=(112,112,3), n_classes=10, k=128):\n",
    "    \"\"\"MobileFaceNets\"\"\"\n",
    "    inputs = Input(shape=input_shape) #112x112，(img-127.5)/255\n",
    "    y      = Input(shape=(1,), dtype=tf.int32)\n",
    "#     y      = Input(shape=(n_classes,))\n",
    "    x = _conv_block(inputs, 64, (3, 3), strides=(2, 2))\n",
    "    \n",
    "    # depthwise conv3x3\n",
    "    x = DepthwiseConv2D(3, strides=(1, 1), depth_multiplier=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = PReLU(cval)(x)\n",
    "#     x = Activation(relu)(x)\n",
    "    \n",
    "    # 5层bottleneck\n",
    "    x = _inverted_residual_block(x, 64, (3, 3), t=2, strides=2, n=5)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=4, strides=2, n=1)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=2, strides=1, n=6)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=4, strides=2, n=1)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=2, strides=1, n=2)\n",
    "    \n",
    "    # conv1x1\n",
    "    x = _conv_block(x, 512, (1, 1), strides=(1, 1))\n",
    "    \n",
    "    # linear GDConv7x7\n",
    "    x = DepthwiseConv2D(7, strides=(1, 1), depth_multiplier=1, padding='valid')(x)\n",
    "#     x = Dropout(0.3, name='Dropout')(x)\n",
    "    \n",
    "    x = Conv2D(k, (1, 1), padding='same',kernel_initializer='glorot_normal',kernel_regularizer=l2(1e-10))(x)\n",
    "    \n",
    "    \n",
    "#     x = Activation(keras.activations.re)\n",
    "    x = Reshape((k,))(x)\n",
    "    print(x)\n",
    "#     x = keras.layers.Lambda(lambda o: K.l2_normalize(o, axis=1))(x)\n",
    "    epsilon=1e-10\n",
    "    x = keras.layers.Lambda(lambda o: o/K.sqrt(K.maximum(K.sum(K.square(o),axis=1,keepdims=True),epsilon)))(x)\n",
    "#     x = tf.nn.l2_normalize(x, 1, 1e-10, name='embeddings')\n",
    "    \n",
    "    # x 为embeddings， y为embeddings对应的类别标签，output为\n",
    "    output = ArcFace(n_classes=n_classes, regularizer=None)([x, y])\n",
    "    \n",
    "    model = Model([inputs, y], output)\n",
    "#     plot_model(model, to_file='images/MobileNetv2.png', show_shapes=True)\n",
    "    print(model.input,model.output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"reshape/Reshape:0\", shape=(?, 128), dtype=float32)\n",
      "[TensorShape([Dimension(None), Dimension(128)]), TensorShape([Dimension(None), Dimension(1)])]\n",
      "build\n",
      "build2\n",
      "build3\n",
      "labels: Tensor(\"arc_face/Reshape:0\", shape=(?,), dtype=int32)\n",
      "WARNING:tensorflow:From <ipython-input-1-107d759353a0>:391: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "labels: Tensor(\"arc_face/Reshape:0\", shape=(?,), dtype=int32) out_num 10572\n",
      "Tensor(\"arc_face/arcface_loss/one_hot_mask:0\", shape=(?, 10572), dtype=float32)\n",
      "Tensor(\"arc_face/arcface_loss/inverse_mask:0\", shape=(?, 10572), dtype=float32)\n",
      "Tensor(\"arc_face/arcface_loss/scalar_cos_t:0\", shape=(?, 10572), dtype=float32)\n",
      "Tensor(\"arc_face/arcface_loss/arcface_loss_output:0\", shape=(?, 10572), dtype=float32)\n",
      "Tensor(\"arc_face/arcface_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"arc_face/arcface_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(?,), dtype=float32)\n",
      "[<tf.Tensor 'input_1:0' shape=(?, 112, 112, 3) dtype=float32>, <tf.Tensor 'input_2:0' shape=(?, 1) dtype=int32>] Tensor(\"arc_face/arcface_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(?,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def my_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = MobileFaceNets(n_classes=NUM_CLASSES)\n",
    "model.compile(optimizer=keras.optimizers.Adam(0.1,beta_1=0.9, beta_2=0.999, epsilon=0.1),\n",
    "      loss=my_loss,\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "# EMAer = ExponentialMovingAverage(model) # 在模型compile之后执行\n",
    "# EMAer.inject() # 在模型compile之后执行\n",
    "\n",
    "# model.compile(optimizer=keras.optimizers.sgd(0.1),loss=my_loss,metrics=['accuracy'])\n",
    "val_model = keras.models.Model(inputs=model.inputs[0], outputs=model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin db lfw convert.\n",
      "loading bin 1000\n",
      "loading bin 2000\n",
      "loading bin 3000\n",
      "loading bin 4000\n",
      "loading bin 5000\n",
      "loading bin 6000\n",
      "loading bin 7000\n",
      "loading bin 8000\n",
      "loading bin 9000\n",
      "loading bin 10000\n",
      "loading bin 11000\n",
      "loading bin 12000\n",
      "(12000, 112, 112, 3)\n",
      "epoch:0, lr:0.1\n",
      "epoch 0, total_step 50, loss is 49.399567, training accuracy is 0.000000, time 317.344 samples/sec\n",
      "epoch 0, total_step 100, loss is 81.460213, training accuracy is 0.000000, time 318.396 samples/sec\n",
      "epoch 0, total_step 150, loss is 68.214470, training accuracy is 0.000000, time 309.112 samples/sec\n",
      "epoch 0, total_step 200, loss is 55.611565, training accuracy is 0.000000, time 321.289 samples/sec\n",
      "epoch 0, total_step 250, loss is 47.203197, training accuracy is 0.000000, time 311.942 samples/sec\n",
      "epoch 0, total_step 300, loss is 43.594082, training accuracy is 0.000000, time 304.533 samples/sec\n",
      "epoch 0, total_step 350, loss is 40.223469, training accuracy is 0.000000, time 321.184 samples/sec\n",
      "epoch 0, total_step 400, loss is 35.631260, training accuracy is 0.000000, time 304.071 samples/sec\n",
      "epoch 0, total_step 450, loss is 35.980431, training accuracy is 0.000000, time 320.247 samples/sec\n",
      "epoch 0, total_step 500, loss is 28.529507, training accuracy is 0.000000, time 318.635 samples/sec\n",
      "\n",
      "Iteration 500 testing...\n",
      "thresholds max: 0 <=> min: 0.0\n",
      "total time 16.599s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.500+-0.000\n",
      "Validation rate: 1.00000+-0.00000 @ FAR=1.00000\n",
      "fpr and tpr: 0.998 0.998\n",
      "Area Under Curve (AUC): 0.500\n",
      "epoch 0, total_step 550, loss is 28.056482, training accuracy is 0.000000, time 311.551 samples/sec\n"
     ]
    }
   ],
   "source": [
    "train_batch_size = 90\n",
    "test_batch_size = 100\n",
    "eval_datasets = ['lfw']\n",
    "eval_db_path = '/workspace/dataset/faces_webface_112x112/'\n",
    "eval_nrof_folds = 10\n",
    "tfrecords_file_path = '/workspace/dataset/faces_webface_112x112/tfrecords/'\n",
    "summary_path = '/workspace/output/summary'\n",
    "ckpt_path = '/workspace/output/ckpt'\n",
    "pretrained_model = False\n",
    "log_file_path = '/workspace/output/logs'\n",
    "ckpt_best_path = '/workspace/output/ckpt_best'\n",
    "saver_maxkeep = 50\n",
    "summary_interval = 400\n",
    "ckpt_interval = 200\n",
    "validate_interval = 500\n",
    "show_info_interval = 50\n",
    "log_device_mapping = False\n",
    "log_histograms = False\n",
    "prelogits_norm_loss_factor = 2e-5\n",
    "prelogits_norm_p = 1.0\n",
    "max_epoch = 12\n",
    "image_size = [112, 112]\n",
    "embedding_size = 128\n",
    "\n",
    "lr_schedule = [4, 7, 9, 11]\n",
    "values=[0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "# prepare validate datasets\n",
    "ver_list = []\n",
    "ver_name_list = []\n",
    "for db in eval_datasets:\n",
    "    print('begin db %s convert.' % db)\n",
    "    data_set = load_data(db, image_size, eval_db_path)\n",
    "    ver_list.append(data_set)\n",
    "    ver_name_list.append(db)\n",
    "\n",
    "# output file path\n",
    "if not os.path.exists(log_file_path):\n",
    "    os.makedirs(log_file_path)\n",
    "if not os.path.exists(ckpt_best_path):\n",
    "    os.makedirs(ckpt_best_path)\n",
    "if not os.path.exists(ckpt_path):\n",
    "    os.makedirs(ckpt_path)\n",
    "# create log dir\n",
    "subdir = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "log_dir = os.path.join(os.path.expanduser(log_file_path), subdir)\n",
    "if not os.path.isdir(log_dir):  # Create the log directory if it doesn't exist\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "# g = my_generator_wrapper()\n",
    "iterator, next_element, g_sess = my_generator()\n",
    "\n",
    "# epoch = -1\n",
    "count = 0\n",
    "total_accuracy = {}\n",
    "for i in range(max_epoch):\n",
    "    # 调整学习率\n",
    "    _, lr = learning_rate_schedule(i,model.optimizer.lr,lr_schedule,values)\n",
    "    print('epoch:{}, lr:{}'.format(i, lr))\n",
    "    # 初始化迭代器\n",
    "    g_sess.run(iterator.initializer)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            x, y = g_sess.run(next_element)\n",
    "            \n",
    "            images_train,labels_train = ([x,y],y)\n",
    "\n",
    "            start = time.time()\n",
    "#             _, total_loss_val, inference_loss_val, reg_loss_val, _, acc_val = \\\n",
    "#             sess.run([train_op, total_loss, inference_loss, regularization_losses, inc_global_step_op, Accuracy_Op],\n",
    "#                      feed_dict=feed_dict)\n",
    "            loss, accuracy = model.train_on_batch(images_train, labels_train)\n",
    "    \n",
    "            end = time.time()\n",
    "            pre_sec = train_batch_size/(end - start)\n",
    "\n",
    "            count += 1\n",
    "            # print training information\n",
    "#             print(i, count, loss, accuracy, pre_sec)\n",
    "            if count > 0 and count % show_info_interval == 0:\n",
    "#                 print('epoch %d, total_step %d, total loss is %.2f , inference loss is %.2f, reg_loss is %.2f, training accuracy is %.6f, time %.3f samples/sec' %\n",
    "#                       (i, count, total_loss_val, inference_loss_val, np.sum(reg_loss_val), acc_val, pre_sec))\n",
    "                print('epoch %d, total_step %d, loss is %.6f, training accuracy is %.6f, time %.3f samples/sec' %\n",
    "                      (i, count, loss, np.mean(accuracy), pre_sec))\n",
    "\n",
    "            # save summary\n",
    "#             if count > 0 and count % summary_interval == 0:\n",
    "#                 feed_dict = {inputs: images_train, labels: labels_train, phase_train_placeholder: True}\n",
    "#                 summary_op_val = sess.run(summary_op, feed_dict=feed_dict)\n",
    "#                 summary.add_summary(summary_op_val, count)\n",
    "\n",
    "            # save ckpt files\n",
    "            if count > 0 and count % ckpt_interval == 0:\n",
    "#                 filename = 'MobileFaceNet_iter_{:d}'.format(count) + '.ckpt'\n",
    "#                 EMAer.apply_ema_weights()\n",
    "                filename = 'MobileFaceNet_iter_{:d}'.format(count) + '.h5'\n",
    "                filename = os.path.join(ckpt_path, filename)\n",
    "                val_model.save(filename)\n",
    "                \n",
    "#                 EMAer.reset_old_weights()\n",
    "\n",
    "            # validate\n",
    "            if count > 0 and count % validate_interval == 0:\n",
    "                print('\\nIteration', count, 'testing...')\n",
    "                \n",
    "#                 EMAer.apply_ema_weights()\n",
    "                \n",
    "                for db_index in range(len(ver_list)):\n",
    "                    start_time = time.time()\n",
    "                    data_sets, issame_list = ver_list[db_index]\n",
    "                    emb_array = np.zeros((data_sets.shape[0], embedding_size))\n",
    "                    nrof_batches = data_sets.shape[0] // test_batch_size\n",
    "                    for index in range(nrof_batches): # actual is same multiply 2, test data total\n",
    "                        start_index = index * test_batch_size\n",
    "                        end_index = min((index + 1) * test_batch_size, data_sets.shape[0])\n",
    "\n",
    "#                         feed_dict = {inputs: data_sets[start_index:end_index, ...], phase_train_placeholder: False}\n",
    "#                         emb_array[start_index:end_index, :] = sess.run(embeddings, feed_dict=feed_dict)\n",
    "                        emb_array[start_index:end_index, :] = val_model.predict(data_sets[start_index:end_index, ...])\n",
    "\n",
    "                    tpr, fpr, accuracy, val, val_std, far = evaluate(emb_array, issame_list, nrof_folds=eval_nrof_folds)\n",
    "                    duration = time.time() - start_time\n",
    "\n",
    "                    print(\"total time %.3fs to evaluate %d images of %s\" % (duration, data_sets.shape[0], ver_name_list[db_index]))\n",
    "                    print('Accuracy: %1.3f+-%1.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "                    print('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far))\n",
    "                    print('fpr and tpr: %1.3f %1.3f' % (np.mean(fpr, 0), np.mean(tpr, 0)))\n",
    "\n",
    "                    auc = metrics.auc(fpr, tpr)\n",
    "                    print('Area Under Curve (AUC): %1.3f' % auc)\n",
    "#                     eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\n",
    "#                     print('Equal Error Rate (EER): %1.3f\\n' % eer)\n",
    "\n",
    "                    with open(os.path.join(log_dir, '{}_result.txt'.format(ver_name_list[db_index])), 'at') as f:\n",
    "                        f.write('%d\\t%.5f\\t%.5f\\n' % (count, np.mean(accuracy), val))\n",
    "\n",
    "                    if ver_name_list == 'lfw' and np.mean(accuracy) > 0.992:\n",
    "                        print('best accuracy is %.5f' % np.mean(accuracy))\n",
    "                        filename = 'MobileFaceNet_iter_best_{:d}'.format(count) + '.ckpt'\n",
    "                        filename = os.path.join(ckpt_best_path, filename)\n",
    "                        saver.save(sess, filename)\n",
    "                        \n",
    "#                 EMAer.reset_old_weights()\n",
    "                \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of epoch %d\" % i)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png',show_shapes=True)\n",
    "plot_model(val_model, to_file='val_model.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_model.save(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_model = keras.models.Model(inputs=model.inputs[0], outputs=model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_model = keras.models.load_model(MODEL_FILE)\n",
    "img = cv2.imread('/home/cmf/workspace/Anthony_Hopkins_0001.jpg')\n",
    "img = cv2.resize(img,(112,112))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = np.asarray(img)\n",
    "img = np.reshape(img,(1,112,112,3))\n",
    "img = (img-127.5)/128\n",
    "val_model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(MODEL_FILE)\n",
    "tflite_model = converter.convert()\n",
    "open(LITE_FILE, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=LITE_FILE)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print('INPUTS: ')\n",
    "print(input_details)\n",
    "print('OUTPUTS: ')\n",
    "print(output_details)\n",
    "\n",
    "# Test model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=input_details[0]['dtype'])\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "print('output:')\n",
    "print(output_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
