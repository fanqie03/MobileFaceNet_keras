{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"MobileFaceNet/reshape_1/Reshape:0\", shape=(?, 128), dtype=float32)\n",
      "labels: Tensor(\"arc_face_1/Reshape:0\", shape=(?,), dtype=int32)\n",
      "WARNING:tensorflow:From <ipython-input-1-a1ea16e26b1a>:391: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "labels: Tensor(\"arc_face_1/Reshape:0\", shape=(?,), dtype=int32) out_num 10572\n",
      "Tensor(\"arc_face_1/arcface_loss/one_hot_mask:0\", shape=(?, 10572), dtype=float32)\n",
      "Tensor(\"arc_face_1/arcface_loss/inverse_mask:0\", shape=(?, 10572), dtype=float32)\n",
      "Tensor(\"arc_face_1/arcface_loss/scalar_cos_t:0\", shape=(?, 10572), dtype=float32)\n",
      "Tensor(\"arc_face_1/arcface_loss/arcface_loss_output:0\", shape=(?, 10572), dtype=float32)\n",
      "Tensor(\"arc_face_1/arcface_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"arc_face_1/arcface_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(?,), dtype=float32)\n",
      "[<tf.Tensor 'MobileFaceNet/input_1:0' shape=(?, 112, 112, 3) dtype=float32>, <tf.Tensor 'MobileFaceNet/input_2:0' shape=(?, 1) dtype=int32>] Tensor(\"arc_face_1/arcface_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(?,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']=\"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import keras as keras\n",
    "from keras.callbacks import *\n",
    "\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import keras.backend as K\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.data_process import parse_function, load_data\n",
    "# from losses.face_losses import arcface_loss\n",
    "# from nets.MobileFaceNet import inference\n",
    "from verification import evaluate\n",
    "from scipy.optimize import brentq\n",
    "from scipy import interpolate\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dropout,PReLU,Layer\n",
    "from keras.layers import Activation, BatchNormalization, add, Reshape,DepthwiseConv2D\n",
    "from keras.utils import plot_model\n",
    "from keras.activations import relu\n",
    "from keras.initializers import Constant\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras import backend as K\n",
    "import math\n",
    "\n",
    "weight_decay = 5e-5  # l2正则化decay常量\n",
    "\n",
    "\n",
    "batch_norm_params = {\n",
    "    'center': True,\n",
    "    'scale': True,\n",
    "    'momentum': 0.995,\n",
    "    'epsilon': 2e-5,\n",
    "}\n",
    "\n",
    "MODEL_FILE = 'MobileFaceNet.h5'\n",
    "LITE_FILE  = 'MobileFaceNet.tflite'\n",
    "\n",
    "NUM_PICTURES=490623\n",
    "NUM_CLASSES=10572\n",
    "BATCH_SIZE=90\n",
    "TARGET_SIZE=(112,112)\n",
    "TFRECORD_PATH='/workspace/dataset/faces_webface_112x112/tfrecords/tran.tfrecords'\n",
    "\n",
    "train_batch_size = 90\n",
    "test_batch_size = 100\n",
    "eval_datasets = ['lfw']\n",
    "eval_db_path = '/workspace/dataset/faces_webface_112x112/'\n",
    "eval_nrof_folds = 10\n",
    "tfrecords_file_path = '/workspace/dataset/faces_webface_112x112/tfrecords/'\n",
    "summary_path = '/workspace/output/summary'\n",
    "ckpt_path = '/workspace/output/ckpt'\n",
    "pretrained_model = False\n",
    "log_file_path = '/workspace/output/logs'\n",
    "ckpt_best_path = '/workspace/output/ckpt_best'\n",
    "saver_maxkeep = 50\n",
    "summary_interval = 400\n",
    "ckpt_interval = 200\n",
    "validate_interval = 500\n",
    "show_info_interval = 50\n",
    "log_device_mapping = False\n",
    "log_histograms = False\n",
    "prelogits_norm_loss_factor = 2e-5\n",
    "prelogits_norm_p = 1.0\n",
    "max_epoch = 12\n",
    "image_size = [112, 112]\n",
    "embedding_size = 128\n",
    "\n",
    "# prepare validate datasets\n",
    "# ver_list = []\n",
    "# ver_name_list = []\n",
    "# for db in eval_datasets:\n",
    "#     print('begin db %s convert.' % db)\n",
    "#     data_set = load_data(db, image_size, eval_db_path)\n",
    "#     ver_list.append(data_set)\n",
    "#     ver_name_list.append(db)\n",
    "\n",
    "# # output file path\n",
    "# if not os.path.exists(log_file_path):\n",
    "#     os.makedirs(log_file_path)\n",
    "# if not os.path.exists(ckpt_best_path):\n",
    "#     os.makedirs(ckpt_best_path)\n",
    "# if not os.path.exists(ckpt_path):\n",
    "#     os.makedirs(ckpt_path)\n",
    "# # create log dir\n",
    "# subdir = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "# log_dir = os.path.join(os.path.expanduser(log_file_path), subdir)\n",
    "# if not os.path.isdir(log_dir):  # Create the log directory if it doesn't exist\n",
    "#     os.makedirs(log_dir)\n",
    "\n",
    "\n",
    "def my_generator(tfrecord_path=TFRECORD_PATH,batch_size=BATCH_SIZE,out_num=NUM_CLASSES):\n",
    "    \"\"\"自定义generator\n",
    "    \n",
    "    # Argument\n",
    "        tfrecord_path:\n",
    "        batch_size\n",
    "        out_num: 类别数量，用于生成onehot\n",
    "        \n",
    "    # Return\n",
    "    \n",
    "    \"\"\"\n",
    "    def parse_function(example_proto):\n",
    "        features = {'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                    'label': tf.FixedLenFeature([], tf.int64)}\n",
    "        features = tf.parse_single_example(example_proto, features)\n",
    "        # You can do more image distortion here for training data\n",
    "        img = tf.image.decode_jpeg(features['image_raw'])\n",
    "        img = tf.reshape(img, shape=(112, 112, 3))\n",
    "\n",
    "        #img = tf.py_func(random_rotate_image, [img], tf.uint8)\n",
    "        img = tf.cast(img, dtype=tf.float32)\n",
    "        img = tf.subtract(img, 127.5)\n",
    "        img = tf.multiply(img,  0.0078125)\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        label = tf.cast(features['label'], tf.int64)\n",
    "#         label = tf.one_hot(label,out_num)\n",
    "#         label = tf.reshape(label,(-1,))\n",
    "#         one_hot = tf.one_hot(label,out_num)\n",
    "        return (img, label)\n",
    "    \n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess = tf.Session(config=config)\n",
    "#     sess = K.get_session()\n",
    "    # training datasets api config\n",
    "    tfrecords_f = os.path.join(tfrecord_path)\n",
    "    dataset = tf.data.TFRecordDataset(tfrecords_f)\n",
    "    dataset = dataset.map(parse_function)\n",
    "#     dataset = dataset.shuffle(buffer_size=5000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    return iterator, next_element, sess\n",
    "    # begin iteration\n",
    "#     while(True):\n",
    "#         sess.run(iterator.initializer)\n",
    "#         while True:\n",
    "#             try:\n",
    "#                 images, labels = sess.run(next_element)\n",
    "# #                 for i in range(len(images)):\n",
    "# #                     images[i,...] = cv2.cvtColor(images[i, ...], cv2.COLOR_RGB2BGR)\n",
    "#                 yield images,labels\n",
    "#             except tf.errors.OutOfRangeError:\n",
    "# #                 print(\"End of dataset\")\n",
    "#                 break\n",
    "\n",
    "\n",
    "def my_generator_wrapper():\n",
    "    for image,label in my_generator():\n",
    "        yield ([image,label],label)\n",
    "#         yield image,label\n",
    "        \n",
    "def test_generator():\n",
    "    a = my_generator_wrapper()\n",
    "    for i in tqdm(range(50000)):\n",
    "        a.__next__()\n",
    "        \n",
    "def learning_rate_schedule(epoch, lr, boundaries, values):\n",
    "    \"\"\"\n",
    "    # Argument:\n",
    "        epoch: now epoch\n",
    "        lr: learning rate to schedule\n",
    "        boundaries: Number of epochs for learning rate piecewise.\n",
    "        values: target value of learning rate\n",
    "    \"\"\"\n",
    "    if epoch <= boundaries[0]:\n",
    "        t = values[0]\n",
    "    for low, high, v in zip(boundaries[:-1],boundaries[1:],values[1:]):\n",
    "        if low < epoch <= high:\n",
    "            t = v\n",
    "    if epoch > boundaries[-1]:\n",
    "        t = values[-1]\n",
    "        \n",
    "    K.get_session().run(lr.assign(t))\n",
    "    return epoch,t\n",
    "\n",
    "class ExponentialMovingAverage:\n",
    "    \"\"\"对模型权重进行指数滑动平均。\n",
    "    用法：在model.compile之后、第一次训练之前使用；\n",
    "    先初始化对象，然后执行inject方法。\n",
    "    \"\"\"\n",
    "    def __init__(self, model, momentum=0.9999):\n",
    "        self.momentum = momentum\n",
    "        self.model = model\n",
    "        self.ema_weights = [K.zeros(K.shape(w)) for w in model.weights]\n",
    "    def inject(self):\n",
    "        \"\"\"添加更新算子到model.metrics_updates。\n",
    "        \"\"\"\n",
    "        self.initialize()\n",
    "        for w1, w2 in zip(self.ema_weights, self.model.weights):\n",
    "            op = K.moving_average_update(w1, w2, self.momentum)\n",
    "            self.model.metrics_updates.append(op)\n",
    "    def initialize(self):\n",
    "        \"\"\"ema_weights初始化跟原模型初始化一致。\n",
    "        \"\"\"\n",
    "        self.old_weights = K.batch_get_value(self.model.weights)\n",
    "        K.batch_set_value(zip(self.ema_weights, self.old_weights))\n",
    "    def apply_ema_weights(self):\n",
    "        \"\"\"备份原模型权重，然后将平均权重应用到模型上去。\n",
    "        \"\"\"\n",
    "        self.old_weights = K.batch_get_value(self.model.weights)\n",
    "        ema_weights = K.batch_get_value(self.ema_weights)\n",
    "        K.batch_set_value(zip(self.model.weights, ema_weights))\n",
    "    def reset_old_weights(self):\n",
    "        \"\"\"恢复模型到旧权重。\n",
    "        \"\"\"\n",
    "        K.batch_set_value(zip(self.model.weights, self.old_weights))\n",
    "\n",
    "\n",
    "\n",
    "def flow_wrapper(flow):\n",
    "    \"\"\"自定义wrapper，将(x,y)变成([x,y],y)\"\"\"\n",
    "    while True:\n",
    "        x,y = flow.next()\n",
    "        yield ([x,y],y)\n",
    "\n",
    "def prelu(input, name=''):\n",
    "    \"\"\"自定义prelu\"\"\"\n",
    "    alphas = K.variable(K.constant(0.25,dtype=tf.float32,shape=[input.get_shape()[-1]]),name=name + 'prelu_alphas')\n",
    "    pos = K.relu(input)\n",
    "    neg = alphas * (input - K.abs(input)) * 0.5\n",
    "    return pos + neg\n",
    "\n",
    "cval = Constant(0.25)  # prelu α 初始常量\n",
    "\n",
    "def _conv_block(inputs, filters, kernel, strides):\n",
    "    \"\"\"Convolution Block\n",
    "    This function defines a 2D convolution operation with BN and relu6.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        strides: An integer or tuple/list of 2 integers,\n",
    "            specifying the strides of the convolution along the width and height.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    x = Conv2D(filters, kernel, padding='same', strides=strides, kernel_initializer='glorot_normal',kernel_regularizer=l2(weight_decay),use_bias=False)(inputs)\n",
    "    x = BatchNormalization(axis=channel_axis,**batch_norm_params)(x)\n",
    "#     x = BatchNormalization(name='BatchNorm',axis=channel_axis,**batch_norm_params)(x)\n",
    "#     x = PReLU(cval,name='prelu')(x)\n",
    "#     x = Activation(relu)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _bottleneck(inputs, filters, kernel, t, s, r=False):\n",
    "    \"\"\"Bottleneck\n",
    "    This function defines a basic bottleneck structure.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        r: Boolean, Whether to use the residuals.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    tchannel = K.int_shape(inputs)[channel_axis] * t\n",
    "\n",
    "    x = _conv_block(inputs, tchannel, (1, 1), (1, 1))\n",
    "\n",
    "    x = DepthwiseConv2D(kernel, strides=(s, s), depth_multiplier=1, padding='same', kernel_initializer='glorot_normal',use_bias=False)(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = PReLU(cval)(x)\n",
    "#     x = Activation(relu)(x)\n",
    "\n",
    "    x = Conv2D(filters, (1, 1), strides=(1, 1), padding='same', kernel_initializer='glorot_normal',kernel_regularizer=l2(weight_decay),use_bias=False)(x)\n",
    "    x = BatchNormalization(axis=channel_axis,**batch_norm_params)(x)\n",
    "\n",
    "    if r:\n",
    "        x = add([x, inputs])\n",
    "    return x\n",
    "\n",
    "\n",
    "def _inverted_residual_block(inputs, filters, kernel, t, strides, n):\n",
    "    \"\"\"Inverted Residual Block\n",
    "    This function defines a sequence of 1 or more identical layers.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        n: Integer, layer repeat times.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    x = _bottleneck(inputs, filters, kernel, t, strides)\n",
    "\n",
    "    for i in range(1, n):\n",
    "        x = _bottleneck(x, filters, kernel, t, 1, True)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class ArcFace(Layer):\n",
    "    \"\"\"改进的softmax，得出的结果再与真是结果之间求交叉熵\"\"\"\n",
    "    def __init__(self, n_classes=10, s=64.0, m=0.50, regularizer=None, **kwargs):\n",
    "        super(ArcFace, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcFace, self).build(input_shape[0])\n",
    "        self.W = self.add_weight(name='embedding_weights',\n",
    "                                shape=(input_shape[0][-1], self.n_classes),\n",
    "                                initializer='glorot_uniform',\n",
    "                                trainable=True,\n",
    "                                regularizer=self.regularizer)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x, y = inputs # x为embeddings，y为labels\n",
    "#         c = K.shape(x)[-1]  # 特征维度\n",
    "#         # 1. normalize feature\n",
    "#         x = tf.nn.l2_normalize(x, axis=1)\n",
    "#         # 2. normalize weights\n",
    "#         W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "#         # dot product\n",
    "#         # 全连接层，x的结构为（None，128）w的结构为（128，n_classes）。logits的结构为(None,n_classes)\n",
    "#         # (np.random.randn(5,128) @ np.random.randn(128,10)).shape # (5, 10)\n",
    "#         # 3. 计算xW得到预测向量y\n",
    "#         logits = x @ W\n",
    "#         # add margin\n",
    "#         # clip logits to prevent zero division when backward\n",
    "#         theta = tf.acos(K.clip(logits, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n",
    "#         target_logits = tf.cos(theta + self.m)\n",
    "#         # sin = tf.sqrt(1 - logits**2)\n",
    "#         # cos_m = tf.cos(logits)\n",
    "#         # sin_m = tf.sin(logits)\n",
    "#         # target_logits = logits * cos_m - sin * sin_m\n",
    "#         logits = logits * (1 - y) + target_logits * y\n",
    "#         # feature re-scale\n",
    "#         # 9. 对所有值乘上固定值s\n",
    "#         logits *= self.s\n",
    "#         out = tf.nn.softmax(logits)\n",
    "#         print(out)\n",
    "#         return out\n",
    "\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        embedding, labels = inputs\n",
    "        labels = tf.reshape(labels,shape=(-1,))\n",
    "        print('labels:',labels)\n",
    "        \n",
    "        out_num = self.n_classes\n",
    "        w_init=None\n",
    "        s=64.\n",
    "        m=0.5\n",
    "        \n",
    "        cos_m = tf.cos(m)\n",
    "        sin_m = tf.sin(m)\n",
    "        mm = sin_m * m  # issue 1\n",
    "        threshold = tf.cos(math.pi - m)\n",
    "        with tf.variable_scope('arcface_loss'):\n",
    "            # inputs and weights norm\n",
    "            embedding_norm = tf.norm(embedding, axis=1, keepdims=True)\n",
    "            embedding = tf.div(embedding, embedding_norm, name='norm_embedding')\n",
    "#             weights = tf.get_variable(name='embedding_weights', shape=(embedding.get_shape().as_list()[-1], out_num),\n",
    "#                                       initializer=w_init, dtype=tf.float32)\n",
    "            weights = self.W\n",
    "            weights_norm = tf.norm(weights, axis=0, keepdims=True)\n",
    "            weights = tf.div(weights, weights_norm, name='norm_weights')\n",
    "            # cos(theta+m)\n",
    "            cos_t = tf.matmul(embedding, weights, name='cos_t')\n",
    "            cos_t2 = tf.square(cos_t, name='cos_2')\n",
    "            sin_t2 = tf.subtract(1., cos_t2, name='sin_2')\n",
    "            sin_t = tf.sqrt(sin_t2, name='sin_t')\n",
    "            cos_mt = s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')\n",
    "\n",
    "            # this condition controls the theta+m should in range [0, pi]\n",
    "            #      0<=theta+m<=pi\n",
    "            #     -m<=theta<=pi-m\n",
    "            cond_v = cos_t - threshold\n",
    "            cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)\n",
    "\n",
    "            keep_val = s*(cos_t - mm)\n",
    "            cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n",
    "            print('labels:',labels,'out_num',out_num)\n",
    "            mask = tf.one_hot(labels, depth=out_num, name='one_hot_mask')\n",
    "#             mask = tf.reshape(mask,(-1,))\n",
    "#             mask = labels\n",
    "            print(mask)\n",
    "            # mask = tf.squeeze(mask, 1)\n",
    "            inv_mask = tf.subtract(1., mask, name='inverse_mask')\n",
    "            print(inv_mask)\n",
    "            s_cos_t = tf.multiply(s, cos_t, name='scalar_cos_t')\n",
    "            print(s_cos_t)\n",
    "            logit = tf.add(tf.multiply(s_cos_t, inv_mask), tf.multiply(cos_mt_temp, mask), name='arcface_loss_output')\n",
    "            print(logit)\n",
    "#             inference_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=labels))\n",
    "            inference_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=labels)\n",
    "            print(inference_loss)\n",
    "#             inference_loss = tf.nn.softmax(logit)\n",
    "            print(inference_loss)\n",
    "        return inference_loss\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.n_classes)\n",
    "\n",
    "def MobileFaceNets(input_shape=(112,112,3), n_classes=10, k=128):\n",
    "    \"\"\"MobileFaceNets\"\"\"\n",
    "    with tf.name_scope('MobileFaceNet'):\n",
    "#         with tf.\n",
    "        inputs = Input(shape=input_shape) #112x112，(img-127.5)/255\n",
    "        y      = Input(shape=(1,), dtype=tf.int32)\n",
    "    #     y      = Input(shape=(n_classes,))\n",
    "        x = _conv_block(inputs, 64, (3, 3), strides=(2, 2))\n",
    "\n",
    "        # depthwise conv3x3\n",
    "        x = DepthwiseConv2D(3, strides=(1, 1), depth_multiplier=1, padding='same',use_bias=False)(x)\n",
    "        x = BatchNormalization(name='BatchNorm')(x)\n",
    "        x = PReLU(cval,name='prelu')(x)\n",
    "    #     x = Activation(relu)(x)\n",
    "\n",
    "        # 5层bottleneck\n",
    "        x = _inverted_residual_block(x, 64, (3, 3), t=2, strides=2, n=5)\n",
    "        x = _inverted_residual_block(x, 128, (3, 3), t=4, strides=2, n=1)\n",
    "        x = _inverted_residual_block(x, 128, (3, 3), t=2, strides=1, n=6)\n",
    "        x = _inverted_residual_block(x, 128, (3, 3), t=4, strides=2, n=1)\n",
    "        x = _inverted_residual_block(x, 128, (3, 3), t=2, strides=1, n=2)\n",
    "        \n",
    "        # conv1x1\n",
    "        x = _conv_block(x, 512, (1, 1), strides=(1, 1))\n",
    "        \n",
    "        with tf.name_scope('Logits'):\n",
    "\n",
    "            # linear GDConv7x7\n",
    "            x = DepthwiseConv2D(7, strides=(1, 1), depth_multiplier=1, padding='valid',use_bias=False)(x)\n",
    "            x = BatchNormalization(**batch_norm_params)(x)\n",
    "            x = Conv2D(512,(1,1),kernel_initializer='glorot_normal',use_bias=False)(x)\n",
    "            x = BatchNormalization(**batch_norm_params)(x)\n",
    "        #     x = Dropout(0.3, name='Dropout')(x)\n",
    "            with tf.name_scope('LinearConv1x1'):\n",
    "                x = Conv2D(k, (1, 1), padding='same',kernel_initializer='glorot_normal',kernel_regularizer=l2(1e-10),use_bias=False)(x)\n",
    "                x = BatchNormalization(**batch_norm_params)(x)\n",
    "                \n",
    "\n",
    "\n",
    "    #     x = Activation(keras.activations.re)\n",
    "        x = Reshape((k,))(x)\n",
    "        print(x)\n",
    "    #     x = keras.layers.Lambda(lambda o: K.l2_normalize(o, axis=1))(x)\n",
    "        epsilon=1e-10\n",
    "        x = keras.layers.Lambda(lambda o: tf.nn.l2_normalize(o, 1, 1e-10, name='embeddings'))(x)\n",
    "#     x = tf.nn.l2_normalize(x, 1, 1e-10, name='embeddings')\n",
    "    \n",
    "    # x 为embeddings， y为embeddings对应的类别标签，output为\n",
    "    output = ArcFace(n_classes=n_classes, regularizer=None)([x, y])\n",
    "    \n",
    "    model = Model([inputs, y], output)\n",
    "#     plot_model(model, to_file='images/MobileNetv2.png', show_shapes=True)\n",
    "    print(model.input,model.output)\n",
    "    return model\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = MobileFaceNets(n_classes=NUM_CLASSES)\n",
    "model.compile(optimizer=keras.optimizers.Adam(0.1,beta_1=0.9, beta_2=0.999, epsilon=0.1),\n",
    "      loss=my_loss,\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "EMAer = ExponentialMovingAverage(model) # 在模型compile之后执行\n",
    "EMAer.inject() # 在模型compile之后执行\n",
    "\n",
    "# model.compile(optimizer=keras.optimizers.sgd(0.1),loss=my_loss,metrics=['accuracy'])\n",
    "val_model = keras.models.Model(inputs=model.inputs[0], outputs=model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin db lfw convert.\n",
      "loading bin 1000\n",
      "loading bin 2000\n",
      "loading bin 3000\n",
      "loading bin 4000\n",
      "loading bin 5000\n",
      "loading bin 6000\n",
      "loading bin 7000\n",
      "loading bin 8000\n",
      "loading bin 9000\n",
      "loading bin 10000\n",
      "loading bin 11000\n",
      "loading bin 12000\n",
      "(12000, 112, 112, 3)\n",
      "epoch:0, lr:0.1\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "epoch 0, total_step 50, loss is 62.851475, training accuracy is 0.000000, time 456.490 samples/sec\n",
      "epoch 0, total_step 100, loss is 66.567482, training accuracy is 0.000000, time 444.124 samples/sec\n",
      "epoch 0, total_step 150, loss is 67.086479, training accuracy is 0.000000, time 450.618 samples/sec\n",
      "epoch 0, total_step 200, loss is 65.415833, training accuracy is 0.000000, time 448.588 samples/sec\n",
      "epoch 0, total_step 250, loss is 60.876808, training accuracy is 0.000000, time 438.735 samples/sec\n",
      "epoch 0, total_step 300, loss is 58.330166, training accuracy is 0.000000, time 412.152 samples/sec\n",
      "epoch 0, total_step 350, loss is 58.329250, training accuracy is 0.000000, time 452.771 samples/sec\n",
      "epoch 0, total_step 400, loss is 55.761002, training accuracy is 0.000000, time 450.803 samples/sec\n",
      "epoch 0, total_step 450, loss is 55.456886, training accuracy is 0.000000, time 452.294 samples/sec\n",
      "epoch 0, total_step 500, loss is 52.109562, training accuracy is 0.000000, time 448.132 samples/sec\n",
      "\n",
      "Iteration 500 testing...\n",
      "thresholds max: 0.52 <=> min: 0.51\n",
      "total time 13.426s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.660+-0.023\n",
      "Validation rate: 0.01200+-0.00306 @ FAR=0.00100\n",
      "fpr and tpr: 0.830 0.890\n",
      "Area Under Curve (AUC): 0.712\n",
      "epoch 0, total_step 550, loss is 52.646683, training accuracy is 0.000000, time 450.695 samples/sec\n",
      "epoch 0, total_step 600, loss is 50.648937, training accuracy is 0.000000, time 451.218 samples/sec\n",
      "epoch 0, total_step 650, loss is 51.210743, training accuracy is 0.000000, time 451.413 samples/sec\n",
      "epoch 0, total_step 700, loss is 50.902588, training accuracy is 0.000000, time 451.192 samples/sec\n",
      "epoch 0, total_step 750, loss is 48.173985, training accuracy is 0.000000, time 444.452 samples/sec\n",
      "epoch 0, total_step 800, loss is 48.133621, training accuracy is 0.000000, time 445.345 samples/sec\n",
      "epoch 0, total_step 850, loss is 49.155643, training accuracy is 0.000000, time 446.923 samples/sec\n",
      "epoch 0, total_step 900, loss is 47.722260, training accuracy is 0.000000, time 447.344 samples/sec\n",
      "epoch 0, total_step 950, loss is 48.614738, training accuracy is 0.000000, time 429.778 samples/sec\n",
      "epoch 0, total_step 1000, loss is 46.379868, training accuracy is 0.000000, time 443.310 samples/sec\n",
      "\n",
      "Iteration 1000 testing...\n",
      "thresholds max: 1.08 <=> min: 0.51\n",
      "total time 12.160s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.655+-0.019\n",
      "Validation rate: 0.01967+-0.00706 @ FAR=0.00133\n",
      "fpr and tpr: 0.718 0.822\n",
      "Area Under Curve (AUC): 0.727\n",
      "epoch 0, total_step 1050, loss is 47.752895, training accuracy is 0.000000, time 437.655 samples/sec\n",
      "epoch 0, total_step 1100, loss is 45.227863, training accuracy is 0.000000, time 446.046 samples/sec\n",
      "epoch 0, total_step 1150, loss is 45.930256, training accuracy is 0.000000, time 448.917 samples/sec\n",
      "epoch 0, total_step 1200, loss is 45.093891, training accuracy is 0.000000, time 433.530 samples/sec\n",
      "epoch 0, total_step 1250, loss is 43.576107, training accuracy is 0.000000, time 449.505 samples/sec\n",
      "epoch 0, total_step 1300, loss is 47.560997, training accuracy is 0.000000, time 412.115 samples/sec\n",
      "epoch 0, total_step 1350, loss is 43.563736, training accuracy is 0.000000, time 413.988 samples/sec\n",
      "epoch 0, total_step 1400, loss is 43.714359, training accuracy is 0.000000, time 449.915 samples/sec\n",
      "epoch 0, total_step 1450, loss is 43.088589, training accuracy is 0.000000, time 454.703 samples/sec\n",
      "epoch 0, total_step 1500, loss is 43.152786, training accuracy is 0.000000, time 445.479 samples/sec\n",
      "\n",
      "Iteration 1500 testing...\n",
      "thresholds max: 1.08 <=> min: 0.51\n",
      "total time 12.085s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.658+-0.019\n",
      "Validation rate: 0.01433+-0.00633 @ FAR=0.00100\n",
      "fpr and tpr: 0.737 0.843\n",
      "Area Under Curve (AUC): 0.728\n",
      "epoch 0, total_step 1550, loss is 43.964588, training accuracy is 0.000000, time 446.133 samples/sec\n",
      "epoch 0, total_step 1600, loss is 42.624359, training accuracy is 0.000000, time 442.993 samples/sec\n",
      "epoch 0, total_step 1650, loss is 42.331932, training accuracy is 0.000000, time 440.702 samples/sec\n",
      "epoch 0, total_step 1700, loss is 42.328266, training accuracy is 0.000000, time 451.503 samples/sec\n",
      "epoch 0, total_step 1750, loss is 42.242039, training accuracy is 0.000000, time 447.675 samples/sec\n",
      "epoch 0, total_step 1800, loss is 42.381947, training accuracy is 0.000000, time 456.097 samples/sec\n",
      "epoch 0, total_step 1850, loss is 41.849987, training accuracy is 0.000000, time 441.816 samples/sec\n",
      "epoch 0, total_step 1900, loss is 42.066879, training accuracy is 0.000000, time 445.139 samples/sec\n",
      "epoch 0, total_step 1950, loss is 41.592548, training accuracy is 0.000000, time 453.840 samples/sec\n",
      "epoch 0, total_step 2000, loss is 41.264706, training accuracy is 0.000000, time 449.608 samples/sec\n",
      "\n",
      "Iteration 2000 testing...\n",
      "thresholds max: 1.08 <=> min: 0.51\n",
      "total time 12.038s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.670+-0.014\n",
      "Validation rate: 0.00967+-0.00526 @ FAR=0.00067\n",
      "fpr and tpr: 0.773 0.874\n",
      "Area Under Curve (AUC): 0.735\n",
      "epoch 0, total_step 2050, loss is 41.918243, training accuracy is 0.000000, time 455.971 samples/sec\n",
      "epoch 0, total_step 2100, loss is 41.970013, training accuracy is 0.000000, time 441.876 samples/sec\n",
      "epoch 0, total_step 2150, loss is 41.623165, training accuracy is 0.000000, time 444.148 samples/sec\n",
      "epoch 0, total_step 2200, loss is 41.905792, training accuracy is 0.000000, time 449.989 samples/sec\n",
      "epoch 0, total_step 2250, loss is 41.614262, training accuracy is 0.000000, time 442.100 samples/sec\n",
      "epoch 0, total_step 2300, loss is 40.865028, training accuracy is 0.000000, time 443.891 samples/sec\n",
      "epoch 0, total_step 2350, loss is 41.086674, training accuracy is 0.000000, time 442.624 samples/sec\n",
      "epoch 0, total_step 2400, loss is 40.899628, training accuracy is 0.000000, time 452.136 samples/sec\n",
      "epoch 0, total_step 2450, loss is 40.840347, training accuracy is 0.000000, time 451.080 samples/sec\n",
      "epoch 0, total_step 2500, loss is 41.098866, training accuracy is 0.000000, time 441.192 samples/sec\n",
      "\n",
      "Iteration 2500 testing...\n",
      "thresholds max: 1.08 <=> min: 0.46\n",
      "total time 12.157s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.679+-0.016\n",
      "Validation rate: 0.01300+-0.00379 @ FAR=0.00100\n",
      "fpr and tpr: 0.805 0.898\n",
      "Area Under Curve (AUC): 0.739\n",
      "epoch 0, total_step 2550, loss is 41.437389, training accuracy is 0.000000, time 455.641 samples/sec\n",
      "epoch 0, total_step 2600, loss is 41.297688, training accuracy is 0.000000, time 441.252 samples/sec\n",
      "epoch 0, total_step 2650, loss is 40.145119, training accuracy is 0.000000, time 437.288 samples/sec\n",
      "epoch 0, total_step 2700, loss is 40.929321, training accuracy is 0.000000, time 444.297 samples/sec\n",
      "epoch 0, total_step 2750, loss is 40.388523, training accuracy is 0.000000, time 448.806 samples/sec\n",
      "epoch 0, total_step 2800, loss is 40.631413, training accuracy is 0.000000, time 452.323 samples/sec\n",
      "epoch 0, total_step 2850, loss is 39.764133, training accuracy is 0.000000, time 448.954 samples/sec\n",
      "epoch 0, total_step 2900, loss is 40.225800, training accuracy is 0.000000, time 449.226 samples/sec\n",
      "epoch 0, total_step 2950, loss is 39.807560, training accuracy is 0.000000, time 448.272 samples/sec\n",
      "epoch 0, total_step 3000, loss is 40.518082, training accuracy is 0.000000, time 443.429 samples/sec\n",
      "\n",
      "Iteration 3000 testing...\n",
      "thresholds max: 1.08 <=> min: 0.34\n",
      "total time 12.144s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.681+-0.012\n",
      "Validation rate: 0.01767+-0.00396 @ FAR=0.00100\n",
      "fpr and tpr: 0.838 0.918\n",
      "Area Under Curve (AUC): 0.743\n",
      "epoch 0, total_step 3050, loss is 40.032024, training accuracy is 0.000000, time 444.792 samples/sec\n",
      "epoch 0, total_step 3100, loss is 40.185772, training accuracy is 0.000000, time 436.545 samples/sec\n",
      "epoch 0, total_step 3150, loss is 39.691757, training accuracy is 0.000000, time 435.950 samples/sec\n",
      "epoch 0, total_step 3200, loss is 39.788933, training accuracy is 0.000000, time 443.492 samples/sec\n",
      "epoch 0, total_step 3250, loss is 39.643879, training accuracy is 0.000000, time 451.766 samples/sec\n",
      "epoch 0, total_step 3300, loss is 39.677727, training accuracy is 0.000000, time 458.797 samples/sec\n",
      "epoch 0, total_step 3350, loss is 39.254948, training accuracy is 0.000000, time 436.804 samples/sec\n",
      "epoch 0, total_step 3400, loss is 39.322803, training accuracy is 0.000000, time 445.720 samples/sec\n",
      "epoch 0, total_step 3450, loss is 39.190289, training accuracy is 0.000000, time 457.076 samples/sec\n",
      "epoch 0, total_step 3500, loss is 39.383652, training accuracy is 0.000000, time 454.070 samples/sec\n",
      "\n",
      "Iteration 3500 testing...\n",
      "thresholds max: 1.08 <=> min: 0.27\n",
      "total time 12.062s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.688+-0.014\n",
      "Validation rate: 0.01767+-0.00448 @ FAR=0.00100\n",
      "fpr and tpr: 0.871 0.937\n",
      "Area Under Curve (AUC): 0.750\n",
      "epoch 0, total_step 3550, loss is 39.071442, training accuracy is 0.000000, time 454.276 samples/sec\n",
      "epoch 0, total_step 3600, loss is 38.990730, training accuracy is 0.000000, time 448.385 samples/sec\n",
      "epoch 0, total_step 3650, loss is 38.555626, training accuracy is 0.000000, time 451.793 samples/sec\n",
      "epoch 0, total_step 3700, loss is 38.580055, training accuracy is 0.000000, time 447.820 samples/sec\n",
      "epoch 0, total_step 3750, loss is 38.710060, training accuracy is 0.000000, time 442.245 samples/sec\n",
      "epoch 0, total_step 3800, loss is 38.390255, training accuracy is 0.000000, time 445.123 samples/sec\n",
      "epoch 0, total_step 3850, loss is 38.405800, training accuracy is 0.000000, time 453.194 samples/sec\n",
      "epoch 0, total_step 3900, loss is 38.029385, training accuracy is 0.000000, time 447.550 samples/sec\n",
      "epoch 0, total_step 3950, loss is 38.267727, training accuracy is 0.000000, time 449.693 samples/sec\n",
      "epoch 0, total_step 4000, loss is 37.745304, training accuracy is 0.000000, time 448.767 samples/sec\n",
      "\n",
      "Iteration 4000 testing...\n",
      "thresholds max: 1.08 <=> min: 0.2\n",
      "total time 12.048s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.692+-0.015\n",
      "Validation rate: 0.02367+-0.00960 @ FAR=0.00100\n",
      "fpr and tpr: 0.904 0.954\n",
      "Area Under Curve (AUC): 0.760\n",
      "epoch 0, total_step 4050, loss is 37.967922, training accuracy is 0.000000, time 446.091 samples/sec\n",
      "epoch 0, total_step 4100, loss is 38.004986, training accuracy is 0.000000, time 440.072 samples/sec\n",
      "epoch 0, total_step 4150, loss is 37.646824, training accuracy is 0.000000, time 441.973 samples/sec\n",
      "epoch 0, total_step 4200, loss is 37.559013, training accuracy is 0.000000, time 448.747 samples/sec\n",
      "epoch 0, total_step 4250, loss is 37.588203, training accuracy is 0.000000, time 449.286 samples/sec\n",
      "epoch 0, total_step 4300, loss is 37.381504, training accuracy is 0.000000, time 459.678 samples/sec\n",
      "epoch 0, total_step 4350, loss is 37.658760, training accuracy is 0.000000, time 448.797 samples/sec\n",
      "epoch 0, total_step 4400, loss is 36.843052, training accuracy is 0.000000, time 447.378 samples/sec\n",
      "epoch 0, total_step 4450, loss is 37.039669, training accuracy is 0.000000, time 442.970 samples/sec\n",
      "epoch 0, total_step 4500, loss is 36.822849, training accuracy is 0.000000, time 443.630 samples/sec\n",
      "\n",
      "Iteration 4500 testing...\n",
      "thresholds max: 1.08 <=> min: 0.14\n",
      "total time 11.978s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.701+-0.012\n",
      "Validation rate: 0.01733+-0.00416 @ FAR=0.00100\n",
      "fpr and tpr: 0.929 0.967\n",
      "Area Under Curve (AUC): 0.769\n",
      "epoch 0, total_step 4550, loss is 36.685757, training accuracy is 0.000000, time 451.216 samples/sec\n",
      "epoch 0, total_step 4600, loss is 36.979774, training accuracy is 0.000000, time 455.534 samples/sec\n",
      "epoch 0, total_step 4650, loss is 36.716434, training accuracy is 0.000000, time 447.370 samples/sec\n",
      "epoch 0, total_step 4700, loss is 36.632664, training accuracy is 0.000000, time 439.760 samples/sec\n",
      "epoch 0, total_step 4750, loss is 36.256420, training accuracy is 0.000000, time 449.240 samples/sec\n",
      "epoch 0, total_step 4800, loss is 36.421188, training accuracy is 0.000000, time 442.976 samples/sec\n",
      "epoch 0, total_step 4850, loss is 36.069256, training accuracy is 0.000000, time 451.548 samples/sec\n",
      "epoch 0, total_step 4900, loss is 36.208954, training accuracy is 0.000000, time 446.932 samples/sec\n",
      "epoch 0, total_step 4950, loss is 36.213150, training accuracy is 0.000000, time 448.307 samples/sec\n",
      "epoch 0, total_step 5000, loss is 35.533348, training accuracy is 0.000000, time 449.587 samples/sec\n",
      "\n",
      "Iteration 5000 testing...\n",
      "thresholds max: 1.08 <=> min: 0.09\n",
      "total time 12.002s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.705+-0.012\n",
      "Validation rate: 0.01600+-0.00554 @ FAR=0.00100\n",
      "fpr and tpr: 0.948 0.976\n",
      "Area Under Curve (AUC): 0.778\n",
      "epoch 0, total_step 5050, loss is 35.486717, training accuracy is 0.000000, time 439.408 samples/sec\n",
      "epoch 0, total_step 5100, loss is 35.688568, training accuracy is 0.000000, time 455.648 samples/sec\n",
      "epoch 0, total_step 5150, loss is 35.197678, training accuracy is 0.000000, time 447.885 samples/sec\n",
      "epoch 0, total_step 5200, loss is 34.868954, training accuracy is 0.000000, time 450.620 samples/sec\n",
      "epoch 0, total_step 5250, loss is 35.086712, training accuracy is 0.000000, time 448.988 samples/sec\n",
      "epoch 0, total_step 5300, loss is 34.698708, training accuracy is 0.000000, time 451.418 samples/sec\n",
      "epoch 0, total_step 5350, loss is 34.448460, training accuracy is 0.000000, time 445.857 samples/sec\n",
      "epoch 0, total_step 5400, loss is 34.402740, training accuracy is 0.000000, time 443.706 samples/sec\n",
      "epoch 0, total_step 5450, loss is 34.466324, training accuracy is 0.000000, time 444.403 samples/sec\n",
      "End of epoch 0\n",
      "epoch:1, lr:0.1\n",
      "epoch 1, total_step 5500, loss is 34.323078, training accuracy is 0.000000, time 447.165 samples/sec\n",
      "\n",
      "Iteration 5500 testing...\n",
      "thresholds max: 1.08 <=> min: 0.08\n",
      "total time 12.394s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.720+-0.009\n",
      "Validation rate: 0.01800+-0.00562 @ FAR=0.00100\n",
      "fpr and tpr: 0.961 0.983\n",
      "Area Under Curve (AUC): 0.786\n",
      "epoch 1, total_step 5550, loss is 34.160934, training accuracy is 0.000000, time 445.028 samples/sec\n",
      "epoch 1, total_step 5600, loss is 33.989941, training accuracy is 0.000000, time 442.182 samples/sec\n",
      "epoch 1, total_step 5650, loss is 33.779339, training accuracy is 0.000000, time 442.300 samples/sec\n",
      "epoch 1, total_step 5700, loss is 33.677620, training accuracy is 0.000000, time 443.451 samples/sec\n",
      "epoch 1, total_step 5750, loss is 33.043591, training accuracy is 0.000000, time 451.118 samples/sec\n",
      "epoch 1, total_step 5800, loss is 33.256741, training accuracy is 0.000000, time 457.160 samples/sec\n",
      "epoch 1, total_step 5850, loss is 32.944714, training accuracy is 0.000000, time 435.053 samples/sec\n",
      "epoch 1, total_step 5900, loss is 32.866264, training accuracy is 0.000000, time 448.122 samples/sec\n",
      "epoch 1, total_step 5950, loss is 32.928051, training accuracy is 0.000000, time 452.160 samples/sec\n",
      "epoch 1, total_step 6000, loss is 32.484924, training accuracy is 0.000000, time 451.187 samples/sec\n",
      "\n",
      "Iteration 6000 testing...\n",
      "thresholds max: 1.08 <=> min: 0.05\n",
      "total time 12.419s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.723+-0.010\n",
      "Validation rate: 0.01933+-0.00629 @ FAR=0.00133\n",
      "fpr and tpr: 0.971 0.987\n",
      "Area Under Curve (AUC): 0.793\n",
      "epoch 1, total_step 6050, loss is 32.159740, training accuracy is 0.000000, time 454.291 samples/sec\n",
      "epoch 1, total_step 6100, loss is 32.048576, training accuracy is 0.000000, time 445.846 samples/sec\n",
      "epoch 1, total_step 6150, loss is 31.882694, training accuracy is 0.000000, time 452.764 samples/sec\n",
      "epoch 1, total_step 6200, loss is 31.629232, training accuracy is 0.000000, time 443.790 samples/sec\n",
      "epoch 1, total_step 6250, loss is 31.344589, training accuracy is 0.000000, time 446.098 samples/sec\n",
      "epoch 1, total_step 6300, loss is 31.155977, training accuracy is 0.000000, time 446.611 samples/sec\n",
      "epoch 1, total_step 6350, loss is 31.015165, training accuracy is 0.000000, time 444.939 samples/sec\n",
      "epoch 1, total_step 6400, loss is 30.925652, training accuracy is 0.000000, time 448.044 samples/sec\n",
      "epoch 1, total_step 6450, loss is 30.516201, training accuracy is 0.000000, time 447.705 samples/sec\n",
      "epoch 1, total_step 6500, loss is 30.392197, training accuracy is 0.000000, time 441.455 samples/sec\n",
      "\n",
      "Iteration 6500 testing...\n",
      "thresholds max: 1.08 <=> min: 0.04\n",
      "total time 12.409s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.730+-0.009\n",
      "Validation rate: 0.02267+-0.00680 @ FAR=0.00167\n",
      "fpr and tpr: 0.979 0.990\n",
      "Area Under Curve (AUC): 0.798\n",
      "epoch 1, total_step 6550, loss is 30.178539, training accuracy is 0.000000, time 445.814 samples/sec\n",
      "epoch 1, total_step 6600, loss is 29.871834, training accuracy is 0.000000, time 448.613 samples/sec\n",
      "epoch 1, total_step 6650, loss is 29.698172, training accuracy is 0.000000, time 448.660 samples/sec\n",
      "epoch 1, total_step 6700, loss is 29.511461, training accuracy is 0.000000, time 444.721 samples/sec\n",
      "epoch 1, total_step 6750, loss is 29.323275, training accuracy is 0.000000, time 452.418 samples/sec\n",
      "epoch 1, total_step 6800, loss is 29.128979, training accuracy is 0.000000, time 450.223 samples/sec\n",
      "epoch 1, total_step 6850, loss is 28.889544, training accuracy is 0.000000, time 453.816 samples/sec\n",
      "epoch 1, total_step 6900, loss is 28.532789, training accuracy is 0.000000, time 451.260 samples/sec\n",
      "epoch 1, total_step 6950, loss is 28.267746, training accuracy is 0.000000, time 448.970 samples/sec\n",
      "epoch 1, total_step 7000, loss is 28.097176, training accuracy is 0.000000, time 452.016 samples/sec\n",
      "\n",
      "Iteration 7000 testing...\n",
      "thresholds max: 1.08 <=> min: 0.03\n",
      "total time 12.319s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.737+-0.010\n",
      "Validation rate: 0.01867+-0.00653 @ FAR=0.00100\n",
      "fpr and tpr: 0.984 0.993\n",
      "Area Under Curve (AUC): 0.802\n",
      "epoch 1, total_step 7050, loss is 27.832064, training accuracy is 0.000000, time 447.743 samples/sec\n",
      "epoch 1, total_step 7100, loss is 27.516148, training accuracy is 0.000000, time 452.084 samples/sec\n",
      "epoch 1, total_step 7150, loss is 27.389978, training accuracy is 0.000000, time 446.602 samples/sec\n",
      "epoch 1, total_step 7200, loss is 27.068195, training accuracy is 0.000000, time 451.314 samples/sec\n",
      "epoch 1, total_step 7250, loss is 26.682995, training accuracy is 0.000000, time 454.110 samples/sec\n",
      "epoch 1, total_step 7300, loss is 26.551001, training accuracy is 0.000000, time 450.800 samples/sec\n",
      "epoch 1, total_step 7350, loss is 26.370461, training accuracy is 0.000000, time 458.261 samples/sec\n",
      "epoch 1, total_step 7400, loss is 25.906672, training accuracy is 0.000000, time 451.847 samples/sec\n",
      "epoch 1, total_step 7450, loss is 25.601002, training accuracy is 0.000000, time 454.035 samples/sec\n",
      "epoch 1, total_step 7500, loss is 25.385668, training accuracy is 0.000000, time 446.068 samples/sec\n",
      "\n",
      "Iteration 7500 testing...\n",
      "thresholds max: 1.08 <=> min: 0.02\n",
      "total time 12.509s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.741+-0.011\n",
      "Validation rate: 0.02067+-0.00680 @ FAR=0.00100\n",
      "fpr and tpr: 0.988 0.994\n",
      "Area Under Curve (AUC): 0.801\n",
      "epoch 1, total_step 7550, loss is 25.057964, training accuracy is 0.000000, time 445.119 samples/sec\n",
      "epoch 1, total_step 7600, loss is 24.708454, training accuracy is 0.000000, time 445.267 samples/sec\n",
      "epoch 1, total_step 7650, loss is 24.504133, training accuracy is 0.000000, time 451.790 samples/sec\n",
      "epoch 1, total_step 7700, loss is 24.152763, training accuracy is 0.000000, time 452.676 samples/sec\n",
      "epoch 1, total_step 7750, loss is 23.863832, training accuracy is 0.000000, time 445.929 samples/sec\n",
      "epoch 1, total_step 7800, loss is 23.490801, training accuracy is 0.000000, time 448.318 samples/sec\n",
      "epoch 1, total_step 7850, loss is 23.235943, training accuracy is 0.000000, time 451.559 samples/sec\n",
      "epoch 1, total_step 7900, loss is 22.826166, training accuracy is 0.000000, time 447.828 samples/sec\n",
      "epoch 1, total_step 7950, loss is 22.509979, training accuracy is 0.000000, time 456.040 samples/sec\n",
      "epoch 1, total_step 8000, loss is 22.227001, training accuracy is 0.000000, time 443.299 samples/sec\n",
      "\n",
      "Iteration 8000 testing...\n",
      "thresholds max: 1.08 <=> min: 0.02\n",
      "total time 12.439s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.731+-0.017\n",
      "Validation rate: 0.01967+-0.00640 @ FAR=0.00100\n",
      "fpr and tpr: 0.991 0.996\n",
      "Area Under Curve (AUC): 0.796\n",
      "epoch 1, total_step 8050, loss is 21.834194, training accuracy is 0.000000, time 444.745 samples/sec\n",
      "epoch 1, total_step 8100, loss is 21.590963, training accuracy is 0.000000, time 446.731 samples/sec\n",
      "epoch 1, total_step 8150, loss is 21.185835, training accuracy is 0.000000, time 451.703 samples/sec\n",
      "epoch 1, total_step 8200, loss is 20.821337, training accuracy is 0.000000, time 444.638 samples/sec\n",
      "epoch 1, total_step 8250, loss is 20.500034, training accuracy is 0.000000, time 456.080 samples/sec\n",
      "epoch 1, total_step 8300, loss is 20.074015, training accuracy is 0.000000, time 447.742 samples/sec\n",
      "epoch 1, total_step 8350, loss is 19.723616, training accuracy is 0.000000, time 444.989 samples/sec\n",
      "epoch 1, total_step 8400, loss is 19.332067, training accuracy is 0.000000, time 451.476 samples/sec\n",
      "epoch 1, total_step 8450, loss is 18.970343, training accuracy is 0.000000, time 449.076 samples/sec\n",
      "epoch 1, total_step 8500, loss is 18.576536, training accuracy is 0.000000, time 435.989 samples/sec\n",
      "\n",
      "Iteration 8500 testing...\n",
      "thresholds max: 1.08 <=> min: 0.01\n",
      "total time 12.517s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.748+-0.012\n",
      "Validation rate: 0.02533+-0.00806 @ FAR=0.00133\n",
      "fpr and tpr: 0.993 0.996\n",
      "Area Under Curve (AUC): 0.787\n",
      "epoch 1, total_step 8550, loss is 18.366070, training accuracy is 0.000000, time 451.283 samples/sec\n",
      "epoch 1, total_step 8600, loss is 19.398256, training accuracy is 0.000000, time 447.216 samples/sec\n",
      "epoch 1, total_step 8650, loss is 19.413521, training accuracy is 0.000000, time 438.527 samples/sec\n",
      "epoch 1, total_step 8700, loss is 20.745377, training accuracy is 0.000000, time 448.592 samples/sec\n",
      "epoch 1, total_step 8750, loss is 20.803110, training accuracy is 0.000000, time 430.890 samples/sec\n",
      "epoch 1, total_step 8800, loss is 19.931353, training accuracy is 0.000000, time 451.139 samples/sec\n",
      "epoch 1, total_step 8850, loss is 20.249418, training accuracy is 0.000000, time 439.277 samples/sec\n",
      "epoch 1, total_step 8900, loss is 20.398376, training accuracy is 0.000000, time 440.469 samples/sec\n",
      "epoch 1, total_step 8950, loss is 21.086519, training accuracy is 0.000000, time 450.749 samples/sec\n",
      "epoch 1, total_step 9000, loss is 21.661417, training accuracy is 0.000000, time 450.035 samples/sec\n",
      "\n",
      "Iteration 9000 testing...\n",
      "thresholds max: 1.08 <=> min: 0.01\n",
      "total time 12.400s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.748+-0.017\n",
      "Validation rate: 0.00500+-0.00453 @ FAR=0.00000\n",
      "fpr and tpr: 0.994 0.997\n",
      "Area Under Curve (AUC): 0.762\n",
      "epoch 1, total_step 9050, loss is 22.034353, training accuracy is 0.000000, time 449.283 samples/sec\n",
      "epoch 1, total_step 9100, loss is 21.553831, training accuracy is 0.000000, time 445.820 samples/sec\n",
      "epoch 1, total_step 9150, loss is 21.503115, training accuracy is 0.000000, time 457.568 samples/sec\n",
      "epoch 1, total_step 9200, loss is 21.562456, training accuracy is 0.000000, time 451.127 samples/sec\n",
      "epoch 1, total_step 9250, loss is 22.216572, training accuracy is 0.000000, time 438.549 samples/sec\n",
      "epoch 1, total_step 9300, loss is 21.719770, training accuracy is 0.000000, time 446.542 samples/sec\n",
      "epoch 1, total_step 9350, loss is 21.711609, training accuracy is 0.000000, time 449.166 samples/sec\n",
      "epoch 1, total_step 9400, loss is 21.784630, training accuracy is 0.000000, time 450.687 samples/sec\n",
      "epoch 1, total_step 9450, loss is 21.864464, training accuracy is 0.000000, time 441.574 samples/sec\n",
      "epoch 1, total_step 9500, loss is 21.408695, training accuracy is 0.000000, time 437.934 samples/sec\n",
      "\n",
      "Iteration 9500 testing...\n",
      "thresholds max: 1.08 <=> min: 0.01\n",
      "total time 12.407s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.722+-0.021\n",
      "Validation rate: 0.00233+-0.00213 @ FAR=0.00000\n",
      "fpr and tpr: 0.995 0.997\n",
      "Area Under Curve (AUC): 0.727\n",
      "epoch 1, total_step 9550, loss is 21.865591, training accuracy is 0.000000, time 443.869 samples/sec\n",
      "epoch 1, total_step 9600, loss is 21.505859, training accuracy is 0.000000, time 445.642 samples/sec\n",
      "epoch 1, total_step 9650, loss is 21.759489, training accuracy is 0.000000, time 448.168 samples/sec\n",
      "epoch 1, total_step 9700, loss is 22.071554, training accuracy is 0.000000, time 453.950 samples/sec\n",
      "epoch 1, total_step 9750, loss is 22.022804, training accuracy is 0.000000, time 455.515 samples/sec\n",
      "epoch 1, total_step 9800, loss is 21.864756, training accuracy is 0.000000, time 447.933 samples/sec\n",
      "epoch 1, total_step 9850, loss is 21.901611, training accuracy is 0.000000, time 450.600 samples/sec\n",
      "epoch 1, total_step 9900, loss is 22.423992, training accuracy is 0.000000, time 441.745 samples/sec\n",
      "epoch 1, total_step 9950, loss is 20.886929, training accuracy is 0.000000, time 447.409 samples/sec\n",
      "epoch 1, total_step 10000, loss is 21.758429, training accuracy is 0.000000, time 447.941 samples/sec\n",
      "\n",
      "Iteration 10000 testing...\n",
      "thresholds max: 1.08 <=> min: 0.01\n",
      "total time 12.532s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.692+-0.017\n",
      "Validation rate: 0.00000+-0.00000 @ FAR=0.00000\n",
      "fpr and tpr: 0.996 0.997\n",
      "Area Under Curve (AUC): 0.693\n",
      "epoch 1, total_step 10050, loss is 21.774448, training accuracy is 0.000000, time 457.014 samples/sec\n",
      "epoch 1, total_step 10100, loss is 21.993448, training accuracy is 0.000000, time 447.248 samples/sec\n",
      "epoch 1, total_step 10150, loss is 22.179211, training accuracy is 0.000000, time 441.460 samples/sec\n",
      "epoch 1, total_step 10200, loss is 21.869162, training accuracy is 0.000000, time 449.688 samples/sec\n",
      "epoch 1, total_step 10250, loss is 22.002352, training accuracy is 0.000000, time 444.737 samples/sec\n",
      "epoch 1, total_step 10300, loss is 22.452003, training accuracy is 0.000000, time 447.811 samples/sec\n",
      "epoch 1, total_step 10350, loss is 22.037882, training accuracy is 0.000000, time 454.816 samples/sec\n",
      "epoch 1, total_step 10400, loss is 21.212753, training accuracy is 0.000000, time 440.313 samples/sec\n",
      "epoch 1, total_step 10450, loss is 21.988401, training accuracy is 0.000000, time 457.517 samples/sec\n",
      "epoch 1, total_step 10500, loss is 22.151207, training accuracy is 0.000000, time 455.984 samples/sec\n",
      "\n",
      "Iteration 10500 testing...\n",
      "thresholds max: 1.08 <=> min: 0.01\n",
      "total time 12.502s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.655+-0.013\n",
      "Validation rate: 0.00000+-0.00000 @ FAR=0.00000\n",
      "fpr and tpr: 0.996 0.997\n",
      "Area Under Curve (AUC): 0.656\n",
      "epoch 1, total_step 10550, loss is 21.269020, training accuracy is 0.000000, time 450.004 samples/sec\n",
      "epoch 1, total_step 10600, loss is 21.684011, training accuracy is 0.000000, time 444.458 samples/sec\n",
      "epoch 1, total_step 10650, loss is 22.751236, training accuracy is 0.000000, time 456.982 samples/sec\n",
      "epoch 1, total_step 10700, loss is 21.734928, training accuracy is 0.000000, time 455.525 samples/sec\n",
      "epoch 1, total_step 10750, loss is 22.099504, training accuracy is 0.000000, time 445.930 samples/sec\n",
      "epoch 1, total_step 10800, loss is 22.058996, training accuracy is 0.000000, time 435.155 samples/sec\n",
      "epoch 1, total_step 10850, loss is 21.942310, training accuracy is 0.000000, time 448.232 samples/sec\n",
      "epoch 1, total_step 10900, loss is 21.094421, training accuracy is 0.000000, time 457.517 samples/sec\n",
      "End of epoch 1\n",
      "epoch:2, lr:0.1\n",
      "epoch 2, total_step 10950, loss is 21.939032, training accuracy is 0.000000, time 448.775 samples/sec\n",
      "epoch 2, total_step 11000, loss is 21.687443, training accuracy is 0.000000, time 450.922 samples/sec\n",
      "\n",
      "Iteration 11000 testing...\n",
      "thresholds max: 1.08 <=> min: 0.01\n",
      "total time 12.299s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.617+-0.012\n",
      "Validation rate: 0.00000+-0.00000 @ FAR=0.00000\n",
      "fpr and tpr: 0.997 0.997\n",
      "Area Under Curve (AUC): 0.617\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-90325b9cefa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m#             sess.run([train_op, total_loss, inference_loss, regularization_losses, inc_global_step_op, Accuracy_Op],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m#                      feed_dict=feed_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_batch_size = 90\n",
    "test_batch_size = 100\n",
    "eval_datasets = ['lfw']\n",
    "eval_db_path = '/workspace/dataset/faces_webface_112x112/'\n",
    "eval_nrof_folds = 10\n",
    "tfrecords_file_path = '/workspace/dataset/faces_webface_112x112/tfrecords/'\n",
    "summary_path = '/workspace/output/summary'\n",
    "ckpt_path = '/workspace/output/ckpt'\n",
    "pretrained_model = False\n",
    "log_file_path = '/workspace/output/logs'\n",
    "ckpt_best_path = '/workspace/output/ckpt_best'\n",
    "saver_maxkeep = 50\n",
    "summary_interval = 400\n",
    "ckpt_interval = 200\n",
    "validate_interval = 500\n",
    "show_info_interval = 50\n",
    "log_device_mapping = False\n",
    "log_histograms = False\n",
    "prelogits_norm_loss_factor = 2e-5\n",
    "prelogits_norm_p = 1.0\n",
    "max_epoch = 12\n",
    "image_size = [112, 112]\n",
    "embedding_size = 128\n",
    "\n",
    "lr_schedule = [4, 7, 9, 11]\n",
    "values=[0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "# prepare validate datasets\n",
    "ver_list = []\n",
    "ver_name_list = []\n",
    "for db in eval_datasets:\n",
    "    print('begin db %s convert.' % db)\n",
    "    data_set = load_data(db, image_size, eval_db_path)\n",
    "    ver_list.append(data_set)\n",
    "    ver_name_list.append(db)\n",
    "\n",
    "# output file path\n",
    "if not os.path.exists(log_file_path):\n",
    "    os.makedirs(log_file_path)\n",
    "if not os.path.exists(ckpt_best_path):\n",
    "    os.makedirs(ckpt_best_path)\n",
    "if not os.path.exists(ckpt_path):\n",
    "    os.makedirs(ckpt_path)\n",
    "# create log dir\n",
    "subdir = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "log_dir = os.path.join(os.path.expanduser(log_file_path), subdir)\n",
    "if not os.path.isdir(log_dir):  # Create the log directory if it doesn't exist\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "# g = my_generator_wrapper()\n",
    "iterator, next_element, g_sess = my_generator()\n",
    "\n",
    "# epoch = -1\n",
    "count = 0\n",
    "total_accuracy = {}\n",
    "for i in range(max_epoch):\n",
    "    # 调整学习率\n",
    "    _, lr = learning_rate_schedule(i,model.optimizer.lr,lr_schedule,values)\n",
    "    print('epoch:{}, lr:{}'.format(i, lr))\n",
    "    # 初始化迭代器\n",
    "    g_sess.run(iterator.initializer)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            x, y = g_sess.run(next_element)\n",
    "            \n",
    "            images_train,labels_train = ([x,y],y)\n",
    "\n",
    "            start = time.time()\n",
    "#             _, total_loss_val, inference_loss_val, reg_loss_val, _, acc_val = \\\n",
    "#             sess.run([train_op, total_loss, inference_loss, regularization_losses, inc_global_step_op, Accuracy_Op],\n",
    "#                      feed_dict=feed_dict)\n",
    "            loss, accuracy = model.train_on_batch(images_train, labels_train)\n",
    "    \n",
    "            end = time.time()\n",
    "            pre_sec = train_batch_size/(end - start)\n",
    "\n",
    "            count += 1\n",
    "            # print training information\n",
    "#             print(i, count, loss, accuracy, pre_sec)\n",
    "            if count > 0 and count % show_info_interval == 0:\n",
    "#                 print('epoch %d, total_step %d, total loss is %.2f , inference loss is %.2f, reg_loss is %.2f, training accuracy is %.6f, time %.3f samples/sec' %\n",
    "#                       (i, count, total_loss_val, inference_loss_val, np.sum(reg_loss_val), acc_val, pre_sec))\n",
    "                print('epoch %d, total_step %d, loss is %.6f, training accuracy is %.6f, time %.3f samples/sec' %\n",
    "                      (i, count, loss, np.mean(accuracy), pre_sec))\n",
    "\n",
    "            # save summary\n",
    "#             if count > 0 and count % summary_interval == 0:\n",
    "#                 feed_dict = {inputs: images_train, labels: labels_train, phase_train_placeholder: True}\n",
    "#                 summary_op_val = sess.run(summary_op, feed_dict=feed_dict)\n",
    "#                 summary.add_summary(summary_op_val, count)\n",
    "\n",
    "            # save ckpt files\n",
    "            if count > 0 and count % ckpt_interval == 0:\n",
    "#                 filename = 'MobileFaceNet_iter_{:d}'.format(count) + '.ckpt'\n",
    "                EMAer.apply_ema_weights()\n",
    "                filename = 'MobileFaceNet_iter_{:d}'.format(count) + '.h5'\n",
    "                filename = os.path.join(ckpt_path, filename)\n",
    "                val_model.save(filename)\n",
    "                \n",
    "                EMAer.reset_old_weights()\n",
    "\n",
    "            # validate\n",
    "            if count > 0 and count % validate_interval == 0:\n",
    "                print('\\nIteration', count, 'testing...')\n",
    "                \n",
    "                EMAer.apply_ema_weights()\n",
    "                \n",
    "                for db_index in range(len(ver_list)):\n",
    "                    start_time = time.time()\n",
    "                    data_sets, issame_list = ver_list[db_index]\n",
    "                    emb_array = np.zeros((data_sets.shape[0], embedding_size))\n",
    "                    nrof_batches = data_sets.shape[0] // test_batch_size\n",
    "                    for index in range(nrof_batches): # actual is same multiply 2, test data total\n",
    "                        start_index = index * test_batch_size\n",
    "                        end_index = min((index + 1) * test_batch_size, data_sets.shape[0])\n",
    "\n",
    "#                         feed_dict = {inputs: data_sets[start_index:end_index, ...], phase_train_placeholder: False}\n",
    "#                         emb_array[start_index:end_index, :] = sess.run(embeddings, feed_dict=feed_dict)\n",
    "                        emb_array[start_index:end_index, :] = val_model.predict(data_sets[start_index:end_index, ...])\n",
    "\n",
    "                    tpr, fpr, accuracy, val, val_std, far = evaluate(emb_array, issame_list, nrof_folds=eval_nrof_folds)\n",
    "                    duration = time.time() - start_time\n",
    "\n",
    "                    print(\"total time %.3fs to evaluate %d images of %s\" % (duration, data_sets.shape[0], ver_name_list[db_index]))\n",
    "                    print('Accuracy: %1.3f+-%1.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "                    print('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far))\n",
    "                    print('fpr and tpr: %1.3f %1.3f' % (np.mean(fpr, 0), np.mean(tpr, 0)))\n",
    "\n",
    "                    auc = metrics.auc(fpr, tpr)\n",
    "                    print('Area Under Curve (AUC): %1.3f' % auc)\n",
    "#                     eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\n",
    "#                     print('Equal Error Rate (EER): %1.3f\\n' % eer)\n",
    "\n",
    "                    with open(os.path.join(log_dir, '{}_result.txt'.format(ver_name_list[db_index])), 'at') as f:\n",
    "                        f.write('%d\\t%.5f\\t%.5f\\n' % (count, np.mean(accuracy), val))\n",
    "\n",
    "                    if ver_name_list == 'lfw' and np.mean(accuracy) > 0.992:\n",
    "                        print('best accuracy is %.5f' % np.mean(accuracy))\n",
    "                        filename = 'MobileFaceNet_iter_best_{:d}'.format(count) + '.ckpt'\n",
    "                        filename = os.path.join(ckpt_best_path, filename)\n",
    "                        saver.save(sess, filename)\n",
    "                        \n",
    "                EMAer.reset_old_weights()\n",
    "                \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of epoch %d\" % i)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png',show_shapes=True)\n",
    "plot_model(val_model, to_file='val_model.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_model.save(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_model = keras.models.Model(inputs=model.inputs[0], outputs=model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_model = keras.models.load_model(MODEL_FILE)\n",
    "img = cv2.imread('/home/cmf/workspace/Anthony_Hopkins_0001.jpg')\n",
    "img = cv2.resize(img,(112,112))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = np.asarray(img)\n",
    "img = np.reshape(img,(1,112,112,3))\n",
    "img = (img-127.5)/128\n",
    "val_model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(MODEL_FILE)\n",
    "tflite_model = converter.convert()\n",
    "open(LITE_FILE, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=LITE_FILE)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print('INPUTS: ')\n",
    "print(input_details)\n",
    "print('OUTPUTS: ')\n",
    "print(output_details)\n",
    "\n",
    "# Test model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=input_details[0]['dtype'])\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "print('output:')\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
