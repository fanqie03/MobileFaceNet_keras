{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_model(model):\n",
    "    # Check if the model is a model directory (containing a metagraph and a checkpoint file)\n",
    "    #  or if it is a protobuf file with a frozen graph\n",
    "    model_exp = os.path.expanduser(model)\n",
    "    if (os.path.isfile(model_exp)):\n",
    "        print('Model filename: %s' % model_exp)\n",
    "        with tf.gfile.FastGFile(model_exp, 'rb') as f:\n",
    "            graph_def = tf.GraphDef()\n",
    "            graph_def.ParseFromString(f.read())\n",
    "            tf.import_graph_def(graph_def, name='')\n",
    "    else:\n",
    "        print('Model directory: %s' % model_exp)\n",
    "        meta_file, ckpt_file = get_model_filenames(model_exp)\n",
    "\n",
    "        print('Metagraph file: %s' % meta_file)\n",
    "        print('Checkpoint file: %s' % ckpt_file)\n",
    "\n",
    "        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file))\n",
    "        saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n",
    "\n",
    "\n",
    "def get_model_filenames(model_dir):\n",
    "    files = os.listdir(model_dir)\n",
    "    meta_files = [s for s in files if s.endswith('.meta')]\n",
    "    if len(meta_files) == 0:\n",
    "        raise ValueError('No meta file found in the model directory (%s)' % model_dir)\n",
    "    elif len(meta_files) > 1:\n",
    "        raise ValueError('There should not be more than one meta file in the model directory (%s)' % model_dir)\n",
    "    meta_file = meta_files[0]\n",
    "    ckpt = tf.train.get_checkpoint_state(model_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        return meta_file, ckpt_file\n",
    "\n",
    "    meta_files = [s for s in files if '.ckpt' in s]\n",
    "    max_step = -1\n",
    "    for f in files:\n",
    "        step_str = re.match(r'(^model-[\\w\\- ]+.ckpt-(\\d+))', f)\n",
    "        if step_str is not None and len(step_str.groups()) >= 2:\n",
    "            step = int(step_str.groups()[1])\n",
    "            if step > max_step:\n",
    "                max_step = step\n",
    "                ckpt_file = step_str.groups()[0]\n",
    "    return meta_file, ckpt_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model filename: arch/pretrained_model/MobileFaceNet_9925_9680.pb\n",
      "WARNING:tensorflow:From <ipython-input-1-8fefdaf8ae98>:12: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    }
   ],
   "source": [
    "load_model('arch/pretrained_model/MobileFaceNet_9925_9680.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [n.name for n in tf.get_default_graph().as_graph_def().node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('pd_node.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from nets.MobileFaceNet import inference\n",
    "import pandas as pd\n",
    "\n",
    "inputs = tf.placeholder(name='img_inputs', shape=[None, 112,112, 3], dtype=tf.float32)\n",
    "labels = tf.placeholder(name='img_labels', shape=[None, ], dtype=tf.int64)\n",
    "phase_train_placeholder = tf.placeholder_with_default(tf.constant(False, dtype=tf.bool), shape=None, name='phase_train')\n",
    "# identity the input, for inference\n",
    "inputs = tf.identity(inputs, 'input')\n",
    "weight_decay=5e-5\n",
    "prelogits, net_points = inference(inputs, bottleneck_layer_size=128, phase_train=phase_train_placeholder, weight_decay=weight_decay)\n",
    "embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name='embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "embeddings input\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow mandates these.\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import namedtuple\n",
    "import functools\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "# Conv and InvResBlock namedtuple define layers of the MobileNet architecture\n",
    "# Conv defines 3x3 convolution layers\n",
    "# InvResBlock defines 3x3 depthwise convolution followed by 1x1 convolution.\n",
    "# stride is the stride of the convolution\n",
    "# depth is the number of channels or filters in a layer\n",
    "Conv = namedtuple('Conv', ['kernel', 'stride', 'depth', 'ratio'])\n",
    "DepthwiseConv = namedtuple('DepthwiseConv', ['kernel', 'stride', 'depth', 'ratio'])\n",
    "InvResBlock = namedtuple('InvResBlock', ['kernel', 'stride', 'depth', 'ratio', 'repeate'])\n",
    "\n",
    "# _CONV_DEFS specifies the MobileNet body\n",
    "_CONV_DEFS = [\n",
    "    Conv(kernel=[3, 3], stride=2, depth=64, ratio=1),\n",
    "    DepthwiseConv(kernel=[3, 3], stride=1, depth=64, ratio=1),\n",
    "\n",
    "    InvResBlock(kernel=[3, 3], stride=2, depth=64, ratio=2, repeate=5),\n",
    "    InvResBlock(kernel=[3, 3], stride=2, depth=128, ratio=4, repeate=1),\n",
    "    InvResBlock(kernel=[3, 3], stride=1, depth=128, ratio=2, repeate=6),\n",
    "    InvResBlock(kernel=[3, 3], stride=2, depth=128, ratio=4, repeate=1),\n",
    "    InvResBlock(kernel=[3, 3], stride=1, depth=128, ratio=2, repeate=2),\n",
    "\n",
    "    Conv(kernel=[1, 1], stride=1, depth=512, ratio=1),\n",
    "]\n",
    "\n",
    "def inverted_block(net, input_filters, output_filters, expand_ratio, stride, scope=None):\n",
    "    '''fundamental network struture of inverted residual block'''\n",
    "    with tf.name_scope(scope):\n",
    "        # 先升维\n",
    "        res_block = slim.conv2d(inputs=net, num_outputs=input_filters * expand_ratio, kernel_size=[1, 1])\n",
    "        # depthwise conv2d\n",
    "        res_block = slim.separable_conv2d(inputs=res_block, num_outputs=None, kernel_size=[3, 3], stride=stride, depth_multiplier=1.0, normalizer_fn=slim.batch_norm)\n",
    "\t\t# 再降维\n",
    "        res_block = slim.conv2d(inputs=res_block, num_outputs=output_filters, kernel_size=[1, 1], activation_fn=None)\n",
    "        # stride 2 blocks\n",
    "        if stride == 2:\n",
    "            return res_block\n",
    "        # stride 1 block\n",
    "        else:\n",
    "\t\t\t# 使维度相同再相加\n",
    "            if input_filters != output_filters:\n",
    "                net = slim.conv2d(inputs=net, num_outputs=output_filters, kernel_size=[1, 1], activation_fn=None)\n",
    "            return tf.add(res_block, net)\n",
    "\n",
    "def mobilenet_v2_base(inputs,\n",
    "                      final_endpoint='Conv2d_7',\n",
    "                      min_depth=8,\n",
    "                      conv_defs=None,\n",
    "                      scope=None):\n",
    "  \"\"\"Mobilenet v2.\n",
    "\n",
    "  Constructs a Mobilenet v2 network from inputs to the given final endpoint.\n",
    "\n",
    "  Args:\n",
    "    inputs: a tensor of shape [batch_size, height, width, channels].\n",
    "    final_endpoint: specifies the endpoint to construct the network up to. It\n",
    "      can be one of ['Conv2d_0', 'Conv2d_1_InvResBlock', 'Conv2d_2_InvResBlock',\n",
    "      'Conv2d_3_InvResBlock', 'Conv2d_4_InvResBlock', 'Conv2d_5_InvResBlock,\n",
    "      'Conv2d_6_InvResBlock', 'Conv2d_7_InvResBlock', 'Conv2d_8'].\n",
    "    min_depth: Minimum depth value (number of channels) for all convolution ops.\n",
    "      Enforced output depth to min_depth.\n",
    "    conv_defs: A list of ConvDef namedtuples specifying the net architecture.\n",
    "    scope: Optional variable_scope.\n",
    "\n",
    "  Returns:\n",
    "    tensor_out: output tensor corresponding to the final_endpoint.\n",
    "    end_points: a set of activations for external use, for example summaries or\n",
    "                losses.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if final_endpoint is not set to one of the predefined values\n",
    "                is not allowed.\n",
    "  \"\"\"\n",
    "  depth = lambda d: max(int(d), min_depth)\n",
    "  end_points = {}\n",
    "\n",
    "  if conv_defs is None:\n",
    "    conv_defs = _CONV_DEFS\n",
    "\n",
    "  with tf.variable_scope(scope, 'MobileFaceNet', [inputs]):\n",
    "    with slim.arg_scope([slim.conv2d, slim.separable_conv2d], padding='SAME'):\n",
    "\n",
    "      net = inputs\n",
    "      for i, conv_def in enumerate(conv_defs):\n",
    "        end_point_base = 'Conv2d_%d' % i\n",
    "\n",
    "        if isinstance(conv_def, Conv):\n",
    "          end_point = end_point_base\n",
    "          net = slim.conv2d(net, depth(conv_def.depth), conv_def.kernel,\n",
    "                            stride=conv_def.stride,\n",
    "                            normalizer_fn=slim.batch_norm,\n",
    "                            scope=end_point)\n",
    "          end_points[end_point] = net\n",
    "          if end_point == final_endpoint:\n",
    "            return net, end_points\n",
    "\n",
    "        elif isinstance(conv_def, DepthwiseConv):\n",
    "            end_point = 'DepthwiseConv'\n",
    "            # depthwise conv2d\n",
    "            net = slim.separable_conv2d(inputs=net, num_outputs=None, kernel_size=conv_def.kernel, stride=conv_def.stride,\n",
    "                                        depth_multiplier=1.0, normalizer_fn=slim.batch_norm)\n",
    "            net = slim.conv2d(inputs=net, num_outputs=conv_def.depth, kernel_size=[1, 1], activation_fn=None)\n",
    "            end_points[end_point] = net\n",
    "            if end_point == final_endpoint:\n",
    "                return net, end_points\n",
    "\n",
    "        elif isinstance(conv_def, InvResBlock):\n",
    "          end_point = end_point_base + '_InvResBlock'\n",
    "          # inverted bottleneck blocks\n",
    "          input_filters = net.shape[3].value\n",
    "          # first layer needs to consider stride\n",
    "          net = inverted_block(net, input_filters, depth(conv_def.depth), conv_def.ratio, conv_def.stride, end_point+'_0')\n",
    "          for index in range(1, conv_def.repeate):\n",
    "              suffix = '_' + str(index)\n",
    "              net = inverted_block(net, input_filters, depth(conv_def.depth), conv_def.ratio, 1, end_point+suffix)\n",
    "\n",
    "          end_points[end_point] = net\n",
    "          if end_point == final_endpoint:\n",
    "            return net, end_points\n",
    "\n",
    "        else:\n",
    "          raise ValueError('Unknown convolution type %s for layer %d'\n",
    "                           % (conv_def.ltype, i))\n",
    "  raise ValueError('Unknown final endpoint %s' % final_endpoint)\n",
    "\n",
    "\n",
    "def mobilenet_v2(inputs,\n",
    "                 bottleneck_layer_size=128,\n",
    "                 is_training=False,\n",
    "                 min_depth=8,\n",
    "                 conv_defs=None,\n",
    "                 spatial_squeeze=True,\n",
    "                 reuse=None,\n",
    "                 scope='MobileFaceNet',\n",
    "                 global_pool=False):\n",
    "  \"\"\"Mobilenet v2 model for classification.\n",
    "\n",
    "  Args:\n",
    "    inputs: a tensor of shape [batch_size, height, width, channels].\n",
    "    bottleneck_layer_size: number of predicted classes. If 0 or None, the logits layer\n",
    "      is omitted and the input features to the logits layer (before dropout)\n",
    "      are returned instead.\n",
    "    is_training: whether is training or not.\n",
    "    min_depth: Minimum depth value (number of channels) for all convolution ops.\n",
    "      Enforced output depth to min_depth..\n",
    "    conv_defs: A list of ConvDef namedtuples specifying the net architecture.\n",
    "    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is\n",
    "        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n",
    "    reuse: whether or not the network and its variables should be reused. To be\n",
    "      able to reuse 'scope' must be given.\n",
    "    scope: Optional variable_scope.\n",
    "    global_pool: Optional boolean flag to control the avgpooling before the\n",
    "      logits layer. If false or unset, pooling is done with a fixed window\n",
    "      that reduces default-sized inputs to 1x1, while larger inputs lead to\n",
    "      larger outputs. If true, any input size is pooled down to 1x1.\n",
    "\n",
    "  Returns:\n",
    "    net: a 2D Tensor with the logits (pre-softmax activations) if bottleneck_layer_size\n",
    "      is a non-zero integer, or the non-dropped-out input to the logits layer\n",
    "      if bottleneck_layer_size is 0 or None.\n",
    "    end_points: a dictionary from components of the network to the corresponding\n",
    "      activation.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: Input rank is invalid.\n",
    "  \"\"\"\n",
    "  input_shape = inputs.get_shape().as_list()\n",
    "  if len(input_shape) != 4:\n",
    "    raise ValueError('Invalid input tensor rank, expected 4, was: %d' %\n",
    "                     len(input_shape))\n",
    "\n",
    "  with tf.variable_scope(scope, 'MobileFaceNet', [inputs], reuse=reuse) as scope:\n",
    "    with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=is_training):\n",
    "      net, end_points = mobilenet_v2_base(inputs, scope=scope, min_depth=min_depth, conv_defs=conv_defs)\n",
    "\n",
    "      with tf.variable_scope('Logits'):\n",
    "        if global_pool:\n",
    "          # Global average pooling.\n",
    "          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')\n",
    "          end_points['global_pool'] = net\n",
    "        else:\n",
    "          # Pooling with a fixed kernel size.\n",
    "          kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n",
    "\n",
    "          # Global depthwise conv2d\n",
    "          net = slim.separable_conv2d(inputs=net, num_outputs=None, kernel_size=kernel_size, stride=1,\n",
    "                                      depth_multiplier=1.0, activation_fn=None, padding='VALID')\n",
    "          net = slim.conv2d(inputs=net, num_outputs=512, kernel_size=[1, 1], stride=1, activation_fn=None, padding='VALID')\n",
    "          end_points['GDConv'] = net\n",
    "\n",
    "        if not bottleneck_layer_size:\n",
    "          return net, end_points\n",
    "        # 1 x 1 x 1024\n",
    "        # net = slim.dropout(net, keep_prob=dropout_keep_prob, scope='Dropout_1b')\n",
    "        logits = slim.conv2d(net, bottleneck_layer_size, kernel_size=[1, 1], stride=1, activation_fn=None, scope='LinearConv1x1')\n",
    "\n",
    "        if spatial_squeeze:\n",
    "          logits = tf.squeeze(logits, [1, 2], name='SpatialSqueeze')\n",
    "      end_points['Logits'] = logits\n",
    "\n",
    "  return logits, end_points\n",
    "\n",
    "mobilenet_v2.default_image_size = 112\n",
    "\n",
    "\n",
    "def wrapped_partial(func, *args, **kwargs):\n",
    "  partial_func = functools.partial(func, *args, **kwargs)\n",
    "  functools.update_wrapper(partial_func, func)\n",
    "  return partial_func\n",
    "\n",
    "def _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n",
    "  \"\"\"Define kernel size which is automatically reduced for small input.\n",
    "\n",
    "  If the shape of the input images is unknown at graph construction time this\n",
    "  function assumes that the input images are large enough.\n",
    "\n",
    "  Args:\n",
    "    input_tensor: input tensor of size [batch_size, height, width, channels].\n",
    "    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n",
    "\n",
    "  Returns:\n",
    "    a tensor with the kernel size.\n",
    "  \"\"\"\n",
    "  shape = input_tensor.get_shape().as_list()\n",
    "  if shape[1] is None or shape[2] is None:\n",
    "    kernel_size_out = kernel_size\n",
    "  else:\n",
    "    kernel_size_out = [min(shape[1], kernel_size[0]),\n",
    "                       min(shape[2], kernel_size[1])]\n",
    "  return kernel_size_out\n",
    "\n",
    "def prelu(input, name=''):\n",
    "    alphas = tf.get_variable(name=name + 'prelu_alphas',initializer=tf.constant(0.25,dtype=tf.float32,shape=[input.get_shape()[-1]]))\n",
    "    pos = tf.nn.relu(input)\n",
    "    neg = alphas * (input - tf.abs(input)) * 0.5\n",
    "    return pos + neg\n",
    "\n",
    "def mobilenet_v2_arg_scope(is_training=True,\n",
    "                           weight_decay=0.00005,\n",
    "                           regularize_depthwise=False):\n",
    "  \"\"\"Defines the default MobilenetV2 arg scope.\n",
    "\n",
    "  Args:\n",
    "    is_training: Whether or not we're training the model.\n",
    "    weight_decay: The weight decay to use for regularizing the model.\n",
    "    regularize_depthwise: Whether or not apply regularization on depthwise.\n",
    "\n",
    "  Returns:\n",
    "    An `arg_scope` to use for the mobilenet v2 model.\n",
    "  \"\"\"\n",
    "  batch_norm_params = {\n",
    "      'is_training': is_training,\n",
    "      'center': True,\n",
    "      'scale': True,\n",
    "      'fused': True,\n",
    "      'decay': 0.995,\n",
    "      'epsilon': 2e-5,\n",
    "      # force in-place updates of mean and variance estimates\n",
    "      'updates_collections': None,\n",
    "      # Moving averages ends up in the trainable variables collection\n",
    "      'variables_collections': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n",
    "  }\n",
    "\n",
    "  # Set weight_decay for weights in Conv and InvResBlock layers.\n",
    "  #weights_init = tf.truncated_normal_initializer(stddev=stddev)\n",
    "  weights_init = tf.contrib.layers.xavier_initializer(uniform=False)\n",
    "  regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n",
    "  if regularize_depthwise:\n",
    "    depthwise_regularizer = regularizer\n",
    "  else:\n",
    "    depthwise_regularizer = None\n",
    "  with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n",
    "                      weights_initializer=weights_init,\n",
    "                      activation_fn=prelu, normalizer_fn=slim.batch_norm): #tf.keras.layers.PReLU\n",
    "    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n",
    "      with slim.arg_scope([slim.conv2d], weights_regularizer=regularizer):\n",
    "        with slim.arg_scope([slim.separable_conv2d],\n",
    "                            weights_regularizer=depthwise_regularizer) as sc:\n",
    "          return sc\n",
    "\n",
    "def inference(images, bottleneck_layer_size=128, phase_train=False,\n",
    "              weight_decay=0.00005, reuse=False):\n",
    "    '''build a mobilenet_v2 graph to training or inference.\n",
    "\n",
    "    Args:\n",
    "        images: a tensor of shape [batch_size, height, width, channels].\n",
    "        bottleneck_layer_size: number of predicted classes. If 0 or None, the logits layer\n",
    "          is omitted and the input features to the logits layer (before dropout)\n",
    "          are returned instead.\n",
    "        phase_train: Whether or not we're training the model.\n",
    "        weight_decay: The weight decay to use for regularizing the model.\n",
    "        reuse: whether or not the network and its variables should be reused. To be\n",
    "          able to reuse 'scope' must be given.\n",
    "\n",
    "    Returns:\n",
    "        net: a 2D Tensor with the logits (pre-softmax activations) if bottleneck_layer_size\n",
    "          is a non-zero integer, or the non-dropped-out input to the logits layer\n",
    "          if bottleneck_layer_size is 0 or None.\n",
    "        end_points: a dictionary from components of the network to the corresponding\n",
    "          activation.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Input rank is invalid.\n",
    "    '''\n",
    "    arg_scope = mobilenet_v2_arg_scope(is_training=phase_train, weight_decay=weight_decay)\n",
    "    with slim.arg_scope(arg_scope):\n",
    "        return mobilenet_v2(images, bottleneck_layer_size=bottleneck_layer_size, is_training=phase_train, reuse=reuse)\n",
    "    \n",
    "inputs = tf.placeholder(name='img_inputs', shape=(None,112,112,3), dtype=tf.float32)\n",
    "labels = tf.placeholder(name='img_labels', shape=[None, ], dtype=tf.int64)\n",
    "inputs = tf.identity(inputs, 'input')\n",
    "\n",
    "phase_train_placeholder = tf.placeholder_with_default(tf.constant(False, dtype=tf.bool), shape=None, name='phase_train')\n",
    "\n",
    "prelogits, net_points = inference(inputs, bottleneck_layer_size=128, phase_train=phase_train_placeholder, weight_decay=5e-5)\n",
    "\n",
    "embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name='embeddings')\n",
    "\n",
    "outputs_name ='embeddings'\n",
    "inputs_name  = 'input'\n",
    "print(outputs_name,inputs_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [x.name for x in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(n)\n",
    "df.to_csv('trainable_variables.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 模型保存\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.get_default_session()\n",
    "\n",
    "if not sess:\n",
    "    sess = tf.Session()\n",
    "    \n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver.save(sess,'output/tmp/model.ckpt')\n",
    "\n",
    "tf.train.write_graph(sess.graph_def, 'output/tmp/', 'model.pb', as_text=True)\n",
    "\n",
    "sess.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-04 21:07:01.022764: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2019-06-04 21:07:01.053758: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3499605000 Hz\n",
      "2019-06-04 21:07:01.055602: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5605204ea940 executing computations on platform Host. Devices:\n",
      "2019-06-04 21:07:01.055658: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-06-04 21:07:01.169245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-06-04 21:07:01.169880: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5605204ea220 executing computations on platform CUDA. Devices:\n",
      "2019-06-04 21:07:01.169930: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2019-06-04 21:07:01.170346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6575\n",
      "pciBusID: 0000:42:00.0\n",
      "totalMemory: 10.91GiB freeMemory: 336.19MiB\n",
      "2019-06-04 21:07:01.170373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2019-06-04 21:07:01.171314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-06-04 21:07:01.171337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2019-06-04 21:07:01.171347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2019-06-04 21:07:01.171521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 111 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:42:00.0, compute capability: 6.1)\n",
      "Model directory: output/tmp\n",
      "Metagraph file: model.ckpt.meta\n",
      "Checkpoint file: model.ckpt\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "WARNING:tensorflow:From utils/freeze_graph.py:112: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "2921 ops in the final graph: output/tmp/freeze_model.pb\n"
     ]
    }
   ],
   "source": [
    "!python utils/freeze_graph.py output/tmp output/tmp/freeze_model.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/tools/freeze_graph.py:249: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "2019-06-04 21:44:11.024221: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2019-06-04 21:44:11.049784: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3499605000 Hz\n",
      "2019-06-04 21:44:11.051828: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55c56fda1a80 executing computations on platform Host. Devices:\n",
      "2019-06-04 21:44:11.051861: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-06-04 21:44:11.138289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-06-04 21:44:11.138988: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55c56ea1e8c0 executing computations on platform CUDA. Devices:\n",
      "2019-06-04 21:44:11.139060: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2019-06-04 21:44:11.139469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6575\n",
      "pciBusID: 0000:42:00.0\n",
      "totalMemory: 10.91GiB freeMemory: 261.31MiB\n",
      "2019-06-04 21:44:11.139495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2019-06-04 21:44:11.140427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-06-04 21:44:11.140446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2019-06-04 21:44:11.140457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2019-06-04 21:44:11.140614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 36 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:42:00.0, compute capability: 6.1)\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/tools/freeze_graph.py:232: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n"
     ]
    }
   ],
   "source": [
    "# 使用freeze_graph进行冻结\n",
    "!freeze_graph --input_graph output/tmp/model.pb \\\n",
    "--input_checkpoint output/tmp/model.ckpt \\\n",
    "--output_graph output/tmp/freeze_model.pb \\\n",
    "--output_node_names embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-04 21:44:21.234111: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2019-06-04 21:44:21.261850: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3499605000 Hz\n",
      "2019-06-04 21:44:21.264274: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55b614afb8a0 executing computations on platform Host. Devices:\n",
      "2019-06-04 21:44:21.264335: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-06-04 21:44:21.359060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-06-04 21:44:21.359856: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55b614b1d7c0 executing computations on platform CUDA. Devices:\n",
      "2019-06-04 21:44:21.359935: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2019-06-04 21:44:21.360359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6575\n",
      "pciBusID: 0000:42:00.0\n",
      "totalMemory: 10.91GiB freeMemory: 261.31MiB\n",
      "2019-06-04 21:44:21.360385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2019-06-04 21:44:21.361309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-06-04 21:44:21.361328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2019-06-04 21:44:21.361339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2019-06-04 21:44:21.361520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 36 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:42:00.0, compute capability: 6.1)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 426, in import_graph_def\n",
      "    graph._c_graph, serialized, options)  # pylint: disable=protected-access\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Input 0 of node MobileFaceNet/MobileFaceNet/Conv2d_0/BatchNorm/cond_1/AssignMovingAvg/Switch was passed float from MobileFaceNet/Conv2d_0/BatchNorm/moving_mean:0 incompatible with expected float_ref.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cmf/anaconda3/bin/tflite_convert\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 442, in main\n",
      "    app.run(main=run_main, argv=sys.argv[:1])\n",
      "  File \"/home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
      "    _sys.exit(main(argv))\n",
      "  File \"/home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 438, in run_main\n",
      "    _convert_model(tflite_flags)\n",
      "  File \"/home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 122, in _convert_model\n",
      "    converter = _get_toco_converter(flags)\n",
      "  File \"/home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 109, in _get_toco_converter\n",
      "    return converter_fn(**converter_kwargs)\n",
      "  File \"/home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 274, in from_frozen_graph\n",
      "    _import_graph_def(graph_def, name=\"\")\n",
      "  File \"/home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 430, in import_graph_def\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Input 0 of node MobileFaceNet/MobileFaceNet/Conv2d_0/BatchNorm/cond_1/AssignMovingAvg/Switch was passed float from MobileFaceNet/Conv2d_0/BatchNorm/moving_mean:0 incompatible with expected float_ref.\n"
     ]
    }
   ],
   "source": [
    "# 使用tflite_convert进行转换\n",
    "!tflite_convert --output_file output/tmp/freeze_model.tflite \\\n",
    "--graph_def_file output/tmp/freeze_model.pb \\\n",
    "--output_format TFLITE \\\n",
    "--inference_type FLOAT \\\n",
    "--inference_input_type FLOAT \\\n",
    "--output_arrays embeddings \\\n",
    "--input_arrays input \\\n",
    "--input_shapes 1,112,112,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=LITE_FILE)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print('INPUTS: ')\n",
    "print(input_details)\n",
    "print('OUTPUTS: ')\n",
    "print(output_details)\n",
    "\n",
    "# Test model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=input_details[0]['dtype'])\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "print('output:')\n",
    "print(output_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
