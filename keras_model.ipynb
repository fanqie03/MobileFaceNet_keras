{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "class Evaluete(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.validation_data = None\n",
    "        self.model = None\n",
    "        self.count=0\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        \"\"\"A backwards compatibility alias for `on_train_batch_begin`.\"\"\"\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        \"\"\"A backwards compatibility alias for `on_train_batch_end`.\"\"\"\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \"\"\"Called at the start of an epoch.\n",
    "        Subclasses should override for any actions to run. This function should only\n",
    "        be called during train mode.\n",
    "        # Arguments\n",
    "            epoch: integer, index of epoch.\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Called at the end of an epoch.\n",
    "        Subclasses should override for any actions to run. This function should only\n",
    "        be called during train mode.\n",
    "        # Arguments\n",
    "            epoch: integer, index of epoch.\n",
    "            logs: dict, metric results for this training epoch, and for the\n",
    "                validation epoch if validation is performed. Validation result keys\n",
    "                are prefixed with `val_`.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        \"\"\"Called at the beginning of a training batch in `fit` methods.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, has keys `batch` and `size` representing the current\n",
    "                batch number and the size of the batch.\n",
    "        \"\"\"\n",
    "        # For backwards compatibility\n",
    "        self.on_batch_begin(batch, logs=logs)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        \"\"\"Called at the end of a training batch in `fit` methods.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, metric results for this batch.\n",
    "        \"\"\"\n",
    "        # For backwards compatibility\n",
    "        self.on_batch_end(batch, logs=logs)\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        \"\"\"Called at the beginning of a batch in `evaluate` methods.\n",
    "        Also called at the beginning of a validation batch in the `fit` methods,\n",
    "        if validation data is provided.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, has keys `batch` and `size` representing the current\n",
    "                batch number and the size of the batch.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        \"\"\"Called at the end of a batch in `evaluate` methods.\n",
    "        Also called at the end of a validation batch in the `fit` methods,\n",
    "        if validation data is provided.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, metric results for this batch.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        \"\"\"Called at the beginning of a batch in `predict` methods.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, has keys `batch` and `size` representing the current\n",
    "                batch number and the size of the batch.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        \"\"\"Called at the end of a batch in `predict` methods.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, metric results for this batch.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        \"\"\"Called at the beginning of training.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        \"\"\"Called at the end of training.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        \"\"\"Called at the beginning of evaluation or validation.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        \"\"\"Called at the end of evaluation or validation.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        \"\"\"Called at the beginning of prediction.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        \"\"\"Called at the end of prediction.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import sys\n",
    " \n",
    "f=open('a.txt','w')\n",
    "old=sys.stdout #将当前系统输出储存到临时变量\n",
    "sys.stdout=f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import keras\n",
    "from keras.callbacks import *\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import keras.backend as K\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.data_process import parse_function, load_data\n",
    "# from losses.face_losses import arcface_loss\n",
    "# from nets.MobileFaceNet import inference\n",
    "from verification import evaluate\n",
    "from scipy.optimize import brentq\n",
    "from scipy import interpolate\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "MODEL_FILE = 'MobileFaceNet.h5'\n",
    "LITE_FILE  = 'MobileFaceNet.tflite'\n",
    "\n",
    "NUM_PICTURES=490623\n",
    "NUM_CLASSES=10572\n",
    "BATCH_SIZE=90\n",
    "TARGET_SIZE=(112,112)\n",
    "TFRECORD_PATH='/workspace/dataset/faces_webface_112x112/tfrecords/tran.tfrecords'\n",
    "\n",
    "def my_generator(tfrecord_path=TFRECORD_PATH,batch_size=BATCH_SIZE,out_num=NUM_CLASSES):\n",
    "    \"\"\"自定义generator\n",
    "    \n",
    "    # Argument\n",
    "        tfrecord_path:\n",
    "        batch_size\n",
    "        out_num: 类别数量，用于生成onehot\n",
    "        \n",
    "    # Return\n",
    "    \n",
    "    \"\"\"\n",
    "    def parse_function(example_proto):\n",
    "        features = {'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                    'label': tf.FixedLenFeature([], tf.int64)}\n",
    "        features = tf.parse_single_example(example_proto, features)\n",
    "        # You can do more image distortion here for training data\n",
    "        img = tf.image.decode_jpeg(features['image_raw'])\n",
    "        img = tf.reshape(img, shape=(112, 112, 3))\n",
    "\n",
    "        #img = tf.py_func(random_rotate_image, [img], tf.uint8)\n",
    "        img = tf.cast(img, dtype=tf.float32)\n",
    "        img = tf.subtract(img, 127.5)\n",
    "        img = tf.multiply(img,  0.0078125)\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        label = tf.cast(features['label'], tf.int64)\n",
    "#         label = tf.one_hot(label,out_num)\n",
    "#         label = tf.reshape(label,(-1,))\n",
    "#         one_hot = tf.one_hot(label,out_num)\n",
    "        return (img, label)\n",
    "    \n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess = tf.Session(config=config)\n",
    "#     sess = K.get_session()\n",
    "    # training datasets api config\n",
    "    tfrecords_f = os.path.join(tfrecord_path)\n",
    "    dataset = tf.data.TFRecordDataset(tfrecords_f)\n",
    "    dataset = dataset.map(parse_function)\n",
    "#     dataset = dataset.shuffle(buffer_size=5000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    # begin iteration\n",
    "    while(True):\n",
    "        sess.run(iterator.initializer)\n",
    "        while True:\n",
    "            try:\n",
    "                images, labels = sess.run(next_element)\n",
    "                for i in range(len(images)):\n",
    "                    images[i,...] = cv2.cvtColor(images[i, ...], cv2.COLOR_RGB2BGR)\n",
    "                yield images,labels\n",
    "            except tf.errors.OutOfRangeError:\n",
    "#                 print(\"End of dataset\")\n",
    "                break\n",
    "\n",
    "def my_generator_wrapper():\n",
    "    for image,label in my_generator():\n",
    "        yield ([image,label],label)\n",
    "#         yield image,label\n",
    "        \n",
    "def test_generator():\n",
    "    a = my_generator_wrapper()\n",
    "    for i in tqdm(range(50000)):\n",
    "        a.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dropout,PReLU,Layer\n",
    "from keras.layers import Activation, BatchNormalization, add, Reshape,DepthwiseConv2D\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.activations import relu\n",
    "from keras.initializers import Constant\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras import backend as K\n",
    "import math\n",
    "\n",
    "weight_decay = 5e-5  # l2正则化decay常量\n",
    "\n",
    "\n",
    "batch_norm_params = {\n",
    "    'center': True,\n",
    "    'scale': True,\n",
    "    'momentum': 0.995,\n",
    "    'epsilon': 2e-5,\n",
    "}\n",
    "\n",
    "\n",
    "def flow_wrapper(flow):\n",
    "    \"\"\"自定义wrapper，将(x,y)变成([x,y],y)\"\"\"\n",
    "    while True:\n",
    "        x,y = flow.next()\n",
    "        yield ([x,y],y)\n",
    "\n",
    "def prelu(input, name=''):\n",
    "    \"\"\"自定义prelu\"\"\"\n",
    "    alphas = K.variable(K.constant(0.25,dtype=tf.float32,shape=[input.get_shape()[-1]]),name=name + 'prelu_alphas')\n",
    "    pos = K.relu(input)\n",
    "    neg = alphas * (input - K.abs(input)) * 0.5\n",
    "    return pos + neg\n",
    "\n",
    "cval = Constant(0.25)  # prelu α 初始常量\n",
    "\n",
    "def _conv_block(inputs, filters, kernel, strides):\n",
    "    \"\"\"Convolution Block\n",
    "    This function defines a 2D convolution operation with BN and relu6.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        strides: An integer or tuple/list of 2 integers,\n",
    "            specifying the strides of the convolution along the width and height.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    x = Conv2D(filters, kernel, padding='same', strides=strides, kernel_initializer='glorot_normal',kernel_regularizer=l2(weight_decay))(inputs)\n",
    "    x = BatchNormalization(axis=channel_axis,**batch_norm_params)(x)\n",
    "    x = PReLU(cval)(x)\n",
    "#     x = Activation(relu)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _bottleneck(inputs, filters, kernel, t, s, r=False):\n",
    "    \"\"\"Bottleneck\n",
    "    This function defines a basic bottleneck structure.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        r: Boolean, Whether to use the residuals.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    tchannel = K.int_shape(inputs)[channel_axis] * t\n",
    "\n",
    "    x = _conv_block(inputs, tchannel, (1, 1), (1, 1))\n",
    "\n",
    "    x = DepthwiseConv2D(kernel, strides=(s, s), depth_multiplier=1, padding='same', kernel_initializer='glorot_normal')(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = PReLU(cval)(x)\n",
    "\n",
    "    x = Conv2D(filters, (1, 1), strides=(1, 1), padding='same', kernel_initializer='glorot_normal',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = BatchNormalization(axis=channel_axis,**batch_norm_params)(x)\n",
    "\n",
    "    if r:\n",
    "        x = add([x, inputs])\n",
    "    return x\n",
    "\n",
    "\n",
    "def _inverted_residual_block(inputs, filters, kernel, t, strides, n):\n",
    "    \"\"\"Inverted Residual Block\n",
    "    This function defines a sequence of 1 or more identical layers.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        n: Integer, layer repeat times.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    x = _bottleneck(inputs, filters, kernel, t, strides)\n",
    "\n",
    "    for i in range(1, n):\n",
    "        x = _bottleneck(x, filters, kernel, t, 1, True)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class ArcFace(Layer):\n",
    "    \"\"\"改进的softmax，得出的结果再与真是结果之间求交叉熵\"\"\"\n",
    "    def __init__(self, n_classes=10, s=64.0, m=0.50, regularizer=None, **kwargs):\n",
    "        super(ArcFace, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcFace, self).build(input_shape[0])\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                shape=(input_shape[0][-1], self.n_classes),\n",
    "                                initializer='glorot_uniform',\n",
    "                                trainable=True,\n",
    "                                regularizer=self.regularizer)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x, y = inputs # x为embeddings，y为labels\n",
    "#         c = K.shape(x)[-1]  # 特征维度\n",
    "#         # 1. normalize feature\n",
    "#         x = tf.nn.l2_normalize(x, axis=1)\n",
    "#         # 2. normalize weights\n",
    "#         W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "#         # dot product\n",
    "#         # 全连接层，x的结构为（None，128）w的结构为（128，n_classes）。logits的结构为(None,n_classes)\n",
    "#         # (np.random.randn(5,128) @ np.random.randn(128,10)).shape # (5, 10)\n",
    "#         # 3. 计算xW得到预测向量y\n",
    "#         logits = x @ W\n",
    "#         # add margin\n",
    "#         # clip logits to prevent zero division when backward\n",
    "#         theta = tf.acos(K.clip(logits, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n",
    "#         target_logits = tf.cos(theta + self.m)\n",
    "#         # sin = tf.sqrt(1 - logits**2)\n",
    "#         # cos_m = tf.cos(logits)\n",
    "#         # sin_m = tf.sin(logits)\n",
    "#         # target_logits = logits * cos_m - sin * sin_m\n",
    "#         logits = logits * (1 - y) + target_logits * y\n",
    "#         # feature re-scale\n",
    "#         # 9. 对所有值乘上固定值s\n",
    "#         logits *= self.s\n",
    "#         out = tf.nn.softmax(logits)\n",
    "#         print(out)\n",
    "#         return out\n",
    "\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        embedding, labels = inputs\n",
    "        labels = tf.reshape(labels,shape=(-1,))\n",
    "        print('labels:',labels)\n",
    "        \n",
    "        out_num = self.n_classes\n",
    "        w_init=None\n",
    "        s=64.\n",
    "        m=0.5\n",
    "        \n",
    "        cos_m = tf.cos(m)\n",
    "        sin_m = tf.sin(m)\n",
    "        mm = sin_m * m  # issue 1\n",
    "        threshold = tf.cos(math.pi - m)\n",
    "        with tf.variable_scope('arcface_loss'):\n",
    "            # inputs and weights norm\n",
    "            embedding_norm = tf.norm(embedding, axis=1, keepdims=True)\n",
    "            embedding = tf.div(embedding, embedding_norm, name='norm_embedding')\n",
    "            weights = tf.get_variable(name='embedding_weights', shape=(embedding.get_shape().as_list()[-1], out_num),\n",
    "                                      initializer=w_init, dtype=tf.float32)\n",
    "            weights_norm = tf.norm(weights, axis=0, keepdims=True)\n",
    "            weights = tf.div(weights, weights_norm, name='norm_weights')\n",
    "            # cos(theta+m)\n",
    "            cos_t = tf.matmul(embedding, weights, name='cos_t')\n",
    "            cos_t2 = tf.square(cos_t, name='cos_2')\n",
    "            sin_t2 = tf.subtract(1., cos_t2, name='sin_2')\n",
    "            sin_t = tf.sqrt(sin_t2, name='sin_t')\n",
    "            cos_mt = s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')\n",
    "\n",
    "            # this condition controls the theta+m should in range [0, pi]\n",
    "            #      0<=theta+m<=pi\n",
    "            #     -m<=theta<=pi-m\n",
    "            cond_v = cos_t - threshold\n",
    "            cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)\n",
    "\n",
    "            keep_val = s*(cos_t - mm)\n",
    "            cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n",
    "            print('labels:',labels,'out_num',out_num)\n",
    "            mask = tf.one_hot(labels, depth=out_num, name='one_hot_mask')\n",
    "#             mask = tf.reshape(mask,(-1,))\n",
    "#             mask = labels\n",
    "            print(mask)\n",
    "            # mask = tf.squeeze(mask, 1)\n",
    "            inv_mask = tf.subtract(1., mask, name='inverse_mask')\n",
    "            print(inv_mask)\n",
    "            s_cos_t = tf.multiply(s, cos_t, name='scalar_cos_t')\n",
    "            print(s_cos_t)\n",
    "            logit = tf.add(tf.multiply(s_cos_t, inv_mask), tf.multiply(cos_mt_temp, mask), name='arcface_loss_output')\n",
    "            print(logit)\n",
    "#             inference_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=labels))\n",
    "            inference_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=labels)\n",
    "            print(inference_loss)\n",
    "#             inference_loss = tf.nn.softmax(logit)\n",
    "            print(inference_loss)\n",
    "        return inference_loss\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.n_classes)\n",
    "\n",
    "def MobileFaceNets(input_shape=(112,112,3), n_classes=10, k=128):\n",
    "    \"\"\"MobileFaceNets\"\"\"\n",
    "    inputs = Input(shape=input_shape) #112x112，(img-127.5)/255\n",
    "    y      = Input(shape=(1,), dtype=tf.int32)\n",
    "#     y      = Input(shape=(n_classes,))\n",
    "    x = _conv_block(inputs, 64, (3, 3), strides=(2, 2))\n",
    "    \n",
    "    # depthwise conv3x3\n",
    "    x = DepthwiseConv2D(3, strides=(1, 1), depth_multiplier=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = PReLU(cval)(x)\n",
    "#     x = Activation(relu)(x)\n",
    "    \n",
    "    # 5层bottleneck\n",
    "    x = _inverted_residual_block(x, 64, (3, 3), t=2, strides=2, n=5)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=4, strides=2, n=1)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=2, strides=1, n=6)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=4, strides=2, n=1)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=2, strides=1, n=2)\n",
    "    \n",
    "    # conv1x1\n",
    "    x = _conv_block(x, 512, (1, 1), strides=(1, 1))\n",
    "    \n",
    "    # linear GDConv7x7\n",
    "    x = DepthwiseConv2D(7, strides=(1, 1), depth_multiplier=1, padding='valid')(x)\n",
    "#     x = Dropout(0.3, name='Dropout')(x)\n",
    "    \n",
    "    x = Conv2D(k, (1, 1), padding='same',kernel_initializer='glorot_normal',kernel_regularizer=l2(1e-10))(x)\n",
    "    x = Reshape((k,))(x)\n",
    "#     x = tf.nn.l2_normalize(x, 1, 1e-10, name='embeddings')\n",
    "    \n",
    "    # x 为embeddings， y为embeddings对应的类别标签，output为\n",
    "    output = ArcFace(n_classes=n_classes, regularizer=regularizers.l2(weight_decay))([x, y])\n",
    "    \n",
    "    model = Model([inputs, y], output)\n",
    "#     plot_model(model, to_file='images/MobileNetv2.png', show_shapes=True)\n",
    "    print(model.input,model.output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "labels: Tensor(\"arc_face_1/Reshape:0\", shape=(?,), dtype=int32)\n",
      "WARNING:tensorflow:From <ipython-input-2-131896dff8a7>:189: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "labels: Tensor(\"arc_face_1/Reshape:0\", shape=(?,), dtype=int32) out_num 10572\n",
      "Tensor(\"arc_face_1/arcface_loss/one_hot_mask:0\", shape=(?, 10572), dtype=float32)\n",
      "Tensor(\"arc_face_1/arcface_loss/inverse_mask:0\", shape=(?, 10572), dtype=float32)\n",
      "Tensor(\"arc_face_1/arcface_loss/scalar_cos_t:0\", shape=(?, 10572), dtype=float32)\n",
      "Tensor(\"arc_face_1/arcface_loss/arcface_loss_output:0\", shape=(?, 10572), dtype=float32)\n",
      "Tensor(\"arc_face_1/arcface_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"arc_face_1/arcface_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(?,), dtype=float32)\n",
      "[<tf.Tensor 'input_1:0' shape=(?, 112, 112, 3) dtype=float32>, <tf.Tensor 'input_2:0' shape=(?, 1) dtype=int32>] Tensor(\"arc_face_1/arcface_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(?,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def my_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred)\n",
    "\n",
    "model = MobileFaceNets(n_classes=NUM_CLASSES)\n",
    "model.compile(optimizer=keras.optimizers.Adam(0.01,beta_1=0.9, beta_2=0.999, epsilon=0.1),\n",
    "      loss=my_loss,\n",
    "      metrics=['accuracy'])\n",
    "# model.compile(optimizer=keras.optimizers.sgd(0.1),loss=my_loss,metrics=['accuracy'])\n",
    "\n",
    "val_model = keras.models.Model(inputs=model.inputs[0], outputs=model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(52.05564, 0.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = my_generator_wrapper()\n",
    "images_train, labels_train = g.__next__()\n",
    "loss, accuracy = model.train_on_batch(images_train, labels_train)\n",
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    # 当监测值不再改善时，该回调函数将中止训练\n",
    "    # 如发现loss相比上一个epoch训练没有下降），则经过patience个epoch后停止训练。\n",
    "    EarlyStopping(monitor='loss', patience=10, verbose=1),\n",
    "    # 该回调函数将日志信息写入TensorBorad\n",
    "#     TensorBoard(log_dir='./models/logs',histogram_freq=1),\n",
    "    # 当评价指标不在提升时，减少学习率\n",
    "    # min_lr：学习率的下限\n",
    "    ReduceLROnPlateau(monitor='loss',factor=0.2,patience=3,min_lr=0.0001),\n",
    "    # 该回调函数将在每个epoch后保存模型到filepath\n",
    "    ModelCheckpoint(filepath='models/weights-autoencoder-{epoch:02d}-{loss:.2f}.h5',save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_array[start_index:end_index, :] = val_model.predict(data_sets[start_index:end_index, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin db lfw convert.\n",
      "loading bin 1000\n",
      "loading bin 2000\n",
      "loading bin 3000\n",
      "loading bin 4000\n",
      "loading bin 5000\n",
      "loading bin 6000\n",
      "loading bin 7000\n",
      "loading bin 8000\n",
      "loading bin 9000\n",
      "loading bin 10000\n",
      "loading bin 11000\n",
      "loading bin 12000\n",
      "(12000, 112, 112, 3)\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "epoch 0, total_step 50, loss is 48.333076, training accuracy is 0.000000, time 336.171 samples/sec\n",
      "epoch 0, total_step 100, loss is 47.587803, training accuracy is 0.000000, time 337.056 samples/sec\n",
      "epoch 0, total_step 150, loss is 48.535820, training accuracy is 0.000000, time 337.238 samples/sec\n",
      "epoch 0, total_step 200, loss is 47.430202, training accuracy is 0.000000, time 336.400 samples/sec\n",
      "epoch 0, total_step 250, loss is 47.383297, training accuracy is 0.000000, time 334.749 samples/sec\n",
      "epoch 0, total_step 300, loss is 47.986134, training accuracy is 0.000000, time 328.890 samples/sec\n",
      "epoch 0, total_step 350, loss is 47.489086, training accuracy is 0.000000, time 339.882 samples/sec\n",
      "epoch 0, total_step 400, loss is 46.665924, training accuracy is 0.000000, time 320.650 samples/sec\n",
      "epoch 0, total_step 450, loss is 47.136181, training accuracy is 0.000000, time 339.272 samples/sec\n",
      "epoch 0, total_step 500, loss is 47.683521, training accuracy is 0.000000, time 334.550 samples/sec\n",
      "\n",
      "Iteration 500 testing...\n",
      "thresholds max: 3.98 <=> min: 3.98\n",
      "total time 16.981s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.544+-0.012\n",
      "Validation rate: 0.01433+-0.00578 @ FAR=0.00133\n",
      "fpr and tpr: 0.014 0.044\n",
      "Area Under Curve (AUC): 0.007\n",
      "epoch 0, total_step 550, loss is 46.606857, training accuracy is 0.000000, time 336.208 samples/sec\n",
      "epoch 0, total_step 600, loss is 47.594635, training accuracy is 0.000000, time 324.834 samples/sec\n",
      "epoch 0, total_step 650, loss is 47.908897, training accuracy is 0.000000, time 338.010 samples/sec\n",
      "epoch 0, total_step 700, loss is 46.849468, training accuracy is 0.000000, time 337.114 samples/sec\n",
      "epoch 0, total_step 750, loss is 46.880188, training accuracy is 0.000000, time 336.760 samples/sec\n",
      "epoch 0, total_step 800, loss is 47.433998, training accuracy is 0.000000, time 337.051 samples/sec\n",
      "epoch 0, total_step 850, loss is 47.204269, training accuracy is 0.000000, time 332.944 samples/sec\n",
      "epoch 0, total_step 900, loss is 47.279419, training accuracy is 0.000000, time 337.952 samples/sec\n",
      "epoch 0, total_step 950, loss is 47.427513, training accuracy is 0.000000, time 316.229 samples/sec\n",
      "epoch 0, total_step 1000, loss is 46.415108, training accuracy is 0.000000, time 337.388 samples/sec\n",
      "\n",
      "Iteration 1000 testing...\n",
      "thresholds max: 3.99 <=> min: 3.91\n",
      "total time 15.856s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.506+-0.004\n",
      "Validation rate: 0.00000+-0.00000 @ FAR=0.00000\n",
      "fpr and tpr: 0.000 0.003\n",
      "Area Under Curve (AUC): 0.000\n",
      "epoch 0, total_step 1050, loss is 46.981415, training accuracy is 0.000000, time 337.229 samples/sec\n",
      "epoch 0, total_step 1100, loss is 47.713890, training accuracy is 0.000000, time 324.421 samples/sec\n",
      "epoch 0, total_step 1150, loss is 47.309128, training accuracy is 0.000000, time 338.131 samples/sec\n",
      "epoch 0, total_step 1200, loss is 47.279564, training accuracy is 0.000000, time 320.598 samples/sec\n",
      "epoch 0, total_step 1250, loss is 47.366699, training accuracy is 0.000000, time 339.511 samples/sec\n",
      "epoch 0, total_step 1300, loss is 46.330364, training accuracy is 0.000000, time 337.535 samples/sec\n",
      "epoch 0, total_step 1350, loss is 47.285469, training accuracy is 0.000000, time 336.681 samples/sec\n",
      "epoch 0, total_step 1400, loss is 46.328217, training accuracy is 0.000000, time 335.761 samples/sec\n",
      "epoch 0, total_step 1450, loss is 46.220940, training accuracy is 0.000000, time 336.821 samples/sec\n",
      "epoch 0, total_step 1500, loss is 46.752502, training accuracy is 0.000000, time 336.017 samples/sec\n",
      "\n",
      "Iteration 1500 testing...\n",
      "thresholds max: 3.99 <=> min: 3.91\n",
      "total time 15.832s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.525+-0.007\n",
      "Validation rate: 0.02000+-0.00943 @ FAR=0.00067\n",
      "fpr and tpr: 0.001 0.014\n",
      "Area Under Curve (AUC): 0.000\n",
      "epoch 0, total_step 1550, loss is 46.960381, training accuracy is 0.000000, time 339.432 samples/sec\n",
      "epoch 0, total_step 1600, loss is 47.281590, training accuracy is 0.000000, time 337.407 samples/sec\n",
      "epoch 0, total_step 1650, loss is 46.773300, training accuracy is 0.000000, time 337.622 samples/sec\n",
      "epoch 0, total_step 1700, loss is 47.033257, training accuracy is 0.000000, time 327.539 samples/sec\n",
      "epoch 0, total_step 1750, loss is 45.945904, training accuracy is 0.000000, time 334.164 samples/sec\n",
      "epoch 0, total_step 1800, loss is 47.286751, training accuracy is 0.000000, time 319.592 samples/sec\n",
      "epoch 0, total_step 1850, loss is 46.866196, training accuracy is 0.000000, time 334.806 samples/sec\n",
      "epoch 0, total_step 1900, loss is 47.033928, training accuracy is 0.000000, time 337.380 samples/sec\n",
      "epoch 0, total_step 1950, loss is 47.159088, training accuracy is 0.000000, time 333.857 samples/sec\n",
      "epoch 0, total_step 2000, loss is 46.675915, training accuracy is 0.000000, time 337.595 samples/sec\n",
      "\n",
      "Iteration 2000 testing...\n",
      "thresholds max: 3.99 <=> min: 3.91\n",
      "total time 15.854s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.508+-0.004\n",
      "Validation rate: 0.01533+-0.00846 @ FAR=0.00133\n",
      "fpr and tpr: 0.000 0.005\n",
      "Area Under Curve (AUC): 0.000\n",
      "epoch 0, total_step 2050, loss is 47.183689, training accuracy is 0.000000, time 337.707 samples/sec\n",
      "epoch 0, total_step 2100, loss is 47.328094, training accuracy is 0.000000, time 337.350 samples/sec\n",
      "epoch 0, total_step 2150, loss is 47.665215, training accuracy is 0.000000, time 337.643 samples/sec\n",
      "epoch 0, total_step 2200, loss is 46.644924, training accuracy is 0.000000, time 336.160 samples/sec\n",
      "epoch 0, total_step 2250, loss is 47.098106, training accuracy is 0.000000, time 337.719 samples/sec\n",
      "epoch 0, total_step 2300, loss is 45.746136, training accuracy is 0.000000, time 338.660 samples/sec\n",
      "epoch 0, total_step 2350, loss is 47.467403, training accuracy is 0.000000, time 335.089 samples/sec\n",
      "epoch 0, total_step 2400, loss is 46.691021, training accuracy is 0.000000, time 328.479 samples/sec\n",
      "epoch 0, total_step 2450, loss is 47.089470, training accuracy is 0.000000, time 338.795 samples/sec\n",
      "epoch 0, total_step 2500, loss is 46.497437, training accuracy is 0.000000, time 333.753 samples/sec\n",
      "\n",
      "Iteration 2500 testing...\n",
      "thresholds max: 3.99 <=> min: 3.64\n",
      "total time 15.854s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.501+-0.001\n",
      "Validation rate: 0.00000+-0.00000 @ FAR=0.00000\n",
      "fpr and tpr: 0.000 0.000\n",
      "Area Under Curve (AUC): 0.000\n",
      "epoch 0, total_step 2550, loss is 47.035267, training accuracy is 0.000000, time 332.442 samples/sec\n",
      "epoch 0, total_step 2600, loss is 46.829510, training accuracy is 0.000000, time 338.888 samples/sec\n",
      "epoch 0, total_step 2650, loss is 46.298264, training accuracy is 0.000000, time 335.312 samples/sec\n",
      "epoch 0, total_step 2700, loss is 46.892780, training accuracy is 0.000000, time 338.439 samples/sec\n",
      "epoch 0, total_step 2750, loss is 47.502460, training accuracy is 0.000000, time 338.962 samples/sec\n",
      "epoch 0, total_step 2800, loss is 47.081268, training accuracy is 0.000000, time 335.668 samples/sec\n",
      "epoch 0, total_step 2850, loss is 47.148434, training accuracy is 0.000000, time 337.889 samples/sec\n",
      "epoch 0, total_step 2900, loss is 47.433842, training accuracy is 0.000000, time 337.514 samples/sec\n",
      "epoch 0, total_step 2950, loss is 47.515041, training accuracy is 0.000000, time 335.408 samples/sec\n",
      "epoch 0, total_step 3000, loss is 47.530159, training accuracy is 0.000000, time 324.025 samples/sec\n",
      "\n",
      "Iteration 3000 testing...\n",
      "thresholds max: 3.99 <=> min: 3.64\n",
      "total time 15.760s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.501+-0.001\n",
      "Validation rate: 0.00000+-0.00000 @ FAR=0.00000\n",
      "fpr and tpr: 0.000 0.001\n",
      "Area Under Curve (AUC): 0.000\n",
      "epoch 0, total_step 3050, loss is 47.143982, training accuracy is 0.000000, time 339.892 samples/sec\n",
      "epoch 0, total_step 3100, loss is 46.575012, training accuracy is 0.000000, time 339.919 samples/sec\n",
      "epoch 0, total_step 3150, loss is 47.005787, training accuracy is 0.000000, time 332.043 samples/sec\n",
      "epoch 0, total_step 3200, loss is 46.702042, training accuracy is 0.000000, time 339.006 samples/sec\n",
      "epoch 0, total_step 3250, loss is 46.504490, training accuracy is 0.000000, time 329.580 samples/sec\n",
      "epoch 0, total_step 3300, loss is 47.447254, training accuracy is 0.000000, time 337.040 samples/sec\n",
      "epoch 0, total_step 3350, loss is 47.101810, training accuracy is 0.000000, time 337.261 samples/sec\n",
      "epoch 0, total_step 3400, loss is 47.033852, training accuracy is 0.000000, time 339.811 samples/sec\n",
      "epoch 0, total_step 3450, loss is 47.450287, training accuracy is 0.000000, time 336.514 samples/sec\n",
      "epoch 0, total_step 3500, loss is 46.873394, training accuracy is 0.000000, time 335.931 samples/sec\n",
      "\n",
      "Iteration 3500 testing...\n",
      "thresholds max: 3.99 <=> min: 3.64\n",
      "total time 15.813s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.501+-0.002\n",
      "Validation rate: 0.00000+-0.00000 @ FAR=0.00000\n",
      "fpr and tpr: 0.000 0.001\n",
      "Area Under Curve (AUC): 0.000\n",
      "epoch 0, total_step 3550, loss is 46.629715, training accuracy is 0.000000, time 337.446 samples/sec\n",
      "epoch 0, total_step 3600, loss is 46.717113, training accuracy is 0.000000, time 337.086 samples/sec\n",
      "epoch 0, total_step 3650, loss is 46.685329, training accuracy is 0.000000, time 335.433 samples/sec\n",
      "epoch 0, total_step 3700, loss is 46.539677, training accuracy is 0.000000, time 338.586 samples/sec\n",
      "epoch 0, total_step 3750, loss is 47.090080, training accuracy is 0.000000, time 336.689 samples/sec\n",
      "epoch 0, total_step 3800, loss is 46.788849, training accuracy is 0.000000, time 337.178 samples/sec\n",
      "epoch 0, total_step 3850, loss is 46.864826, training accuracy is 0.000000, time 323.523 samples/sec\n",
      "epoch 0, total_step 3900, loss is 45.081944, training accuracy is 0.000000, time 336.591 samples/sec\n",
      "epoch 0, total_step 3950, loss is 46.680252, training accuracy is 0.000000, time 335.746 samples/sec\n",
      "epoch 0, total_step 4000, loss is 46.334221, training accuracy is 0.000000, time 337.007 samples/sec\n",
      "\n",
      "Iteration 4000 testing...\n",
      "thresholds max: 3.99 <=> min: 3.64\n",
      "total time 15.625s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.502+-0.002\n",
      "Validation rate: 0.00000+-0.00000 @ FAR=0.00000\n",
      "fpr and tpr: 0.000 0.001\n",
      "Area Under Curve (AUC): 0.000\n",
      "epoch 0, total_step 4050, loss is 46.536488, training accuracy is 0.000000, time 341.589 samples/sec\n",
      "epoch 0, total_step 4100, loss is 46.958511, training accuracy is 0.000000, time 325.936 samples/sec\n",
      "epoch 0, total_step 4150, loss is 47.244698, training accuracy is 0.000000, time 333.568 samples/sec\n",
      "epoch 0, total_step 4200, loss is 46.884708, training accuracy is 0.000000, time 329.121 samples/sec\n",
      "epoch 0, total_step 4250, loss is 46.939556, training accuracy is 0.000000, time 340.673 samples/sec\n",
      "epoch 0, total_step 4300, loss is 45.039421, training accuracy is 0.000000, time 338.185 samples/sec\n",
      "epoch 0, total_step 4350, loss is 47.158459, training accuracy is 0.000000, time 333.412 samples/sec\n",
      "epoch 0, total_step 4400, loss is 46.078541, training accuracy is 0.000000, time 337.555 samples/sec\n",
      "epoch 0, total_step 4450, loss is 45.774311, training accuracy is 0.000000, time 336.072 samples/sec\n",
      "epoch 0, total_step 4500, loss is 46.841858, training accuracy is 0.000000, time 336.749 samples/sec\n",
      "\n",
      "Iteration 4500 testing...\n",
      "thresholds max: 3.99 <=> min: 3.49\n",
      "total time 15.856s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.500+-0.001\n",
      "Validation rate: 0.00000+-0.00000 @ FAR=0.00000\n",
      "fpr and tpr: 0.000 0.000\n",
      "Area Under Curve (AUC): 0.000\n",
      "epoch 0, total_step 4550, loss is 46.904247, training accuracy is 0.000000, time 336.018 samples/sec\n",
      "epoch 0, total_step 4600, loss is 47.392529, training accuracy is 0.000000, time 334.755 samples/sec\n",
      "epoch 0, total_step 4650, loss is 46.603691, training accuracy is 0.000000, time 337.396 samples/sec\n",
      "epoch 0, total_step 4700, loss is 46.983952, training accuracy is 0.000000, time 332.071 samples/sec\n",
      "epoch 0, total_step 4750, loss is 47.053616, training accuracy is 0.000000, time 339.415 samples/sec\n",
      "epoch 0, total_step 4800, loss is 45.779766, training accuracy is 0.000000, time 325.635 samples/sec\n",
      "epoch 0, total_step 4850, loss is 46.439373, training accuracy is 0.000000, time 337.274 samples/sec\n",
      "epoch 0, total_step 4900, loss is 46.722847, training accuracy is 0.000000, time 338.089 samples/sec\n",
      "epoch 0, total_step 4950, loss is 46.670765, training accuracy is 0.000000, time 325.291 samples/sec\n",
      "epoch 0, total_step 5000, loss is 46.007458, training accuracy is 0.000000, time 337.495 samples/sec\n",
      "\n",
      "Iteration 5000 testing...\n",
      "thresholds max: 3.99 <=> min: 2.32\n",
      "total time 15.750s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.500+-0.001\n",
      "Validation rate: 0.00000+-0.00000 @ FAR=0.00000\n",
      "fpr and tpr: 0.000 0.000\n",
      "Area Under Curve (AUC): 0.000\n",
      "epoch 0, total_step 5050, loss is 47.923882, training accuracy is 0.000000, time 333.180 samples/sec\n",
      "epoch 0, total_step 5100, loss is 47.137440, training accuracy is 0.000000, time 336.014 samples/sec\n",
      "epoch 0, total_step 5150, loss is 46.649067, training accuracy is 0.000000, time 340.115 samples/sec\n",
      "epoch 0, total_step 5200, loss is 46.482384, training accuracy is 0.000000, time 337.584 samples/sec\n",
      "epoch 0, total_step 5250, loss is 46.478523, training accuracy is 0.000000, time 335.855 samples/sec\n",
      "epoch 0, total_step 5300, loss is 46.172886, training accuracy is 0.000000, time 335.306 samples/sec\n",
      "epoch 0, total_step 5350, loss is 46.287590, training accuracy is 0.000000, time 335.935 samples/sec\n",
      "epoch 0, total_step 5400, loss is 47.034328, training accuracy is 0.000000, time 336.467 samples/sec\n",
      "epoch 0, total_step 5450, loss is 46.953697, training accuracy is 0.000000, time 336.926 samples/sec\n",
      "epoch 0, total_step 5500, loss is 45.547657, training accuracy is 0.000000, time 322.513 samples/sec\n",
      "\n",
      "Iteration 5500 testing...\n",
      "thresholds max: 3.99 <=> min: 2.32\n",
      "total time 15.604s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.502+-0.002\n",
      "Validation rate: 0.00000+-0.00000 @ FAR=0.00000\n",
      "fpr and tpr: 0.000 0.001\n",
      "Area Under Curve (AUC): 0.000\n",
      "epoch 0, total_step 5550, loss is 47.124355, training accuracy is 0.000000, time 339.690 samples/sec\n",
      "epoch 0, total_step 5600, loss is 47.152443, training accuracy is 0.000000, time 339.384 samples/sec\n",
      "epoch 0, total_step 5650, loss is 46.895554, training accuracy is 0.000000, time 335.513 samples/sec\n",
      "epoch 0, total_step 5700, loss is 46.081535, training accuracy is 0.000000, time 337.071 samples/sec\n",
      "epoch 0, total_step 5750, loss is 46.428413, training accuracy is 0.000000, time 324.646 samples/sec\n",
      "epoch 0, total_step 5800, loss is 46.303745, training accuracy is 0.000000, time 336.422 samples/sec\n",
      "epoch 0, total_step 5850, loss is 46.986420, training accuracy is 0.000000, time 332.423 samples/sec\n",
      "epoch 0, total_step 5900, loss is 46.824116, training accuracy is 0.000000, time 335.495 samples/sec\n",
      "epoch 0, total_step 5950, loss is 46.758148, training accuracy is 0.000000, time 336.428 samples/sec\n",
      "epoch 0, total_step 6000, loss is 47.370029, training accuracy is 0.000000, time 336.115 samples/sec\n",
      "\n",
      "Iteration 6000 testing...\n",
      "thresholds max: 3.99 <=> min: 2.32\n",
      "total time 15.829s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.501+-0.001\n",
      "Validation rate: 0.00000+-0.00000 @ FAR=0.00000\n",
      "fpr and tpr: 0.000 0.000\n",
      "Area Under Curve (AUC): 0.000\n",
      "epoch 0, total_step 6050, loss is 46.630436, training accuracy is 0.000000, time 338.586 samples/sec\n",
      "epoch 0, total_step 6100, loss is 46.368816, training accuracy is 0.000000, time 338.803 samples/sec\n",
      "epoch 0, total_step 6150, loss is 47.010227, training accuracy is 0.000000, time 339.024 samples/sec\n",
      "epoch 0, total_step 6200, loss is 46.881367, training accuracy is 0.000000, time 336.130 samples/sec\n",
      "epoch 0, total_step 6250, loss is 46.331829, training accuracy is 0.000000, time 337.057 samples/sec\n",
      "epoch 0, total_step 6300, loss is 46.733345, training accuracy is 0.000000, time 335.683 samples/sec\n",
      "epoch 0, total_step 6350, loss is 46.591030, training accuracy is 0.000000, time 332.533 samples/sec\n",
      "epoch 0, total_step 6400, loss is 46.715359, training accuracy is 0.000000, time 337.337 samples/sec\n",
      "epoch 0, total_step 6450, loss is 46.595703, training accuracy is 0.000000, time 328.670 samples/sec\n",
      "epoch 0, total_step 6500, loss is 46.368038, training accuracy is 0.000000, time 337.362 samples/sec\n",
      "\n",
      "Iteration 6500 testing...\n",
      "thresholds max: 3.99 <=> min: 0.0\n",
      "total time 15.863s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.500+-0.000\n",
      "Validation rate: 0.00000+-0.00000 @ FAR=0.00000\n",
      "fpr and tpr: 0.000 0.000\n",
      "Area Under Curve (AUC): 0.000\n",
      "epoch 0, total_step 6550, loss is 45.950611, training accuracy is 0.000000, time 333.682 samples/sec\n",
      "epoch 0, total_step 6600, loss is 46.343204, training accuracy is 0.000000, time 326.013 samples/sec\n",
      "epoch 0, total_step 6650, loss is 46.063805, training accuracy is 0.000000, time 336.388 samples/sec\n",
      "epoch 0, total_step 6700, loss is 47.439552, training accuracy is 0.000000, time 337.539 samples/sec\n",
      "epoch 0, total_step 6750, loss is 46.455025, training accuracy is 0.000000, time 334.600 samples/sec\n",
      "epoch 0, total_step 6800, loss is 46.655060, training accuracy is 0.000000, time 334.295 samples/sec\n",
      "epoch 0, total_step 6850, loss is 46.429123, training accuracy is 0.000000, time 339.513 samples/sec\n"
     ]
    }
   ],
   "source": [
    "train_batch_size = 90\n",
    "test_batch_size = 100\n",
    "eval_datasets = ['lfw']\n",
    "eval_db_path = '/workspace/dataset/faces_webface_112x112/'\n",
    "eval_nrof_folds = 10\n",
    "tfrecords_file_path = '/workspace/dataset/faces_webface_112x112/tfrecords/'\n",
    "summary_path = '/workspace/output/summary'\n",
    "ckpt_path = '/workspace/output/ckpt'\n",
    "pretrained_model = False\n",
    "log_file_path = '/workspace/output/logs'\n",
    "ckpt_best_path = '/workspace/output/ckpt_best'\n",
    "saver_maxkeep = 50\n",
    "summary_interval = 400\n",
    "ckpt_interval = 200\n",
    "validate_interval = 500\n",
    "show_info_interval = 50\n",
    "log_device_mapping = False\n",
    "log_histograms = False\n",
    "prelogits_norm_loss_factor = 2e-5\n",
    "prelogits_norm_p = 1.0\n",
    "max_epoch = 12\n",
    "image_size = [112, 112]\n",
    "embedding_size = 128\n",
    "\n",
    "# prepare validate datasets\n",
    "ver_list = []\n",
    "ver_name_list = []\n",
    "for db in eval_datasets:\n",
    "    print('begin db %s convert.' % db)\n",
    "    data_set = load_data(db, image_size, eval_db_path)\n",
    "    ver_list.append(data_set)\n",
    "    ver_name_list.append(db)\n",
    "\n",
    "# output file path\n",
    "if not os.path.exists(log_file_path):\n",
    "    os.makedirs(log_file_path)\n",
    "if not os.path.exists(ckpt_best_path):\n",
    "    os.makedirs(ckpt_best_path)\n",
    "if not os.path.exists(ckpt_path):\n",
    "    os.makedirs(ckpt_path)\n",
    "# create log dir\n",
    "subdir = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "log_dir = os.path.join(os.path.expanduser(log_file_path), subdir)\n",
    "if not os.path.isdir(log_dir):  # Create the log directory if it doesn't exist\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "g = my_generator_wrapper()\n",
    "    \n",
    "# epoch = -1\n",
    "count = 0\n",
    "total_accuracy = {}\n",
    "for i in range(max_epoch):\n",
    "#     epoch += 1\n",
    "    while True:\n",
    "        try:\n",
    "            images_train, labels_train = g.__next__()\n",
    "\n",
    "            start = time.time()\n",
    "#             _, total_loss_val, inference_loss_val, reg_loss_val, _, acc_val = \\\n",
    "#             sess.run([train_op, total_loss, inference_loss, regularization_losses, inc_global_step_op, Accuracy_Op],\n",
    "#                      feed_dict=feed_dict)\n",
    "            loss, accuracy = model.train_on_batch(images_train, labels_train)\n",
    "    \n",
    "            end = time.time()\n",
    "            pre_sec = train_batch_size/(end - start)\n",
    "\n",
    "            count += 1\n",
    "            # print training information\n",
    "            if count > 0 and count % show_info_interval == 0:\n",
    "#                 print('epoch %d, total_step %d, total loss is %.2f , inference loss is %.2f, reg_loss is %.2f, training accuracy is %.6f, time %.3f samples/sec' %\n",
    "#                       (i, count, total_loss_val, inference_loss_val, np.sum(reg_loss_val), acc_val, pre_sec))\n",
    "                print('epoch %d, total_step %d, loss is %.6f, training accuracy is %.6f, time %.3f samples/sec' %\n",
    "                      (i, count, loss, accuracy, pre_sec))\n",
    "\n",
    "            # save summary\n",
    "#             if count > 0 and count % summary_interval == 0:\n",
    "#                 feed_dict = {inputs: images_train, labels: labels_train, phase_train_placeholder: True}\n",
    "#                 summary_op_val = sess.run(summary_op, feed_dict=feed_dict)\n",
    "#                 summary.add_summary(summary_op_val, count)\n",
    "\n",
    "            # save ckpt files\n",
    "            if count > 0 and count % ckpt_interval == 0:\n",
    "#                 filename = 'MobileFaceNet_iter_{:d}'.format(count) + '.ckpt'\n",
    "                filename = 'MobileFaceNet_iter_{:d}'.format(count) + '.h5'\n",
    "                filename = os.path.join(ckpt_path, filename)\n",
    "                val_model.save(filename)\n",
    "\n",
    "            # validate\n",
    "            if count > 0 and count % validate_interval == 0:\n",
    "                print('\\nIteration', count, 'testing...')\n",
    "                for db_index in range(len(ver_list)):\n",
    "                    start_time = time.time()\n",
    "                    data_sets, issame_list = ver_list[db_index]\n",
    "                    emb_array = np.zeros((data_sets.shape[0], embedding_size))\n",
    "                    nrof_batches = data_sets.shape[0] // test_batch_size\n",
    "                    for index in range(nrof_batches): # actual is same multiply 2, test data total\n",
    "                        start_index = index * test_batch_size\n",
    "                        end_index = min((index + 1) * test_batch_size, data_sets.shape[0])\n",
    "\n",
    "#                         feed_dict = {inputs: data_sets[start_index:end_index, ...], phase_train_placeholder: False}\n",
    "#                         emb_array[start_index:end_index, :] = sess.run(embeddings, feed_dict=feed_dict)\n",
    "                        emb_array[start_index:end_index, :] = val_model.predict(data_sets[start_index:end_index, ...])\n",
    "\n",
    "                    tpr, fpr, accuracy, val, val_std, far = evaluate(emb_array, issame_list, nrof_folds=eval_nrof_folds)\n",
    "                    duration = time.time() - start_time\n",
    "\n",
    "                    print(\"total time %.3fs to evaluate %d images of %s\" % (duration, data_sets.shape[0], ver_name_list[db_index]))\n",
    "                    print('Accuracy: %1.3f+-%1.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "                    print('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far))\n",
    "                    print('fpr and tpr: %1.3f %1.3f' % (np.mean(fpr, 0), np.mean(tpr, 0)))\n",
    "\n",
    "                    auc = metrics.auc(fpr, tpr)\n",
    "                    print('Area Under Curve (AUC): %1.3f' % auc)\n",
    "#                     eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\n",
    "#                     print('Equal Error Rate (EER): %1.3f\\n' % eer)\n",
    "\n",
    "                    with open(os.path.join(log_dir, '{}_result.txt'.format(ver_name_list[db_index])), 'at') as f:\n",
    "                        f.write('%d\\t%.5f\\t%.5f\\n' % (count, np.mean(accuracy), val))\n",
    "\n",
    "                    if ver_name_list == 'lfw' and np.mean(accuracy) > 0.992:\n",
    "                        print('best accuracy is %.5f' % np.mean(accuracy))\n",
    "                        filename = 'MobileFaceNet_iter_best_{:d}'.format(count) + '.ckpt'\n",
    "                        filename = os.path.join(ckpt_best_path, filename)\n",
    "                        saver.save(sess, filename)\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of epoch %d\" % i)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 68543 images belonging to 1005 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((90, 112, 112, 3), 68543, 1005)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(img):\n",
    "    \"\"\"图片预处理，the image is substracted 127.5 and multiplied 1/128.\"\"\"\n",
    "    return (img-127.5)*0.0078125\n",
    "\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess,)\n",
    "\n",
    "flow = datagen.flow_from_directory('/workspace/dataset/face_ms1m1/',target_size=(112,112),batch_size=90,)\n",
    "\n",
    "flow.next()[0].shape,flow.n,flow.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_from_directory(path='/workspace/dataset/face_ms1m1/'):\n",
    "    \"\"\"从path中加载图片的标签和路径\"\"\"\n",
    "    p = Path(path)\n",
    "    # 结构为图片的标签和图片的绝对路径\n",
    "    rets = []\n",
    "    \n",
    "    # 图片父目录文件夹名对应的id\n",
    "    label_dict = {}\n",
    "    \n",
    "    for i,file in enumerate(p.glob('*')):\n",
    "        file_name=str(file.name)\n",
    "        label_dict[file_name]=i\n",
    "    \n",
    "    for i,file in enumerate(p.glob('*/*')):\n",
    "        # 获得图片的父目录名字\n",
    "        parent = str(file.parent.name)\n",
    "        # 图片绝对路径\n",
    "        file_path = str(file)\n",
    "        # 重新设置标签\n",
    "        index = label_dict[parent]\n",
    "        rets.append([index,file_path])\n",
    "    return rets, label_dict\n",
    "\n",
    "def my_generator(images,labels,batch_size=32,preprocess_function=lambda x:(x-127.5)*0.0078125,shuffle=False,target_size=(112,112)):\n",
    "    \"\"\"图片和标签生成器，images为路径，label为标签\"\"\"\n",
    "    \n",
    "    def load_img(imgs_path):\n",
    "        imgs = []\n",
    "        for img_path in imgs_path:\n",
    "            img = tf.image.decode_image(img_path)\n",
    "            img = tf.image.resize(img, target_size)\n",
    "            img = preprocess_function(img)\n",
    "            imgs.append(img)\n",
    "        imgs = np.asarray(imgs)    \n",
    "        return imgs\n",
    "        \n",
    "    \n",
    "    num_imgs = len(images)\n",
    "    # 52//32=1\n",
    "    num_of_steps = num_imgs//batch_size\n",
    "#     tf.image.flip_left_right()\n",
    "    while True:\n",
    "        for step in range(num_of_steps):\n",
    "            left = step*batch_size\n",
    "            right = (step+1)*batch_size\n",
    "            batch_imgs = images[left:right]\n",
    "            Image.open(batch_imgs[1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs,label_dict= load_img_from_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1005"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/workspace/dataset/face_ms1m/'\n",
    "path2 = '/workspace/dataset/face_ms1m1/'\n",
    "di = os.listdir('/workspace/dataset/face_ms1m/')\n",
    "di1 = [path1 + x for x in di]\n",
    "di2 = [path2 + x for x in di]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    !cp -r {di1[i]} {di2[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006\n"
     ]
    }
   ],
   "source": [
    "!ls -l {path1} | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=model.inputs, outputs=model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:96: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 325 variables.\n",
      "INFO:tensorflow:Converted 325 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8333408"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(MODEL_FILE)\n",
    "tflite_model = converter.convert()\n",
    "open(LITE_FILE, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS: \n",
      "[{'name': 'input_1', 'index': 160, 'shape': array([  1, 112, 112,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\n",
      "OUTPUTS: \n",
      "[{'name': 'reshape_1/Reshape', 'index': 202, 'shape': array([  1, 128], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\n",
      "output:\n",
      "[[-0.08600037  0.05214042 -0.00632014 -0.18984038 -0.25819245  0.45930105\n",
      "   0.06309387 -0.05336918  0.01776544  0.17570892  0.03209278  0.19029188\n",
      "   0.08877638  0.12179503  0.3424917  -0.0028923   0.04212724  0.11744951\n",
      "  -0.2688377  -0.0320788   0.182279   -0.09998893 -0.1303117  -0.1734086\n",
      "  -0.45946413  0.38137147  0.13438156  0.22810458 -0.01080376  0.3113878\n",
      "  -0.08299556 -0.00777123 -0.41161337 -0.14367008 -0.08051802 -0.23692316\n",
      "  -0.14595474  0.4072163  -0.19652095  0.26270148  0.25884676 -0.01005202\n",
      "   0.12291791  0.0415182   0.20192674  0.08535753  0.05900731 -0.5025201\n",
      "  -0.18770207 -0.14571914 -0.39670256  0.0708349   0.06726214 -0.19018859\n",
      "  -0.32096493 -0.11409592 -0.18103702 -0.16161959  0.01819853  0.1610372\n",
      "   0.03568053 -0.28144065  0.0764532   0.05957528  0.19033423  0.2033018\n",
      "  -0.05423108  0.44498196  0.34074464  0.00451815  0.08423764  0.30288506\n",
      "  -0.2448225   0.24951142 -0.15476753 -0.14374866 -0.05931505 -0.01745556\n",
      "   0.19685847  0.21447727 -0.15327658  0.01499852  0.1559698  -0.07102221\n",
      "   0.41383314 -0.3866267  -0.2284707   0.04490305 -0.1188985   0.14605296\n",
      "  -0.38869002  0.11600556 -0.09334408 -0.00370034 -0.4302274  -0.04600368\n",
      "   0.14603639 -0.11916718  0.2898529   0.16218635  0.26870775  0.19022101\n",
      "   0.1633618   0.30971295  0.01424661 -0.25980586 -0.24919204 -0.21968555\n",
      "   0.07027547 -0.0332697  -0.05821012 -0.21478066 -0.2729344  -0.11499254\n",
      "   0.2078399   0.40441075  0.2206167   0.3435076  -0.01374407 -0.06401148\n",
      "   0.2143121  -0.00822046 -0.35574165  0.21061812 -0.00313184 -0.33308637\n",
      "   0.08944872  0.08538473]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=LITE_FILE)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print('INPUTS: ')\n",
    "print(input_details)\n",
    "print('OUTPUTS: ')\n",
    "print(output_details)\n",
    "\n",
    "# Test model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=input_details[0]['dtype'])\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "print('output:')\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
