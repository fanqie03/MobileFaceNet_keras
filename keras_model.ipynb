{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "class Evaluete(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.validation_data = None\n",
    "        self.model = None\n",
    "        self.count=0\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        \"\"\"A backwards compatibility alias for `on_train_batch_begin`.\"\"\"\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        \"\"\"A backwards compatibility alias for `on_train_batch_end`.\"\"\"\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \"\"\"Called at the start of an epoch.\n",
    "        Subclasses should override for any actions to run. This function should only\n",
    "        be called during train mode.\n",
    "        # Arguments\n",
    "            epoch: integer, index of epoch.\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Called at the end of an epoch.\n",
    "        Subclasses should override for any actions to run. This function should only\n",
    "        be called during train mode.\n",
    "        # Arguments\n",
    "            epoch: integer, index of epoch.\n",
    "            logs: dict, metric results for this training epoch, and for the\n",
    "                validation epoch if validation is performed. Validation result keys\n",
    "                are prefixed with `val_`.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        \"\"\"Called at the beginning of a training batch in `fit` methods.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, has keys `batch` and `size` representing the current\n",
    "                batch number and the size of the batch.\n",
    "        \"\"\"\n",
    "        # For backwards compatibility\n",
    "        self.on_batch_begin(batch, logs=logs)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        \"\"\"Called at the end of a training batch in `fit` methods.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, metric results for this batch.\n",
    "        \"\"\"\n",
    "        # For backwards compatibility\n",
    "        self.on_batch_end(batch, logs=logs)\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        \"\"\"Called at the beginning of a batch in `evaluate` methods.\n",
    "        Also called at the beginning of a validation batch in the `fit` methods,\n",
    "        if validation data is provided.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, has keys `batch` and `size` representing the current\n",
    "                batch number and the size of the batch.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        \"\"\"Called at the end of a batch in `evaluate` methods.\n",
    "        Also called at the end of a validation batch in the `fit` methods,\n",
    "        if validation data is provided.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, metric results for this batch.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        \"\"\"Called at the beginning of a batch in `predict` methods.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, has keys `batch` and `size` representing the current\n",
    "                batch number and the size of the batch.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        \"\"\"Called at the end of a batch in `predict` methods.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, metric results for this batch.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        \"\"\"Called at the beginning of training.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        \"\"\"Called at the end of training.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        \"\"\"Called at the beginning of evaluation or validation.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        \"\"\"Called at the end of evaluation or validation.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        \"\"\"Called at the beginning of prediction.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        \"\"\"Called at the end of prediction.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    " \n",
    "f=open('c.txt','w')\n",
    "old=sys.stdout #将当前系统输出储存到临时变量\n",
    "sys.stdout=f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import keras\n",
    "from keras.callbacks import *\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import keras.backend as K\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.data_process import parse_function, load_data\n",
    "# from losses.face_losses import arcface_loss\n",
    "# from nets.MobileFaceNet import inference\n",
    "from verification import evaluate\n",
    "from scipy.optimize import brentq\n",
    "from scipy import interpolate\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "MODEL_FILE = 'MobileFaceNet.h5'\n",
    "LITE_FILE  = 'MobileFaceNet.tflite'\n",
    "\n",
    "NUM_PICTURES=490623\n",
    "NUM_CLASSES=10572\n",
    "BATCH_SIZE=90\n",
    "TARGET_SIZE=(112,112)\n",
    "TFRECORD_PATH='/workspace/dataset/faces_webface_112x112/tfrecords/tran.tfrecords'\n",
    "\n",
    "def my_generator(tfrecord_path=TFRECORD_PATH,batch_size=BATCH_SIZE,out_num=NUM_CLASSES):\n",
    "    \"\"\"自定义generator\n",
    "    \n",
    "    # Argument\n",
    "        tfrecord_path:\n",
    "        batch_size\n",
    "        out_num: 类别数量，用于生成onehot\n",
    "        \n",
    "    # Return\n",
    "    \n",
    "    \"\"\"\n",
    "    def parse_function(example_proto):\n",
    "        features = {'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                    'label': tf.FixedLenFeature([], tf.int64)}\n",
    "        features = tf.parse_single_example(example_proto, features)\n",
    "        # You can do more image distortion here for training data\n",
    "        img = tf.image.decode_jpeg(features['image_raw'])\n",
    "        img = tf.reshape(img, shape=(112, 112, 3))\n",
    "\n",
    "        #img = tf.py_func(random_rotate_image, [img], tf.uint8)\n",
    "        img = tf.cast(img, dtype=tf.float32)\n",
    "        img = tf.subtract(img, 127.5)\n",
    "        img = tf.multiply(img,  0.0078125)\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        label = tf.cast(features['label'], tf.int64)\n",
    "#         label = tf.one_hot(label,out_num)\n",
    "#         label = tf.reshape(label,(-1,))\n",
    "#         one_hot = tf.one_hot(label,out_num)\n",
    "        return (img, label)\n",
    "    \n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess = tf.Session(config=config)\n",
    "#     sess = K.get_session()\n",
    "    # training datasets api config\n",
    "    tfrecords_f = os.path.join(tfrecord_path)\n",
    "    dataset = tf.data.TFRecordDataset(tfrecords_f)\n",
    "    dataset = dataset.map(parse_function)\n",
    "#     dataset = dataset.shuffle(buffer_size=5000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    return iterator, next_element, sess\n",
    "    # begin iteration\n",
    "#     while(True):\n",
    "#         sess.run(iterator.initializer)\n",
    "#         while True:\n",
    "#             try:\n",
    "#                 images, labels = sess.run(next_element)\n",
    "# #                 for i in range(len(images)):\n",
    "# #                     images[i,...] = cv2.cvtColor(images[i, ...], cv2.COLOR_RGB2BGR)\n",
    "#                 yield images,labels\n",
    "#             except tf.errors.OutOfRangeError:\n",
    "# #                 print(\"End of dataset\")\n",
    "#                 break\n",
    "\n",
    "\n",
    "def my_generator_wrapper():\n",
    "    for image,label in my_generator():\n",
    "        yield ([image,label],label)\n",
    "#         yield image,label\n",
    "        \n",
    "def test_generator():\n",
    "    a = my_generator_wrapper()\n",
    "    for i in tqdm(range(50000)):\n",
    "        a.__next__()\n",
    "        \n",
    "def learning_rate_schedule(epoch, lr, boundaries, values):\n",
    "    \"\"\"\n",
    "    # Argument:\n",
    "        epoch: now epoch\n",
    "        lr: learning rate to schedule\n",
    "        boundaries: Number of epochs for learning rate piecewise.\n",
    "        values: target value of learning rate\n",
    "    \"\"\"\n",
    "    if epoch <= boundaries[0]:\n",
    "        t = values[0]\n",
    "    for low, high, v in zip(boundaries[:-1],boundaries[1:],values[1:]):\n",
    "        if low < epoch <= high:\n",
    "            t = v\n",
    "    if epoch > boundaries[-1]:\n",
    "        t = values[-1]\n",
    "        \n",
    "    K.get_session().run(lr.assign(t))\n",
    "    return epoch,t\n",
    "\n",
    "class ExponentialMovingAverage:\n",
    "    \"\"\"对模型权重进行指数滑动平均。\n",
    "    用法：在model.compile之后、第一次训练之前使用；\n",
    "    先初始化对象，然后执行inject方法。\n",
    "    \"\"\"\n",
    "    def __init__(self, model, momentum=0.9999):\n",
    "        self.momentum = momentum\n",
    "        self.model = model\n",
    "        self.ema_weights = [K.zeros(K.shape(w)) for w in model.weights]\n",
    "    def inject(self):\n",
    "        \"\"\"添加更新算子到model.metrics_updates。\n",
    "        \"\"\"\n",
    "        self.initialize()\n",
    "        for w1, w2 in zip(self.ema_weights, self.model.weights):\n",
    "            op = K.moving_average_update(w1, w2, self.momentum)\n",
    "            self.model.metrics_updates.append(op)\n",
    "    def initialize(self):\n",
    "        \"\"\"ema_weights初始化跟原模型初始化一致。\n",
    "        \"\"\"\n",
    "        self.old_weights = K.batch_get_value(self.model.weights)\n",
    "        K.batch_set_value(zip(self.ema_weights, self.old_weights))\n",
    "    def apply_ema_weights(self):\n",
    "        \"\"\"备份原模型权重，然后将平均权重应用到模型上去。\n",
    "        \"\"\"\n",
    "        self.old_weights = K.batch_get_value(self.model.weights)\n",
    "        ema_weights = K.batch_get_value(self.ema_weights)\n",
    "        K.batch_set_value(zip(self.model.weights, ema_weights))\n",
    "    def reset_old_weights(self):\n",
    "        \"\"\"恢复模型到旧权重。\n",
    "        \"\"\"\n",
    "        K.batch_set_value(zip(self.model.weights, self.old_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dropout,PReLU,Layer\n",
    "from keras.layers import Activation, BatchNormalization, add, Reshape,DepthwiseConv2D\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.activations import relu\n",
    "from keras.initializers import Constant\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras import backend as K\n",
    "import math\n",
    "\n",
    "weight_decay = 5e-5  # l2正则化decay常量\n",
    "\n",
    "\n",
    "batch_norm_params = {\n",
    "    'center': True,\n",
    "    'scale': True,\n",
    "    'momentum': 0.995,\n",
    "    'epsilon': 2e-5,\n",
    "}\n",
    "\n",
    "\n",
    "def flow_wrapper(flow):\n",
    "    \"\"\"自定义wrapper，将(x,y)变成([x,y],y)\"\"\"\n",
    "    while True:\n",
    "        x,y = flow.next()\n",
    "        yield ([x,y],y)\n",
    "\n",
    "def prelu(input, name=''):\n",
    "    \"\"\"自定义prelu\"\"\"\n",
    "    alphas = K.variable(K.constant(0.25,dtype=tf.float32,shape=[input.get_shape()[-1]]),name=name + 'prelu_alphas')\n",
    "    pos = K.relu(input)\n",
    "    neg = alphas * (input - K.abs(input)) * 0.5\n",
    "    return pos + neg\n",
    "\n",
    "cval = Constant(0.25)  # prelu α 初始常量\n",
    "\n",
    "def _conv_block(inputs, filters, kernel, strides):\n",
    "    \"\"\"Convolution Block\n",
    "    This function defines a 2D convolution operation with BN and relu6.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        strides: An integer or tuple/list of 2 integers,\n",
    "            specifying the strides of the convolution along the width and height.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    x = Conv2D(filters, kernel, padding='same', strides=strides, kernel_initializer='glorot_normal',kernel_regularizer=l2(weight_decay))(inputs)\n",
    "    x = BatchNormalization(axis=channel_axis,**batch_norm_params)(x)\n",
    "    x = PReLU(cval)(x)\n",
    "#     x = Activation(relu)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _bottleneck(inputs, filters, kernel, t, s, r=False):\n",
    "    \"\"\"Bottleneck\n",
    "    This function defines a basic bottleneck structure.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        r: Boolean, Whether to use the residuals.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    tchannel = K.int_shape(inputs)[channel_axis] * t\n",
    "\n",
    "    x = _conv_block(inputs, tchannel, (1, 1), (1, 1))\n",
    "\n",
    "    x = DepthwiseConv2D(kernel, strides=(s, s), depth_multiplier=1, padding='same', kernel_initializer='glorot_normal')(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = PReLU(cval)(x)\n",
    "#     x = Activation(relu)(x)\n",
    "\n",
    "    x = Conv2D(filters, (1, 1), strides=(1, 1), padding='same', kernel_initializer='glorot_normal',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = BatchNormalization(axis=channel_axis,**batch_norm_params)(x)\n",
    "\n",
    "    if r:\n",
    "        x = add([x, inputs])\n",
    "    return x\n",
    "\n",
    "\n",
    "def _inverted_residual_block(inputs, filters, kernel, t, strides, n):\n",
    "    \"\"\"Inverted Residual Block\n",
    "    This function defines a sequence of 1 or more identical layers.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        n: Integer, layer repeat times.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    x = _bottleneck(inputs, filters, kernel, t, strides)\n",
    "\n",
    "    for i in range(1, n):\n",
    "        x = _bottleneck(x, filters, kernel, t, 1, True)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class ArcFace(Layer):\n",
    "    \"\"\"改进的softmax，得出的结果再与真是结果之间求交叉熵\"\"\"\n",
    "    def __init__(self, n_classes=10, s=64.0, m=0.50, regularizer=None, **kwargs):\n",
    "        super(ArcFace, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcFace, self).build(input_shape[0])\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                shape=(input_shape[0][-1], self.n_classes),\n",
    "                                initializer='glorot_uniform',\n",
    "                                trainable=True,\n",
    "                                regularizer=self.regularizer)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x, y = inputs # x为embeddings，y为labels\n",
    "#         c = K.shape(x)[-1]  # 特征维度\n",
    "#         # 1. normalize feature\n",
    "#         x = tf.nn.l2_normalize(x, axis=1)\n",
    "#         # 2. normalize weights\n",
    "#         W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "#         # dot product\n",
    "#         # 全连接层，x的结构为（None，128）w的结构为（128，n_classes）。logits的结构为(None,n_classes)\n",
    "#         # (np.random.randn(5,128) @ np.random.randn(128,10)).shape # (5, 10)\n",
    "#         # 3. 计算xW得到预测向量y\n",
    "#         logits = x @ W\n",
    "#         # add margin\n",
    "#         # clip logits to prevent zero division when backward\n",
    "#         theta = tf.acos(K.clip(logits, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n",
    "#         target_logits = tf.cos(theta + self.m)\n",
    "#         # sin = tf.sqrt(1 - logits**2)\n",
    "#         # cos_m = tf.cos(logits)\n",
    "#         # sin_m = tf.sin(logits)\n",
    "#         # target_logits = logits * cos_m - sin * sin_m\n",
    "#         logits = logits * (1 - y) + target_logits * y\n",
    "#         # feature re-scale\n",
    "#         # 9. 对所有值乘上固定值s\n",
    "#         logits *= self.s\n",
    "#         out = tf.nn.softmax(logits)\n",
    "#         print(out)\n",
    "#         return out\n",
    "\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        embedding, labels = inputs\n",
    "        labels = tf.reshape(labels,shape=(-1,))\n",
    "        print('labels:',labels)\n",
    "        \n",
    "        out_num = self.n_classes\n",
    "        w_init=None\n",
    "        s=64.\n",
    "        m=0.5\n",
    "        \n",
    "        cos_m = tf.cos(m)\n",
    "        sin_m = tf.sin(m)\n",
    "        mm = sin_m * m  # issue 1\n",
    "        threshold = tf.cos(math.pi - m)\n",
    "        with tf.variable_scope('arcface_loss'):\n",
    "            # inputs and weights norm\n",
    "            embedding_norm = tf.norm(embedding, axis=1, keepdims=True)\n",
    "            embedding = tf.div(embedding, embedding_norm, name='norm_embedding')\n",
    "#             weights = tf.get_variable(name='embedding_weights', shape=(embedding.get_shape().as_list()[-1], out_num),\n",
    "#                                       initializer=w_init, dtype=tf.float32)\n",
    "            weights = self.W\n",
    "            weights_norm = tf.norm(weights, axis=0, keepdims=True)\n",
    "            weights = tf.div(weights, weights_norm, name='norm_weights')\n",
    "            # cos(theta+m)\n",
    "            cos_t = tf.matmul(embedding, weights, name='cos_t')\n",
    "            cos_t2 = tf.square(cos_t, name='cos_2')\n",
    "            sin_t2 = tf.subtract(1., cos_t2, name='sin_2')\n",
    "            sin_t = tf.sqrt(sin_t2, name='sin_t')\n",
    "            cos_mt = s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')\n",
    "\n",
    "            # this condition controls the theta+m should in range [0, pi]\n",
    "            #      0<=theta+m<=pi\n",
    "            #     -m<=theta<=pi-m\n",
    "            cond_v = cos_t - threshold\n",
    "            cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)\n",
    "\n",
    "            keep_val = s*(cos_t - mm)\n",
    "            cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n",
    "            print('labels:',labels,'out_num',out_num)\n",
    "            mask = tf.one_hot(labels, depth=out_num, name='one_hot_mask')\n",
    "#             mask = tf.reshape(mask,(-1,))\n",
    "#             mask = labels\n",
    "            print(mask)\n",
    "            # mask = tf.squeeze(mask, 1)\n",
    "            inv_mask = tf.subtract(1., mask, name='inverse_mask')\n",
    "            print(inv_mask)\n",
    "            s_cos_t = tf.multiply(s, cos_t, name='scalar_cos_t')\n",
    "            print(s_cos_t)\n",
    "            logit = tf.add(tf.multiply(s_cos_t, inv_mask), tf.multiply(cos_mt_temp, mask), name='arcface_loss_output')\n",
    "            print(logit)\n",
    "#             inference_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=labels))\n",
    "            inference_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=labels)\n",
    "            print(inference_loss)\n",
    "#             inference_loss = tf.nn.softmax(logit)\n",
    "            print(inference_loss)\n",
    "        return inference_loss\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.n_classes)\n",
    "\n",
    "def MobileFaceNets(input_shape=(112,112,3), n_classes=10, k=128):\n",
    "    \"\"\"MobileFaceNets\"\"\"\n",
    "    inputs = Input(shape=input_shape) #112x112，(img-127.5)/255\n",
    "    y      = Input(shape=(1,), dtype=tf.int32)\n",
    "#     y      = Input(shape=(n_classes,))\n",
    "    x = _conv_block(inputs, 64, (3, 3), strides=(2, 2))\n",
    "    \n",
    "    # depthwise conv3x3\n",
    "    x = DepthwiseConv2D(3, strides=(1, 1), depth_multiplier=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = PReLU(cval)(x)\n",
    "#     x = Activation(relu)(x)\n",
    "    \n",
    "    # 5层bottleneck\n",
    "    x = _inverted_residual_block(x, 64, (3, 3), t=2, strides=2, n=5)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=4, strides=2, n=1)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=2, strides=1, n=6)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=4, strides=2, n=1)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=2, strides=1, n=2)\n",
    "    \n",
    "    # conv1x1\n",
    "    x = _conv_block(x, 512, (1, 1), strides=(1, 1))\n",
    "    \n",
    "    # linear GDConv7x7\n",
    "    x = DepthwiseConv2D(7, strides=(1, 1), depth_multiplier=1, padding='valid')(x)\n",
    "#     x = Dropout(0.3, name='Dropout')(x)\n",
    "    \n",
    "    x = Conv2D(k, (1, 1), padding='same',kernel_initializer='glorot_normal',kernel_regularizer=l2(1e-10))(x)\n",
    "    \n",
    "    \n",
    "#     x = Activation(keras.activations.re)\n",
    "    x = Reshape((k,))(x)\n",
    "    print(x)\n",
    "#     x = keras.layers.Lambda(lambda o: K.l2_normalize(o, axis=1))(x)\n",
    "    epsilon=1e-10\n",
    "    x = keras.layers.Lambda(lambda o: o/K.sqrt(K.maximum(K.sum(K.square(o),axis=1,keepdims=True),epsilon)))(x)\n",
    "#     x = tf.nn.l2_normalize(x, 1, 1e-10, name='embeddings')\n",
    "    \n",
    "    # x 为embeddings， y为embeddings对应的类别标签，output为\n",
    "    output = ArcFace(n_classes=n_classes, regularizer=None)([x, y])\n",
    "    \n",
    "    model = Model([inputs, y], output)\n",
    "#     plot_model(model, to_file='images/MobileNetv2.png', show_shapes=True)\n",
    "    print(model.input,model.output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"reshape_1/Reshape:0\", shape=(?, 128), dtype=float32)\n",
      "labels: Tensor(\"arc_face_1/Reshape:0\", shape=(?,), dtype=int32)\n",
      "WARNING:tensorflow:From <ipython-input-4-106b1654766a>:190: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "labels: Tensor(\"arc_face_1/Reshape:0\", shape=(?,), dtype=int32) out_num 10572\n",
      "Tensor(\"arc_face_1/arcface_loss/one_hot_mask:0\", shape=(?, 10572), dtype=float32)\n",
      "Tensor(\"arc_face_1/arcface_loss/inverse_mask:0\", shape=(?, 10572), dtype=float32)\n",
      "Tensor(\"arc_face_1/arcface_loss/scalar_cos_t:0\", shape=(?, 10572), dtype=float32)\n",
      "Tensor(\"arc_face_1/arcface_loss/arcface_loss_output:0\", shape=(?, 10572), dtype=float32)\n",
      "Tensor(\"arc_face_1/arcface_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"arc_face_1/arcface_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(?,), dtype=float32)\n",
      "[<tf.Tensor 'input_1:0' shape=(?, 112, 112, 3) dtype=float32>, <tf.Tensor 'input_2:0' shape=(?, 1) dtype=int32>] Tensor(\"arc_face_1/arcface_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(?,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def my_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred)\n",
    "\n",
    "model = MobileFaceNets(n_classes=NUM_CLASSES)\n",
    "model.compile(optimizer=keras.optimizers.Adam(0.1,beta_1=0.9, beta_2=0.999, epsilon=0.1),\n",
    "      loss=my_loss,\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "EMAer = ExponentialMovingAverage(model) # 在模型compile之后执行\n",
    "EMAer.inject() # 在模型compile之后执行\n",
    "\n",
    "# model.compile(optimizer=keras.optimizers.sgd(0.1),loss=my_loss,metrics=['accuracy'])\n",
    "val_model = keras.models.Model(inputs=model.inputs[0], outputs=model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(53.444035, 0.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = my_generator_wrapper()\n",
    "images_train, labels_train = g.__next__()\n",
    "loss, accuracy = model.train_on_batch(images_train, labels_train)\n",
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    # 当监测值不再改善时，该回调函数将中止训练\n",
    "    # 如发现loss相比上一个epoch训练没有下降），则经过patience个epoch后停止训练。\n",
    "    EarlyStopping(monitor='loss', patience=10, verbose=1),\n",
    "    # 该回调函数将日志信息写入TensorBorad\n",
    "#     TensorBoard(log_dir='./models/logs',histogram_freq=1),\n",
    "    # 当评价指标不在提升时，减少学习率\n",
    "    # min_lr：学习率的下限\n",
    "    ReduceLROnPlateau(monitor='loss',factor=0.2,patience=3,min_lr=0.0001),\n",
    "    # 该回调函数将在每个epoch后保存模型到filepath\n",
    "    ModelCheckpoint(filepath='models/weights-autoencoder-{epoch:02d}-{loss:.2f}.h5',save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresholds max: 3.99 <=> min: 0.79\n"
     ]
    }
   ],
   "source": [
    "tpr, fpr, accuracy, val, val_std, far = evaluate(emb_array, issame_list, nrof_folds=eval_nrof_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,  lr:(0, 0.1)\n",
      "epoch:1,  lr:(1, 0.1)\n",
      "epoch:2,  lr:(2, 0.1)\n",
      "epoch:3,  lr:(3, 0.1)\n",
      "epoch:4,  lr:(4, 0.1)\n",
      "epoch:5,  lr:(5, 0.01)\n",
      "epoch:6,  lr:(6, 0.01)\n",
      "epoch:7,  lr:(7, 0.01)\n",
      "epoch:8,  lr:(8, 0.001)\n",
      "epoch:9,  lr:(9, 0.001)\n",
      "epoch:10,  lr:(10, 1e-04)\n",
      "epoch:11,  lr:(11, 1e-04)\n",
      "epoch:12,  lr:(12, 1e-05)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1e-05"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning_rate_schedule\n",
    "\n",
    "lr_schedule = [4, 7, 9, 11]\n",
    "values=[0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "def learning_rate_schedule(epoch, lr, boundaries, values):\n",
    "    \"\"\"\n",
    "    # Argument:\n",
    "        epoch: now epoch\n",
    "        lr: learning rate to schedule\n",
    "        boundaries: Number of epochs for learning rate piecewise.\n",
    "        values: target value of learning rate\n",
    "    \"\"\"\n",
    "    if epoch <= boundaries[0]:\n",
    "        t = values[0]\n",
    "    for low, high, v in zip(boundaries[:-1],boundaries[1:],values[1:]):\n",
    "        if low < epoch <= high:\n",
    "            t = v\n",
    "    if epoch > boundaries[-1]:\n",
    "        t = values[-1]\n",
    "        \n",
    "    K.get_session().run(lr.assign(t))\n",
    "    return epoch,K.get_session().run(lr)\n",
    "\n",
    "lr = 0\n",
    "\n",
    "for i in range(13):\n",
    "    lr = learning_rate_schedule(i,model.optimizer.lr,lr_schedule,values)\n",
    "    print('epoch:{},  lr:{}'.format(i,lr))\n",
    "\n",
    "# K.get_session().run(model.optimizer.lr.assign(0.001))\n",
    "K.get_session().run(model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, lr:0.1\n",
      "End of epoch 0\n",
      "epoch:1, lr:0.1\n",
      "End of epoch 1\n",
      "epoch:2, lr:0.1\n",
      "End of epoch 2\n",
      "epoch:3, lr:0.1\n"
     ]
    }
   ],
   "source": [
    "# test_iterator\n",
    "iterator, next_element, g_sess = my_generator()\n",
    "\n",
    "# epoch = -1\n",
    "count = 0\n",
    "total_accuracy = {}\n",
    "for i in range(max_epoch):\n",
    "    g_sess.run(iterator.initializer)\n",
    "    _, lr = learning_rate_schedule(i,model.optimizer.lr,lr_schedule,values)\n",
    "    print('epoch:{}, lr:{}'.format(i, lr))\n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            images_train, labels_train = g_sess.run(next_element)\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of epoch %d\" % i)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin db lfw convert.\n",
      "loading bin 1000\n",
      "loading bin 2000\n",
      "loading bin 3000\n",
      "loading bin 4000\n",
      "loading bin 5000\n",
      "loading bin 6000\n",
      "loading bin 7000\n",
      "loading bin 8000\n",
      "loading bin 9000\n",
      "loading bin 10000\n",
      "loading bin 11000\n",
      "loading bin 12000\n",
      "(12000, 112, 112, 3)\n",
      "epoch:0, lr:0.1\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "epoch 0, total_step 50, loss is 78.894653, training accuracy is 0.000000, time 300.640 samples/sec\n",
      "epoch 0, total_step 100, loss is 66.870270, training accuracy is 0.000000, time 297.644 samples/sec\n",
      "epoch 0, total_step 150, loss is 57.706238, training accuracy is 0.000000, time 298.493 samples/sec\n",
      "epoch 0, total_step 200, loss is 46.662277, training accuracy is 0.000000, time 301.045 samples/sec\n",
      "epoch 0, total_step 250, loss is 38.208523, training accuracy is 0.000000, time 299.105 samples/sec\n",
      "epoch 0, total_step 300, loss is 37.079002, training accuracy is 0.000000, time 296.067 samples/sec\n",
      "epoch 0, total_step 350, loss is 36.528091, training accuracy is 0.000000, time 296.667 samples/sec\n",
      "epoch 0, total_step 400, loss is 28.906736, training accuracy is 0.000000, time 298.116 samples/sec\n",
      "epoch 0, total_step 450, loss is 29.433771, training accuracy is 0.000000, time 294.136 samples/sec\n",
      "epoch 0, total_step 500, loss is 24.236380, training accuracy is 0.000000, time 298.426 samples/sec\n",
      "\n",
      "Iteration 500 testing...\n",
      "thresholds max: 0.01 <=> min: 0.01\n",
      "total time 18.805s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.505+-0.003\n",
      "Validation rate: 0.00000+-0.00000 @ FAR=0.00000\n",
      "fpr and tpr: 0.997 0.997\n",
      "Area Under Curve (AUC): 0.505\n",
      "epoch 0, total_step 550, loss is 25.119757, training accuracy is 0.000000, time 297.298 samples/sec\n",
      "epoch 0, total_step 600, loss is 21.917776, training accuracy is 0.000000, time 301.352 samples/sec\n",
      "epoch 0, total_step 650, loss is 26.458786, training accuracy is 0.000000, time 294.397 samples/sec\n",
      "epoch 0, total_step 700, loss is 25.408859, training accuracy is 0.000000, time 293.704 samples/sec\n",
      "epoch 0, total_step 750, loss is 23.373892, training accuracy is 0.000000, time 303.201 samples/sec\n",
      "epoch 0, total_step 800, loss is 27.320095, training accuracy is 0.000000, time 299.380 samples/sec\n",
      "epoch 0, total_step 850, loss is 27.486626, training accuracy is 0.000000, time 290.920 samples/sec\n",
      "epoch 0, total_step 900, loss is 22.905794, training accuracy is 0.000000, time 297.230 samples/sec\n",
      "epoch 0, total_step 950, loss is 21.678471, training accuracy is 0.000000, time 297.485 samples/sec\n",
      "epoch 0, total_step 1000, loss is 23.724039, training accuracy is 0.000000, time 299.141 samples/sec\n",
      "\n",
      "Iteration 1000 testing...\n",
      "thresholds max: 0.01 <=> min: 0.0\n",
      "total time 17.303s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.500+-0.000\n",
      "Validation rate: 0.00000+-0.00000 @ FAR=0.00000\n",
      "fpr and tpr: 0.998 0.998\n",
      "Area Under Curve (AUC): 0.500\n",
      "epoch 0, total_step 1050, loss is 20.984850, training accuracy is 0.000000, time 299.090 samples/sec\n",
      "epoch 0, total_step 1100, loss is 23.348682, training accuracy is 0.000000, time 299.858 samples/sec\n",
      "epoch 0, total_step 1150, loss is 22.319042, training accuracy is 0.000000, time 301.706 samples/sec\n",
      "epoch 0, total_step 1200, loss is 23.266247, training accuracy is 0.000000, time 303.259 samples/sec\n",
      "epoch 0, total_step 1250, loss is 21.347044, training accuracy is 0.000000, time 298.178 samples/sec\n",
      "epoch 0, total_step 1300, loss is 20.841063, training accuracy is 0.000000, time 301.806 samples/sec\n",
      "epoch 0, total_step 1350, loss is 22.752975, training accuracy is 0.000000, time 298.143 samples/sec\n",
      "epoch 0, total_step 1400, loss is 22.150675, training accuracy is 0.000000, time 299.672 samples/sec\n",
      "epoch 0, total_step 1450, loss is 20.923069, training accuracy is 0.000000, time 306.794 samples/sec\n",
      "epoch 0, total_step 1500, loss is 20.456654, training accuracy is 0.000000, time 295.907 samples/sec\n",
      "\n",
      "Iteration 1500 testing...\n",
      "thresholds max: 0.01 <=> min: 0.0\n",
      "total time 17.335s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.500+-0.000\n",
      "Validation rate: 1.00000+-0.00000 @ FAR=1.00000\n",
      "fpr and tpr: 0.998 0.998\n",
      "Area Under Curve (AUC): 0.500\n",
      "epoch 0, total_step 1550, loss is 20.901863, training accuracy is 0.000000, time 295.037 samples/sec\n",
      "epoch 0, total_step 1600, loss is 21.780899, training accuracy is 0.000000, time 293.779 samples/sec\n",
      "epoch 0, total_step 1650, loss is 20.495403, training accuracy is 0.000000, time 300.421 samples/sec\n",
      "epoch 0, total_step 1700, loss is 20.675200, training accuracy is 0.000000, time 298.501 samples/sec\n",
      "epoch 0, total_step 1750, loss is 20.265263, training accuracy is 0.000000, time 292.821 samples/sec\n",
      "epoch 0, total_step 1800, loss is 20.656759, training accuracy is 0.000000, time 298.743 samples/sec\n",
      "epoch 0, total_step 1850, loss is 20.503731, training accuracy is 0.000000, time 303.343 samples/sec\n",
      "epoch 0, total_step 1900, loss is 20.613249, training accuracy is 0.000000, time 298.963 samples/sec\n",
      "epoch 0, total_step 1950, loss is 19.943974, training accuracy is 0.000000, time 295.411 samples/sec\n",
      "epoch 0, total_step 2000, loss is 19.824797, training accuracy is 0.000000, time 288.303 samples/sec\n",
      "\n",
      "Iteration 2000 testing...\n",
      "thresholds max: 0.01 <=> min: 0.0\n",
      "total time 17.330s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.500+-0.000\n",
      "Validation rate: 1.00000+-0.00000 @ FAR=1.00000\n",
      "fpr and tpr: 0.998 0.998\n",
      "Area Under Curve (AUC): 0.500\n",
      "epoch 0, total_step 2050, loss is 19.523815, training accuracy is 0.000000, time 302.492 samples/sec\n",
      "epoch 0, total_step 2100, loss is 19.939943, training accuracy is 0.000000, time 297.757 samples/sec\n",
      "epoch 0, total_step 2150, loss is 20.146318, training accuracy is 0.000000, time 305.091 samples/sec\n",
      "epoch 0, total_step 2200, loss is 19.374180, training accuracy is 0.000000, time 298.156 samples/sec\n",
      "epoch 0, total_step 2250, loss is 19.753431, training accuracy is 0.000000, time 295.346 samples/sec\n",
      "epoch 0, total_step 2300, loss is 19.258089, training accuracy is 0.000000, time 299.426 samples/sec\n",
      "epoch 0, total_step 2350, loss is 20.286325, training accuracy is 0.000000, time 298.130 samples/sec\n",
      "epoch 0, total_step 2400, loss is 19.076265, training accuracy is 0.000000, time 301.411 samples/sec\n",
      "epoch 0, total_step 2450, loss is 19.823944, training accuracy is 0.000000, time 292.274 samples/sec\n",
      "epoch 0, total_step 2500, loss is 19.446186, training accuracy is 0.000000, time 299.200 samples/sec\n",
      "\n",
      "Iteration 2500 testing...\n",
      "thresholds max: 0.01 <=> min: 0.0\n",
      "total time 17.325s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.500+-0.000\n",
      "Validation rate: 1.00000+-0.00000 @ FAR=1.00000\n",
      "fpr and tpr: 0.998 0.998\n",
      "Area Under Curve (AUC): 0.500\n",
      "epoch 0, total_step 2550, loss is 19.463064, training accuracy is 0.000000, time 296.338 samples/sec\n",
      "epoch 0, total_step 2600, loss is 19.328951, training accuracy is 0.000000, time 294.204 samples/sec\n",
      "epoch 0, total_step 2650, loss is 19.194262, training accuracy is 0.000000, time 298.424 samples/sec\n",
      "epoch 0, total_step 2700, loss is 19.507645, training accuracy is 0.000000, time 300.237 samples/sec\n",
      "epoch 0, total_step 2750, loss is 19.966593, training accuracy is 0.000000, time 294.463 samples/sec\n",
      "epoch 0, total_step 2800, loss is 19.624706, training accuracy is 0.000000, time 293.281 samples/sec\n",
      "epoch 0, total_step 2850, loss is 19.427294, training accuracy is 0.000000, time 299.338 samples/sec\n",
      "epoch 0, total_step 2900, loss is 19.439402, training accuracy is 0.000000, time 300.327 samples/sec\n",
      "epoch 0, total_step 2950, loss is 19.909643, training accuracy is 0.000000, time 293.272 samples/sec\n",
      "epoch 0, total_step 3000, loss is 18.954718, training accuracy is 0.000000, time 296.272 samples/sec\n",
      "\n",
      "Iteration 3000 testing...\n",
      "thresholds max: 0.01 <=> min: 0.0\n",
      "total time 17.182s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.500+-0.000\n",
      "Validation rate: 1.00000+-0.00000 @ FAR=1.00000\n",
      "fpr and tpr: 0.998 0.998\n",
      "Area Under Curve (AUC): 0.500\n",
      "epoch 0, total_step 3050, loss is 19.842445, training accuracy is 0.000000, time 300.603 samples/sec\n",
      "epoch 0, total_step 3100, loss is 19.192780, training accuracy is 0.000000, time 296.067 samples/sec\n",
      "epoch 0, total_step 3150, loss is 19.232235, training accuracy is 0.000000, time 296.123 samples/sec\n",
      "epoch 0, total_step 3200, loss is 19.284809, training accuracy is 0.000000, time 297.177 samples/sec\n",
      "epoch 0, total_step 3250, loss is 18.863741, training accuracy is 0.000000, time 296.743 samples/sec\n",
      "epoch 0, total_step 3300, loss is 19.313484, training accuracy is 0.000000, time 303.157 samples/sec\n",
      "epoch 0, total_step 3350, loss is 19.175076, training accuracy is 0.000000, time 302.853 samples/sec\n",
      "epoch 0, total_step 3400, loss is 20.270632, training accuracy is 0.000000, time 295.416 samples/sec\n",
      "epoch 0, total_step 3450, loss is 19.009264, training accuracy is 0.000000, time 305.040 samples/sec\n",
      "epoch 0, total_step 3500, loss is 18.932098, training accuracy is 0.000000, time 297.440 samples/sec\n",
      "\n",
      "Iteration 3500 testing...\n",
      "thresholds max: 0.01 <=> min: 0.0\n",
      "total time 17.204s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.500+-0.000\n",
      "Validation rate: 1.00000+-0.00000 @ FAR=1.00000\n",
      "fpr and tpr: 0.998 0.998\n",
      "Area Under Curve (AUC): 0.500\n",
      "epoch 0, total_step 3550, loss is 19.111473, training accuracy is 0.000000, time 299.208 samples/sec\n",
      "epoch 0, total_step 3600, loss is 19.182848, training accuracy is 0.000000, time 299.937 samples/sec\n",
      "epoch 0, total_step 3650, loss is 19.016768, training accuracy is 0.000000, time 296.465 samples/sec\n",
      "epoch 0, total_step 3700, loss is 18.991421, training accuracy is 0.000000, time 299.791 samples/sec\n",
      "epoch 0, total_step 3750, loss is 19.185595, training accuracy is 0.000000, time 303.697 samples/sec\n",
      "epoch 0, total_step 3800, loss is 18.707573, training accuracy is 0.000000, time 294.178 samples/sec\n",
      "epoch 0, total_step 3850, loss is 19.105255, training accuracy is 0.000000, time 299.663 samples/sec\n",
      "epoch 0, total_step 3900, loss is 18.716196, training accuracy is 0.000000, time 303.606 samples/sec\n",
      "epoch 0, total_step 3950, loss is 19.032904, training accuracy is 0.000000, time 303.042 samples/sec\n",
      "epoch 0, total_step 4000, loss is 19.669649, training accuracy is 0.000000, time 302.010 samples/sec\n",
      "\n",
      "Iteration 4000 testing...\n",
      "thresholds max: 0.01 <=> min: 0.0\n",
      "total time 17.276s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.500+-0.000\n",
      "Validation rate: 1.00000+-0.00000 @ FAR=1.00000\n",
      "fpr and tpr: 0.998 0.998\n",
      "Area Under Curve (AUC): 0.500\n",
      "epoch 0, total_step 4050, loss is 18.921324, training accuracy is 0.000000, time 300.075 samples/sec\n",
      "epoch 0, total_step 4100, loss is 19.113066, training accuracy is 0.000000, time 301.642 samples/sec\n",
      "epoch 0, total_step 4150, loss is 18.969879, training accuracy is 0.000000, time 296.526 samples/sec\n",
      "epoch 0, total_step 4200, loss is 18.721039, training accuracy is 0.000000, time 295.416 samples/sec\n",
      "epoch 0, total_step 4250, loss is 19.152531, training accuracy is 0.000000, time 302.381 samples/sec\n",
      "epoch 0, total_step 4300, loss is 19.073763, training accuracy is 0.000000, time 301.538 samples/sec\n",
      "epoch 0, total_step 4350, loss is 19.220074, training accuracy is 0.000000, time 290.569 samples/sec\n",
      "epoch 0, total_step 4400, loss is 18.756552, training accuracy is 0.000000, time 290.877 samples/sec\n",
      "epoch 0, total_step 4450, loss is 19.117088, training accuracy is 0.000000, time 299.337 samples/sec\n",
      "epoch 0, total_step 4500, loss is 18.736320, training accuracy is 0.000000, time 297.816 samples/sec\n",
      "\n",
      "Iteration 4500 testing...\n",
      "thresholds max: 0.01 <=> min: 0.0\n",
      "total time 17.377s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.500+-0.000\n",
      "Validation rate: 1.00000+-0.00000 @ FAR=1.00000\n",
      "fpr and tpr: 0.998 0.998\n",
      "Area Under Curve (AUC): 0.500\n",
      "epoch 0, total_step 4550, loss is 18.696812, training accuracy is 0.000000, time 296.901 samples/sec\n",
      "epoch 0, total_step 4600, loss is 18.821455, training accuracy is 0.000000, time 299.043 samples/sec\n",
      "epoch 0, total_step 4650, loss is 19.111816, training accuracy is 0.000000, time 298.615 samples/sec\n",
      "epoch 0, total_step 4700, loss is 18.855185, training accuracy is 0.000000, time 303.727 samples/sec\n",
      "epoch 0, total_step 4750, loss is 18.602280, training accuracy is 0.000000, time 298.237 samples/sec\n",
      "epoch 0, total_step 4800, loss is 18.915840, training accuracy is 0.000000, time 299.055 samples/sec\n",
      "epoch 0, total_step 4850, loss is 18.753130, training accuracy is 0.000000, time 298.235 samples/sec\n",
      "epoch 0, total_step 4900, loss is 19.358618, training accuracy is 0.000000, time 301.163 samples/sec\n",
      "epoch 0, total_step 4950, loss is 18.914427, training accuracy is 0.000000, time 295.782 samples/sec\n",
      "epoch 0, total_step 5000, loss is 19.020109, training accuracy is 0.000000, time 301.861 samples/sec\n",
      "\n",
      "Iteration 5000 testing...\n",
      "thresholds max: 0.01 <=> min: 0.0\n",
      "total time 17.254s to evaluate 12000 images of lfw\n",
      "Accuracy: 0.500+-0.000\n",
      "Validation rate: 1.00000+-0.00000 @ FAR=1.00000\n",
      "fpr and tpr: 0.998 0.998\n",
      "Area Under Curve (AUC): 0.500\n",
      "epoch 0, total_step 5050, loss is 18.697821, training accuracy is 0.000000, time 305.933 samples/sec\n",
      "epoch 0, total_step 5100, loss is 18.855522, training accuracy is 0.000000, time 292.713 samples/sec\n",
      "epoch 0, total_step 5150, loss is 18.774168, training accuracy is 0.000000, time 293.944 samples/sec\n",
      "epoch 0, total_step 5200, loss is 18.432026, training accuracy is 0.000000, time 303.397 samples/sec\n",
      "epoch 0, total_step 5250, loss is 18.875126, training accuracy is 0.000000, time 297.351 samples/sec\n",
      "epoch 0, total_step 5300, loss is 18.469681, training accuracy is 0.000000, time 299.609 samples/sec\n"
     ]
    }
   ],
   "source": [
    "train_batch_size = 90\n",
    "test_batch_size = 100\n",
    "eval_datasets = ['lfw']\n",
    "eval_db_path = '/workspace/dataset/faces_webface_112x112/'\n",
    "eval_nrof_folds = 10\n",
    "tfrecords_file_path = '/workspace/dataset/faces_webface_112x112/tfrecords/'\n",
    "summary_path = '/workspace/output/summary'\n",
    "ckpt_path = '/workspace/output/ckpt'\n",
    "pretrained_model = False\n",
    "log_file_path = '/workspace/output/logs'\n",
    "ckpt_best_path = '/workspace/output/ckpt_best'\n",
    "saver_maxkeep = 50\n",
    "summary_interval = 400\n",
    "ckpt_interval = 200\n",
    "validate_interval = 500\n",
    "show_info_interval = 50\n",
    "log_device_mapping = False\n",
    "log_histograms = False\n",
    "prelogits_norm_loss_factor = 2e-5\n",
    "prelogits_norm_p = 1.0\n",
    "max_epoch = 12\n",
    "image_size = [112, 112]\n",
    "embedding_size = 128\n",
    "\n",
    "lr_schedule = [4, 7, 9, 11]\n",
    "values=[0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "# prepare validate datasets\n",
    "ver_list = []\n",
    "ver_name_list = []\n",
    "for db in eval_datasets:\n",
    "    print('begin db %s convert.' % db)\n",
    "    data_set = load_data(db, image_size, eval_db_path)\n",
    "    ver_list.append(data_set)\n",
    "    ver_name_list.append(db)\n",
    "\n",
    "# output file path\n",
    "if not os.path.exists(log_file_path):\n",
    "    os.makedirs(log_file_path)\n",
    "if not os.path.exists(ckpt_best_path):\n",
    "    os.makedirs(ckpt_best_path)\n",
    "if not os.path.exists(ckpt_path):\n",
    "    os.makedirs(ckpt_path)\n",
    "# create log dir\n",
    "subdir = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "log_dir = os.path.join(os.path.expanduser(log_file_path), subdir)\n",
    "if not os.path.isdir(log_dir):  # Create the log directory if it doesn't exist\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "# g = my_generator_wrapper()\n",
    "iterator, next_element, g_sess = my_generator()\n",
    "\n",
    "# epoch = -1\n",
    "count = 0\n",
    "total_accuracy = {}\n",
    "for i in range(max_epoch):\n",
    "    # 调整学习率\n",
    "    _, lr = learning_rate_schedule(i,model.optimizer.lr,lr_schedule,values)\n",
    "    print('epoch:{}, lr:{}'.format(i, lr))\n",
    "    # 初始化迭代器\n",
    "    g_sess.run(iterator.initializer)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            x, y = g_sess.run(next_element)\n",
    "            \n",
    "            images_train,labels_train = ([x,y],y)\n",
    "\n",
    "            start = time.time()\n",
    "#             _, total_loss_val, inference_loss_val, reg_loss_val, _, acc_val = \\\n",
    "#             sess.run([train_op, total_loss, inference_loss, regularization_losses, inc_global_step_op, Accuracy_Op],\n",
    "#                      feed_dict=feed_dict)\n",
    "            loss, accuracy = model.train_on_batch(images_train, labels_train)\n",
    "    \n",
    "            end = time.time()\n",
    "            pre_sec = train_batch_size/(end - start)\n",
    "\n",
    "            count += 1\n",
    "            # print training information\n",
    "            if count > 0 and count % show_info_interval == 0:\n",
    "#                 print('epoch %d, total_step %d, total loss is %.2f , inference loss is %.2f, reg_loss is %.2f, training accuracy is %.6f, time %.3f samples/sec' %\n",
    "#                       (i, count, total_loss_val, inference_loss_val, np.sum(reg_loss_val), acc_val, pre_sec))\n",
    "                print('epoch %d, total_step %d, loss is %.6f, training accuracy is %.6f, time %.3f samples/sec' %\n",
    "                      (i, count, loss, accuracy, pre_sec))\n",
    "\n",
    "            # save summary\n",
    "#             if count > 0 and count % summary_interval == 0:\n",
    "#                 feed_dict = {inputs: images_train, labels: labels_train, phase_train_placeholder: True}\n",
    "#                 summary_op_val = sess.run(summary_op, feed_dict=feed_dict)\n",
    "#                 summary.add_summary(summary_op_val, count)\n",
    "\n",
    "            # save ckpt files\n",
    "            if count > 0 and count % ckpt_interval == 0:\n",
    "#                 filename = 'MobileFaceNet_iter_{:d}'.format(count) + '.ckpt'\n",
    "                EMAer.apply_ema_weights()\n",
    "                filename = 'MobileFaceNet_iter_{:d}'.format(count) + '.h5'\n",
    "                filename = os.path.join(ckpt_path, filename)\n",
    "                val_model.save(filename)\n",
    "                \n",
    "                EMAer.reset_old_weights()\n",
    "\n",
    "            # validate\n",
    "            if count > 0 and count % validate_interval == 0:\n",
    "                print('\\nIteration', count, 'testing...')\n",
    "                \n",
    "                EMAer.apply_ema_weights()\n",
    "                \n",
    "                for db_index in range(len(ver_list)):\n",
    "                    start_time = time.time()\n",
    "                    data_sets, issame_list = ver_list[db_index]\n",
    "                    emb_array = np.zeros((data_sets.shape[0], embedding_size))\n",
    "                    nrof_batches = data_sets.shape[0] // test_batch_size\n",
    "                    for index in range(nrof_batches): # actual is same multiply 2, test data total\n",
    "                        start_index = index * test_batch_size\n",
    "                        end_index = min((index + 1) * test_batch_size, data_sets.shape[0])\n",
    "\n",
    "#                         feed_dict = {inputs: data_sets[start_index:end_index, ...], phase_train_placeholder: False}\n",
    "#                         emb_array[start_index:end_index, :] = sess.run(embeddings, feed_dict=feed_dict)\n",
    "                        emb_array[start_index:end_index, :] = val_model.predict(data_sets[start_index:end_index, ...])\n",
    "\n",
    "                    tpr, fpr, accuracy, val, val_std, far = evaluate(emb_array, issame_list, nrof_folds=eval_nrof_folds)\n",
    "                    duration = time.time() - start_time\n",
    "\n",
    "                    print(\"total time %.3fs to evaluate %d images of %s\" % (duration, data_sets.shape[0], ver_name_list[db_index]))\n",
    "                    print('Accuracy: %1.3f+-%1.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "                    print('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far))\n",
    "                    print('fpr and tpr: %1.3f %1.3f' % (np.mean(fpr, 0), np.mean(tpr, 0)))\n",
    "\n",
    "                    auc = metrics.auc(fpr, tpr)\n",
    "                    print('Area Under Curve (AUC): %1.3f' % auc)\n",
    "#                     eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\n",
    "#                     print('Equal Error Rate (EER): %1.3f\\n' % eer)\n",
    "\n",
    "                    with open(os.path.join(log_dir, '{}_result.txt'.format(ver_name_list[db_index])), 'at') as f:\n",
    "                        f.write('%d\\t%.5f\\t%.5f\\n' % (count, np.mean(accuracy), val))\n",
    "\n",
    "                    if ver_name_list == 'lfw' and np.mean(accuracy) > 0.992:\n",
    "                        print('best accuracy is %.5f' % np.mean(accuracy))\n",
    "                        filename = 'MobileFaceNet_iter_best_{:d}'.format(count) + '.ckpt'\n",
    "                        filename = os.path.join(ckpt_best_path, filename)\n",
    "                        saver.save(sess, filename)\n",
    "                        \n",
    "                EMAer.reset_old_weights()\n",
    "                \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of epoch %d\" % i)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 68543 images belonging to 1005 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((90, 112, 112, 3), 68543, 1005)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(img):\n",
    "    \"\"\"图片预处理，the image is substracted 127.5 and multiplied 1/128.\"\"\"\n",
    "    return (img-127.5)*0.0078125\n",
    "\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess,)\n",
    "\n",
    "flow = datagen.flow_from_directory('/workspace/dataset/face_ms1m1/',target_size=(112,112),batch_size=90,)\n",
    "\n",
    "flow.next()[0].shape,flow.n,flow.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_from_directory(path='/workspace/dataset/face_ms1m1/'):\n",
    "    \"\"\"从path中加载图片的标签和路径\"\"\"\n",
    "    p = Path(path)\n",
    "    # 结构为图片的标签和图片的绝对路径\n",
    "    rets = []\n",
    "    \n",
    "    # 图片父目录文件夹名对应的id\n",
    "    label_dict = {}\n",
    "    \n",
    "    for i,file in enumerate(p.glob('*')):\n",
    "        file_name=str(file.name)\n",
    "        label_dict[file_name]=i\n",
    "    \n",
    "    for i,file in enumerate(p.glob('*/*')):\n",
    "        # 获得图片的父目录名字\n",
    "        parent = str(file.parent.name)\n",
    "        # 图片绝对路径\n",
    "        file_path = str(file)\n",
    "        # 重新设置标签\n",
    "        index = label_dict[parent]\n",
    "        rets.append([index,file_path])\n",
    "    return rets, label_dict\n",
    "\n",
    "def my_generator(images,labels,batch_size=32,preprocess_function=lambda x:(x-127.5)*0.0078125,shuffle=False,target_size=(112,112)):\n",
    "    \"\"\"图片和标签生成器，images为路径，label为标签\"\"\"\n",
    "    \n",
    "    def load_img(imgs_path):\n",
    "        imgs = []\n",
    "        for img_path in imgs_path:\n",
    "            img = tf.image.decode_image(img_path)\n",
    "            img = tf.image.resize(img, target_size)\n",
    "            img = preprocess_function(img)\n",
    "            imgs.append(img)\n",
    "        imgs = np.asarray(imgs)    \n",
    "        return imgs\n",
    "        \n",
    "    \n",
    "    num_imgs = len(images)\n",
    "    # 52//32=1\n",
    "    num_of_steps = num_imgs//batch_size\n",
    "#     tf.image.flip_left_right()\n",
    "    while True:\n",
    "        for step in range(num_of_steps):\n",
    "            left = step*batch_size\n",
    "            right = (step+1)*batch_size\n",
    "            batch_imgs = images[left:right]\n",
    "            Image.open(batch_imgs[1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs,label_dict= load_img_from_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1005"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/workspace/dataset/face_ms1m/'\n",
    "path2 = '/workspace/dataset/face_ms1m1/'\n",
    "di = os.listdir('/workspace/dataset/face_ms1m/')\n",
    "di1 = [path1 + x for x in di]\n",
    "di2 = [path2 + x for x in di]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    !cp -r {di1[i]} {di2[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006\n"
     ]
    }
   ],
   "source": [
    "!ls -l {path1} | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=model.inputs, outputs=model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.advanced_activations.PReLU at 0x7f1e482cd438>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1e4829f048>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1e48187438>,\n",
       " <keras.layers.merge.Add at 0x7f1e481870b8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1e4813dac8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1e480bd2e8>,\n",
       " <keras.layers.advanced_activations.PReLU at 0x7f1e48093748>,\n",
       " <keras.layers.convolutional.DepthwiseConv2D at 0x7f1e187ef978>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1e186bca90>,\n",
       " <keras.layers.core.Reshape at 0x7f1e186f1a20>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_model.layers[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.normalization.BatchNormalization at 0x7f1e48187438>,\n",
       " <keras.layers.merge.Add at 0x7f1e481870b8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1e4813dac8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1e480bd2e8>,\n",
       " <keras.layers.advanced_activations.PReLU at 0x7f1e48093748>,\n",
       " <keras.layers.convolutional.DepthwiseConv2D at 0x7f1e187ef978>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1e186bca90>,\n",
       " <keras.layers.core.Reshape at 0x7f1e186f1a20>,\n",
       " <keras.engine.input_layer.InputLayer at 0x7f20303f95c0>,\n",
       " <__main__.ArcFace at 0x7f1e186f1940>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_model.save(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-69d4a76c522d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "val_model = keras.models.Model(inputs=model.inputs[0], outputs=model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.44795601e-02, -4.93897684e-02,  2.67045256e-02,\n",
       "         8.79481658e-02,  2.96527278e-02, -6.36440068e-02,\n",
       "        -1.64403915e-01,  4.86411192e-02, -9.89624858e-02,\n",
       "         2.93357838e-02, -9.46226418e-02,  6.86199367e-02,\n",
       "         5.50536290e-02, -1.61739945e-01, -2.71783918e-02,\n",
       "        -7.85409659e-02,  1.65611416e-01,  4.48400974e-02,\n",
       "         1.47263050e-01, -6.74478635e-02, -9.54268351e-02,\n",
       "        -2.76568793e-02,  5.85431345e-02,  5.76006174e-02,\n",
       "         7.49714226e-02, -2.17964649e-01,  8.00830424e-02,\n",
       "         8.17273408e-02,  1.82649910e-01,  2.42473893e-02,\n",
       "        -2.83653680e-02, -9.91192367e-03, -1.64037757e-02,\n",
       "        -5.67956381e-02,  4.97903861e-02,  2.27088425e-02,\n",
       "        -1.09187104e-01,  4.13182788e-02, -8.38868022e-02,\n",
       "        -2.12932900e-02, -4.02032472e-02, -1.77931562e-02,\n",
       "         6.15943000e-02,  2.36417316e-02,  1.49886727e-01,\n",
       "         6.56394139e-02,  1.42375752e-01, -9.92162302e-02,\n",
       "         4.71796170e-02, -1.21848896e-01, -9.61284619e-03,\n",
       "        -2.87576914e-02,  1.48720253e-04, -5.43487668e-02,\n",
       "         5.00987098e-02,  7.63343573e-02,  1.18309692e-01,\n",
       "        -7.95019716e-02, -2.13648006e-02, -2.49229632e-02,\n",
       "        -5.36924042e-02,  6.14891909e-02, -2.58131295e-01,\n",
       "         2.52188798e-02, -6.72347620e-02,  5.07386811e-02,\n",
       "        -9.03601479e-03,  9.53509659e-03,  8.59392807e-02,\n",
       "         2.26140898e-02,  4.14946489e-02,  1.21049978e-01,\n",
       "         1.63233858e-02,  2.75626272e-01, -2.06917569e-01,\n",
       "        -9.02201682e-02,  4.31087837e-02,  1.03664614e-01,\n",
       "         5.51117882e-02, -3.44727486e-02, -2.07319483e-02,\n",
       "         1.52674377e-01, -1.96714289e-02,  1.03933904e-02,\n",
       "         1.09822273e-01, -2.13580057e-02, -3.64402309e-02,\n",
       "        -6.58739656e-02, -8.18317458e-02, -1.23924479e-01,\n",
       "        -4.30930443e-02,  3.34773585e-02,  4.89080437e-02,\n",
       "        -8.05490371e-03, -1.30935177e-01,  2.59593427e-02,\n",
       "         7.13872164e-02,  1.15726609e-02, -6.54073060e-02,\n",
       "        -1.37492660e-02, -5.39877862e-02, -3.23853865e-02,\n",
       "        -1.48977757e-01, -1.14537872e-01, -3.53233032e-02,\n",
       "        -4.31374535e-02, -4.03475426e-02, -5.55431619e-02,\n",
       "         1.87721774e-01,  2.69049317e-01,  1.42596871e-01,\n",
       "         8.81274566e-02,  7.79791996e-02, -6.82223737e-02,\n",
       "         4.21132408e-02,  2.78130677e-02, -4.77642566e-02,\n",
       "         7.70573914e-02, -1.16495248e-02,  3.87428217e-02,\n",
       "         3.55157927e-02, -6.35443926e-02,  1.17392251e-02,\n",
       "        -6.21941909e-02,  2.56494638e-02, -1.04875803e-01,\n",
       "        -1.86775476e-02,  5.81430830e-02]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# val_model = keras.models.load_model(MODEL_FILE)\n",
    "img = cv2.imread('/home/cmf/workspace/Anthony_Hopkins_0001.jpg')\n",
    "img = cv2.resize(img,(112,112))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = np.asarray(img)\n",
    "img = np.reshape(img,(1,112,112,3))\n",
    "img = (img-127.5)/128\n",
    "val_model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:96: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 325 variables.\n",
      "INFO:tensorflow:Converted 325 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15010176"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(MODEL_FILE)\n",
    "tflite_model = converter.convert()\n",
    "open(LITE_FILE, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS: \n",
      "[{'name': 'input_1', 'index': 160, 'shape': array([  1, 112, 112,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\n",
      "OUTPUTS: \n",
      "[{'name': 'lambda_1/truediv', 'index': 171, 'shape': array([  1, 128], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\n",
      "output:\n",
      "[[ 3.37950550e-02  3.99726667e-02 -4.37457301e-02  4.19396907e-02\n",
      "   4.68057618e-02 -3.24871279e-02 -1.04737123e-02  1.19938217e-01\n",
      "   3.62994410e-02  2.50157062e-02 -2.67436486e-02 -8.95689428e-02\n",
      "  -4.46092617e-03  1.84888896e-02 -9.00549740e-02 -3.59047428e-02\n",
      "   1.53528224e-03  1.12368574e-03 -2.26208121e-02  4.83164005e-02\n",
      "  -8.76553357e-03  7.04717869e-03 -3.21341306e-02  8.03467557e-02\n",
      "   8.17857832e-02 -8.75848606e-02 -7.32383644e-03 -8.28910843e-02\n",
      "   1.29807025e-01  5.56052104e-02  1.59716502e-01  2.27917865e-01\n",
      "   5.68093657e-02  1.77277148e-01 -2.29551479e-01 -4.27678227e-02\n",
      "  -1.06863130e-03  9.63932574e-02 -6.19358979e-02 -5.01528643e-02\n",
      "   1.22482516e-01 -4.90806289e-02  2.08056755e-02  1.86441727e-02\n",
      "   6.80646077e-02 -3.68941091e-02 -4.09982651e-02  9.93448719e-02\n",
      "   1.65397301e-01 -8.96675289e-02 -6.59749843e-03  2.31437776e-02\n",
      "   2.40363404e-02  6.93397373e-02  2.97459234e-02 -3.01251318e-02\n",
      "   4.81069349e-02 -4.82892357e-02  1.72230229e-01  1.20264599e-02\n",
      "  -1.87094677e-02 -4.05861586e-02  6.31441399e-02  1.37712911e-01\n",
      "   1.33985147e-01 -1.66982889e-01 -2.02525616e-01  4.64190496e-03\n",
      "   1.54700447e-02 -1.23149760e-01 -1.03522660e-02 -2.57884681e-01\n",
      "   7.70666897e-02  8.70905071e-03 -3.20191719e-02 -2.05918811e-02\n",
      "  -1.99634787e-02 -2.38262594e-01  4.21910323e-02 -4.58161143e-04\n",
      "  -5.73938303e-02  8.00295323e-02 -1.18074799e-03  1.63448844e-02\n",
      "  -4.49287482e-02 -5.36499359e-02 -1.11085102e-01 -6.00851849e-02\n",
      "   8.54848549e-02  3.41756754e-02  1.23940455e-02  5.53250201e-02\n",
      "   3.80106503e-03 -1.05246417e-01  8.31958000e-03 -3.13085109e-01\n",
      "   7.83452839e-02  1.22022264e-01 -3.50346528e-02  9.50880290e-04\n",
      "   2.26314459e-03  2.29442164e-01  1.37514779e-02 -9.72715789e-04\n",
      "  -2.36101430e-02  1.44762993e-01 -4.63983789e-02  3.35797928e-02\n",
      "   6.64600059e-02 -1.71196051e-02  1.12808593e-05 -7.00011328e-02\n",
      "   6.39237836e-02  1.84025597e-02  9.22303926e-03  9.82939154e-02\n",
      "   5.97464910e-04 -1.14094429e-02  1.40978331e-02  1.05907931e-03\n",
      "   8.47442448e-02 -2.46337485e-02  1.71806425e-01  2.61908341e-02\n",
      "  -5.17003015e-02 -4.44637127e-02  3.97866778e-02 -1.67115569e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=LITE_FILE)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print('INPUTS: ')\n",
    "print(input_details)\n",
    "print('OUTPUTS: ')\n",
    "print(output_details)\n",
    "\n",
    "# Test model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=input_details[0]['dtype'])\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "print('output:')\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
