{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import keras\n",
    "from keras.callbacks import *\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import keras.backend as K\n",
    "\n",
    "model_file = 'keras_model.h5'\n",
    "lite_file = 'keras_model.tflite'\n",
    "\n",
    "num_pictures=490623\n",
    "num_classes=10572\n",
    "batch_size=32\n",
    "target_size=(112,112)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_generator(tfrecord_path='C:/Users/mengf/workspace/MobileFaceNet_TF_i/datasets/faces_webface_112x112/tfrecords/tran.tfrecords',batch_size=32,out_num=10572):\n",
    "    \"\"\"自定义generator\n",
    "    \n",
    "    # Argument\n",
    "        tfrecord_path:\n",
    "        batch_size\n",
    "        out_num: 类别数量，用于生成onehot\n",
    "        \n",
    "    # Return\n",
    "    \n",
    "    \"\"\"\n",
    "    def parse_function(example_proto):\n",
    "        features = {'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                    'label': tf.FixedLenFeature([], tf.int64)}\n",
    "        features = tf.parse_single_example(example_proto, features)\n",
    "        # You can do more image distortion here for training data\n",
    "        img = tf.image.decode_jpeg(features['image_raw'])\n",
    "        img = tf.reshape(img, shape=(112, 112, 3))\n",
    "\n",
    "        #img = tf.py_func(random_rotate_image, [img], tf.uint8)\n",
    "        img = tf.cast(img, dtype=tf.float32)\n",
    "        img = tf.subtract(img, 127.5)\n",
    "        img = tf.multiply(img,  0.0078125)\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        label = tf.cast(features['label'], tf.int64)\n",
    "        label = tf.one_hot(label,out_num)\n",
    "        return img, label\n",
    "    \n",
    "#     config = tf.ConfigProto(allow_soft_placement=True)\n",
    "#     sess = tf.Session(config=config)\n",
    "    sess = K.get_session()\n",
    "    # training datasets api config\n",
    "    tfrecords_f = os.path.join(tfrecord_path)\n",
    "    dataset = tf.data.TFRecordDataset(tfrecords_f)\n",
    "    dataset = dataset.map(parse_function)\n",
    "    dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    # begin iteration\n",
    "    while(True):\n",
    "        sess.run(iterator.initializer)\n",
    "        while True:\n",
    "            try:\n",
    "                images, labels = sess.run(next_element)\n",
    "                for i in range(len(images)):\n",
    "                    images[i,...] = cv2.cvtColor(images[i, ...], cv2.COLOR_RGB2BGR)\n",
    "                yield images,labels\n",
    "            except tf.errors.OutOfRangeError:\n",
    "#                 print(\"End of dataset\")\n",
    "                pass\n",
    "\n",
    "def my_generator_wrapper():\n",
    "    for image,label in my_generator():\n",
    "        yield ([image,label],label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dropout,PReLU,Layer\n",
    "from keras.layers import Activation, BatchNormalization, add, Reshape,DepthwiseConv2D\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.activations import relu\n",
    "from keras.initializers import Constant\n",
    "from keras import regularizers\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "weight_decay = 1e-4  # l2正则化decay常量\n",
    "\n",
    "def flow_wrapper(flow):\n",
    "    \"\"\"自定义wrapper，将(x,y)变成([x,y],y)\"\"\"\n",
    "    while True:\n",
    "        x,y = flow.next()\n",
    "        yield ([x,y],y)\n",
    "\n",
    "def prelu(input, name=''):\n",
    "    \"\"\"自定义prelu\"\"\"\n",
    "    alphas = K.variable(K.constant(0.25,dtype=tf.float32,shape=[input.get_shape()[-1]]),name=name + 'prelu_alphas')\n",
    "    pos = K.relu(input)\n",
    "    neg = alphas * (input - K.abs(input)) * 0.5\n",
    "    return pos + neg\n",
    "\n",
    "cval = Constant(0.25)  # prelu α 初始常量\n",
    "\n",
    "def _conv_block(inputs, filters, kernel, strides):\n",
    "    \"\"\"Convolution Block\n",
    "    This function defines a 2D convolution operation with BN and relu6.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        strides: An integer or tuple/list of 2 integers,\n",
    "            specifying the strides of the convolution along the width and height.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    x = Conv2D(filters, kernel, padding='same', strides=strides, kernel_initializer='glorot_normal',)(inputs)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = PReLU(cval)(x)\n",
    "#     x = Activation(relu)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _bottleneck(inputs, filters, kernel, t, s, r=False):\n",
    "    \"\"\"Bottleneck\n",
    "    This function defines a basic bottleneck structure.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        r: Boolean, Whether to use the residuals.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    tchannel = K.int_shape(inputs)[channel_axis] * t\n",
    "\n",
    "    x = _conv_block(inputs, tchannel, (1, 1), (1, 1))\n",
    "\n",
    "    x = DepthwiseConv2D(kernel, strides=(s, s), depth_multiplier=1, padding='same', kernel_initializer='glorot_normal')(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = PReLU(cval)(x)\n",
    "#     x = Activation(relu)(x)\n",
    "\n",
    "    x = Conv2D(filters, (1, 1), strides=(1, 1), padding='same', kernel_initializer='glorot_normal')(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "\n",
    "    if r:\n",
    "        x = add([x, inputs])\n",
    "    return x\n",
    "\n",
    "\n",
    "def _inverted_residual_block(inputs, filters, kernel, t, strides, n):\n",
    "    \"\"\"Inverted Residual Block\n",
    "    This function defines a sequence of 1 or more identical layers.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        n: Integer, layer repeat times.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    x = _bottleneck(inputs, filters, kernel, t, strides)\n",
    "\n",
    "    for i in range(1, n):\n",
    "        x = _bottleneck(x, filters, kernel, t, 1, True)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class ArcFace(Layer):\n",
    "    \"\"\"改进的softmax，得出的结果再与真是结果之间求交叉熵\"\"\"\n",
    "    def __init__(self, n_classes=10, s=30.0, m=0.50, regularizer=None, **kwargs):\n",
    "        super(ArcFace, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcFace, self).build(input_shape[0])\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                shape=(input_shape[0][-1], self.n_classes),\n",
    "                                initializer='glorot_uniform',\n",
    "                                trainable=True,\n",
    "                                regularizer=self.regularizer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, y = inputs # x为embeddings，y为labels\n",
    "        c = K.shape(x)[-1]  # 特征维度\n",
    "        # normalize feature\n",
    "        x = tf.nn.l2_normalize(x, axis=1)\n",
    "        # normalize weights\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "        # dot product\n",
    "        # 全连接层，x的结构为（None，128）w的结构为（128，n_classes）。logits的结构为(None,n_classes)\n",
    "        # (np.random.randn(5,128) @ np.random.randn(128,10)).shape # (5, 10)\n",
    "        logits = x @ W\n",
    "        # add margin\n",
    "        # clip logits to prevent zero division when backward\n",
    "        theta = tf.acos(K.clip(logits, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n",
    "        target_logits = tf.cos(theta + self.m)\n",
    "        # sin = tf.sqrt(1 - logits**2)\n",
    "        # cos_m = tf.cos(logits)\n",
    "        # sin_m = tf.sin(logits)\n",
    "        # target_logits = logits * cos_m - sin * sin_m\n",
    "        #\n",
    "        logits = logits * (1 - y) + target_logits * y\n",
    "        # feature re-scale\n",
    "        logits *= self.s\n",
    "        out = tf.nn.softmax(logits)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.n_classes)\n",
    "\n",
    "def MobileFaceNets(input_shape=(112,112,3), n_classes=10, k=128):\n",
    "    \"\"\"MobileFaceNets\"\"\"\n",
    "    inputs = Input(shape=input_shape) #112x112，(img-127.5)/255\n",
    "    y      = Input(shape=(n_classes,))\n",
    "    x = _conv_block(inputs, 64, (3, 3), strides=(2, 2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # depthwise conv3x3\n",
    "    x = DepthwiseConv2D(3, strides=(1, 1), depth_multiplier=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = PReLU(cval)(x)\n",
    "#     x = Activation(relu)(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 5层bottleneck\n",
    "    x = _inverted_residual_block(x, 64, (3, 3), t=2, strides=2, n=5)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=4, strides=2, n=1)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=2, strides=1, n=6)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=4, strides=2, n=1)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=2, strides=1, n=2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # conv1x1\n",
    "    x = _conv_block(x, 512, (1, 1), strides=(1, 1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # linear GDConv7x7\n",
    "    x = DepthwiseConv2D(7, strides=(1, 1), depth_multiplier=1, padding='valid')(x)\n",
    "    x = Dropout(0.3, name='Dropout')(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = Conv2D(k, (1, 1), padding='same')(x)\n",
    "    x = Reshape((k,))(x)\n",
    "    # x 为embeddings， y为embeddings对应的类别标签，output为\n",
    "    output = ArcFace(n_classes=n_classes, regularizer=regularizers.l2(weight_decay))([x, y])\n",
    "    \n",
    "    model = Model([inputs, y], output)\n",
    "#     plot_model(model, to_file='images/MobileNetv2.png', show_shapes=True)\n",
    "    print(model.input,model.output)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = MobileFaceNets(n_classes=num_classes)\n",
    "model.compile(optimizer='adam',\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    # 当监测值不再改善时，该回调函数将中止训练\n",
    "    # 如发现loss相比上一个epoch训练没有下降），则经过patience个epoch后停止训练。\n",
    "    EarlyStopping(monitor='loss', patience=10, verbose=1),\n",
    "    # 该回调函数将日志信息写入TensorBorad\n",
    "#     TensorBoard(log_dir='./models/logs',histogram_freq=1),\n",
    "    # 当评价指标不在提升时，减少学习率\n",
    "    # min_lr：学习率的下限\n",
    "    ReduceLROnPlateau(monitor='loss',factor=0.2,patience=3,min_lr=0.0001),\n",
    "    # 该回调函数将在每个epoch后保存模型到filepath\n",
    "    ModelCheckpoint(filepath='models/weights-autoencoder-{epoch:02d}-{loss:.2f}.h5',save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit_generator(my_generator_wrapper(),steps_per_epoch=num_pictures//batch_size,epochs=100,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 68543 images belonging to 1005 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((90, 112, 112, 3), 68543, 1005)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(img):\n",
    "    \"\"\"图片预处理，the image is substracted 127.5 and multiplied 1/128.\"\"\"\n",
    "    return (img-127.5)*0.0078125\n",
    "\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess,)\n",
    "\n",
    "flow = datagen.flow_from_directory('/workspace/dataset/face_ms1m1/',target_size=(112,112),batch_size=90,)\n",
    "\n",
    "flow.next()[0].shape,flow.n,flow.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_from_directory(path='/workspace/dataset/face_ms1m1/'):\n",
    "    \"\"\"从path中加载图片的标签和路径\"\"\"\n",
    "    p = Path(path)\n",
    "    # 结构为图片的标签和图片的绝对路径\n",
    "    rets = []\n",
    "    \n",
    "    # 图片父目录文件夹名对应的id\n",
    "    label_dict = {}\n",
    "    \n",
    "    for i,file in enumerate(p.glob('*')):\n",
    "        file_name=str(file.name)\n",
    "        label_dict[file_name]=i\n",
    "    \n",
    "    for i,file in enumerate(p.glob('*/*')):\n",
    "        # 获得图片的父目录名字\n",
    "        parent = str(file.parent.name)\n",
    "        # 图片绝对路径\n",
    "        file_path = str(file)\n",
    "        # 重新设置标签\n",
    "        index = label_dict[parent]\n",
    "        rets.append([index,file_path])\n",
    "    return rets, label_dict\n",
    "\n",
    "def my_generator(images,labels,batch_size=32,preprocess_function=lambda x:(x-127.5)*0.0078125,shuffle=False,target_size=(112,112)):\n",
    "    \"\"\"图片和标签生成器，images为路径，label为标签\"\"\"\n",
    "    \n",
    "    def load_img(imgs_path):\n",
    "        imgs = []\n",
    "        for img_path in imgs_path:\n",
    "            img = tf.image.decode_image(img_path)\n",
    "            img = tf.image.resize(img, target_size)\n",
    "            img = preprocess_function(img)\n",
    "            imgs.append(img)\n",
    "        imgs = np.asarray(imgs)    \n",
    "        return imgs\n",
    "        \n",
    "    \n",
    "    num_imgs = len(images)\n",
    "    # 52//32=1\n",
    "    num_of_steps = num_imgs//batch_size\n",
    "#     tf.image.flip_left_right()\n",
    "    while True:\n",
    "        for step in range(num_of_steps):\n",
    "            left = step*batch_size\n",
    "            right = (step+1)*batch_size\n",
    "            batch_imgs = images[left:right]\n",
    "            Image.open(batch_imgs[1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs,label_dict= load_img_from_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1005"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/workspace/dataset/face_ms1m/'\n",
    "path2 = '/workspace/dataset/face_ms1m1/'\n",
    "di = os.listdir('/workspace/dataset/face_ms1m/')\n",
    "di1 = [path1 + x for x in di]\n",
    "di2 = [path2 + x for x in di]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    !cp -r {di1[i]} {di2[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006\n"
     ]
    }
   ],
   "source": [
    "!ls -l {path1} | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=model.inputs, outputs=model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 325 variables.\n",
      "INFO:tensorflow:Converted 325 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3923212"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(model_file)\n",
    "tflite_model = converter.convert()\n",
    "open(lite_file, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS: \n",
      "[{'name': 'input_1', 'index': 81, 'shape': array([  1, 112, 112,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\n",
      "OUTPUTS: \n",
      "[{'name': 'reshape_1/Reshape', 'index': 115, 'shape': array([  1, 128], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\n",
      "output:\n",
      "[[ 2.04079999e-11  2.82742059e-11 -4.58211351e-11 -5.56394123e-14\n",
      "  -3.91572851e-11 -1.02505678e-11 -2.42160354e-11  4.17224762e-12\n",
      "  -1.25698618e-11  4.44564940e-11  3.76039894e-11 -2.25695504e-11\n",
      "  -4.11849027e-11 -3.03026076e-11 -1.68697938e-11 -4.17013212e-12\n",
      "   6.04923837e-12 -2.29401793e-11  2.21237317e-11 -2.08520666e-11\n",
      "   1.48790737e-11 -9.07520933e-12 -3.46710993e-11  4.28729413e-11\n",
      "  -2.50271748e-11 -2.58911746e-11  2.78507616e-11 -1.55599752e-11\n",
      "  -8.76598620e-12 -3.71855706e-11  1.05097554e-11 -7.10245231e-12\n",
      "  -5.50741310e-12 -3.52396862e-11  2.18869870e-11  1.15694555e-11\n",
      "  -1.38298765e-11  7.41387247e-12 -2.91055756e-11 -4.24613643e-11\n",
      "   2.57873133e-11 -3.23684482e-11  2.56772520e-11 -2.07006357e-11\n",
      "  -2.91546474e-12  8.94034152e-12  2.29441847e-11 -6.72331288e-12\n",
      "   3.55131688e-11  3.29329793e-12 -2.49444528e-11 -1.58962687e-11\n",
      "  -2.47433116e-11  2.88735910e-11  3.64105343e-11 -6.87123136e-11\n",
      "   2.70469931e-11  3.72360857e-11  2.58724934e-11  2.63144125e-11\n",
      "  -3.10395390e-11 -1.02073593e-11 -2.00860301e-11  2.22391428e-11\n",
      "   3.44237937e-11 -1.30422999e-11 -5.19213214e-11  2.01530351e-12\n",
      "  -2.80272958e-11  2.30282217e-11 -1.38255553e-11 -3.68877082e-11\n",
      "   3.17318151e-11 -2.06306985e-11 -1.73982009e-11 -2.23268973e-12\n",
      "   5.24760270e-11 -3.54471001e-11 -2.20746234e-11  6.00234055e-13\n",
      "  -3.15647820e-11 -3.14058085e-11 -5.41182932e-11  1.57830433e-11\n",
      "   4.85685589e-11  3.36118841e-11 -4.44305183e-13 -2.03781280e-11\n",
      "  -3.19174166e-11 -2.69856394e-11  1.32486470e-11 -1.79176517e-11\n",
      "   6.53109511e-12  1.92522682e-11 -1.72178591e-11 -4.78632099e-11\n",
      "  -6.50207926e-12  5.68584849e-12 -8.92293964e-12  3.95293832e-11\n",
      "  -1.11274705e-11 -5.59775247e-11  1.45682320e-11 -2.35604748e-11\n",
      "  -1.18483296e-11  1.78471092e-11  3.50077398e-11 -6.06611844e-11\n",
      "   2.42441067e-11  3.99901223e-11  6.34384992e-12  3.70099481e-12\n",
      "   6.39294227e-13  1.78857762e-11  2.11589392e-11 -5.03419528e-11\n",
      "  -3.57141296e-11 -2.51463711e-11 -4.30055921e-11  1.68086742e-11\n",
      "  -1.86434514e-11  8.13556236e-11 -2.16585153e-11 -1.37955064e-11\n",
      "  -2.51319486e-11 -4.50881381e-11  5.57737918e-11 -3.55613525e-11]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=lite_file)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print('INPUTS: ')\n",
    "print(input_details)\n",
    "print('OUTPUTS: ')\n",
    "print(output_details)\n",
    "\n",
    "# Test model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=input_details[0]['dtype'])\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "print('output:')\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
