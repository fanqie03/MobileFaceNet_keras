{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "class Evaluete(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.validation_data = None\n",
    "        self.model = None\n",
    "        self.count=0\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        \"\"\"A backwards compatibility alias for `on_train_batch_begin`.\"\"\"\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        \"\"\"A backwards compatibility alias for `on_train_batch_end`.\"\"\"\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \"\"\"Called at the start of an epoch.\n",
    "        Subclasses should override for any actions to run. This function should only\n",
    "        be called during train mode.\n",
    "        # Arguments\n",
    "            epoch: integer, index of epoch.\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Called at the end of an epoch.\n",
    "        Subclasses should override for any actions to run. This function should only\n",
    "        be called during train mode.\n",
    "        # Arguments\n",
    "            epoch: integer, index of epoch.\n",
    "            logs: dict, metric results for this training epoch, and for the\n",
    "                validation epoch if validation is performed. Validation result keys\n",
    "                are prefixed with `val_`.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        \"\"\"Called at the beginning of a training batch in `fit` methods.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, has keys `batch` and `size` representing the current\n",
    "                batch number and the size of the batch.\n",
    "        \"\"\"\n",
    "        # For backwards compatibility\n",
    "        self.on_batch_begin(batch, logs=logs)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        \"\"\"Called at the end of a training batch in `fit` methods.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, metric results for this batch.\n",
    "        \"\"\"\n",
    "        # For backwards compatibility\n",
    "        self.on_batch_end(batch, logs=logs)\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        \"\"\"Called at the beginning of a batch in `evaluate` methods.\n",
    "        Also called at the beginning of a validation batch in the `fit` methods,\n",
    "        if validation data is provided.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, has keys `batch` and `size` representing the current\n",
    "                batch number and the size of the batch.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        \"\"\"Called at the end of a batch in `evaluate` methods.\n",
    "        Also called at the end of a validation batch in the `fit` methods,\n",
    "        if validation data is provided.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, metric results for this batch.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        \"\"\"Called at the beginning of a batch in `predict` methods.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, has keys `batch` and `size` representing the current\n",
    "                batch number and the size of the batch.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        \"\"\"Called at the end of a batch in `predict` methods.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            batch: integer, index of batch within the current epoch.\n",
    "            logs: dict, metric results for this batch.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        \"\"\"Called at the beginning of training.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        \"\"\"Called at the end of training.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        \"\"\"Called at the beginning of evaluation or validation.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        \"\"\"Called at the end of evaluation or validation.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        \"\"\"Called at the beginning of prediction.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        \"\"\"Called at the end of prediction.\n",
    "        Subclasses should override for any actions to run.\n",
    "        # Arguments\n",
    "            logs: dict, currently no data is passed to this argument for this method\n",
    "                but that may change in the future.\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    " \n",
    "f=open('a.txt','w')\n",
    "old=sys.stdout #将当前系统输出储存到临时变量\n",
    "sys.stdout=f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import keras\n",
    "from keras.callbacks import *\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import keras.backend as K\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.data_process import parse_function, load_data\n",
    "# from losses.face_losses import arcface_loss\n",
    "# from nets.MobileFaceNet import inference\n",
    "from verification import evaluate\n",
    "from scipy.optimize import brentq\n",
    "from scipy import interpolate\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "MODEL_FILE = 'MobileFaceNet.h5'\n",
    "LITE_FILE  = 'MobileFaceNet.tflite'\n",
    "\n",
    "NUM_PICTURES=490623\n",
    "NUM_CLASSES=10572\n",
    "BATCH_SIZE=90\n",
    "TARGET_SIZE=(112,112)\n",
    "TFRECORD_PATH='/workspace/dataset/faces_webface_112x112/tfrecords/tran.tfrecords'\n",
    "\n",
    "def my_generator(tfrecord_path=TFRECORD_PATH,batch_size=BATCH_SIZE,out_num=NUM_CLASSES):\n",
    "    \"\"\"自定义generator\n",
    "    \n",
    "    # Argument\n",
    "        tfrecord_path:\n",
    "        batch_size\n",
    "        out_num: 类别数量，用于生成onehot\n",
    "        \n",
    "    # Return\n",
    "    \n",
    "    \"\"\"\n",
    "    def parse_function(example_proto):\n",
    "        features = {'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                    'label': tf.FixedLenFeature([], tf.int64)}\n",
    "        features = tf.parse_single_example(example_proto, features)\n",
    "        # You can do more image distortion here for training data\n",
    "        img = tf.image.decode_jpeg(features['image_raw'])\n",
    "        img = tf.reshape(img, shape=(112, 112, 3))\n",
    "\n",
    "        #img = tf.py_func(random_rotate_image, [img], tf.uint8)\n",
    "        img = tf.cast(img, dtype=tf.float32)\n",
    "        img = tf.subtract(img, 127.5)\n",
    "        img = tf.multiply(img,  0.0078125)\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        label = tf.cast(features['label'], tf.int64)\n",
    "#         label = tf.one_hot(label,out_num)\n",
    "#         label = tf.reshape(label,(-1,))\n",
    "#         one_hot = tf.one_hot(label,out_num)\n",
    "        return (img, label)\n",
    "    \n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess = tf.Session(config=config)\n",
    "#     sess = K.get_session()\n",
    "    # training datasets api config\n",
    "    tfrecords_f = os.path.join(tfrecord_path)\n",
    "    dataset = tf.data.TFRecordDataset(tfrecords_f)\n",
    "    dataset = dataset.map(parse_function)\n",
    "#     dataset = dataset.shuffle(buffer_size=5000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    return iterator, next_element, sess\n",
    "    # begin iteration\n",
    "#     while(True):\n",
    "#         sess.run(iterator.initializer)\n",
    "#         while True:\n",
    "#             try:\n",
    "#                 images, labels = sess.run(next_element)\n",
    "# #                 for i in range(len(images)):\n",
    "# #                     images[i,...] = cv2.cvtColor(images[i, ...], cv2.COLOR_RGB2BGR)\n",
    "#                 yield images,labels\n",
    "#             except tf.errors.OutOfRangeError:\n",
    "# #                 print(\"End of dataset\")\n",
    "#                 break\n",
    "\n",
    "\n",
    "def my_generator_wrapper():\n",
    "    for image,label in my_generator():\n",
    "        yield ([image,label],label)\n",
    "#         yield image,label\n",
    "        \n",
    "def test_generator():\n",
    "    a = my_generator_wrapper()\n",
    "    for i in tqdm(range(50000)):\n",
    "        a.__next__()\n",
    "        \n",
    "def learning_rate_schedule(epoch, lr, boundaries, values):\n",
    "    \"\"\"\n",
    "    # Argument:\n",
    "        epoch: now epoch\n",
    "        lr: learning rate to schedule\n",
    "        boundaries: Number of epochs for learning rate piecewise.\n",
    "        values: target value of learning rate\n",
    "    \"\"\"\n",
    "    if epoch <= boundaries[0]:\n",
    "        t = values[0]\n",
    "    for low, high, v in zip(boundaries[:-1],boundaries[1:],values[1:]):\n",
    "        if low < epoch <= high:\n",
    "            t = v\n",
    "    if epoch > boundaries[-1]:\n",
    "        t = values[-1]\n",
    "        \n",
    "    K.get_session().run(lr.assign(t))\n",
    "    return epoch,t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dropout,PReLU,Layer\n",
    "from keras.layers import Activation, BatchNormalization, add, Reshape,DepthwiseConv2D\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.activations import relu\n",
    "from keras.initializers import Constant\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras import backend as K\n",
    "import math\n",
    "\n",
    "weight_decay = 5e-5  # l2正则化decay常量\n",
    "\n",
    "\n",
    "batch_norm_params = {\n",
    "    'center': True,\n",
    "    'scale': True,\n",
    "    'momentum': 0.995,\n",
    "    'epsilon': 2e-5,\n",
    "}\n",
    "\n",
    "\n",
    "def flow_wrapper(flow):\n",
    "    \"\"\"自定义wrapper，将(x,y)变成([x,y],y)\"\"\"\n",
    "    while True:\n",
    "        x,y = flow.next()\n",
    "        yield ([x,y],y)\n",
    "\n",
    "def prelu(input, name=''):\n",
    "    \"\"\"自定义prelu\"\"\"\n",
    "    alphas = K.variable(K.constant(0.25,dtype=tf.float32,shape=[input.get_shape()[-1]]),name=name + 'prelu_alphas')\n",
    "    pos = K.relu(input)\n",
    "    neg = alphas * (input - K.abs(input)) * 0.5\n",
    "    return pos + neg\n",
    "\n",
    "cval = Constant(0.25)  # prelu α 初始常量\n",
    "\n",
    "def _conv_block(inputs, filters, kernel, strides):\n",
    "    \"\"\"Convolution Block\n",
    "    This function defines a 2D convolution operation with BN and relu6.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        strides: An integer or tuple/list of 2 integers,\n",
    "            specifying the strides of the convolution along the width and height.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    x = Conv2D(filters, kernel, padding='same', strides=strides, kernel_initializer='glorot_normal',kernel_regularizer=l2(weight_decay))(inputs)\n",
    "    x = BatchNormalization(axis=channel_axis,**batch_norm_params)(x)\n",
    "    x = PReLU(cval)(x)\n",
    "#     x = Activation(relu)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _bottleneck(inputs, filters, kernel, t, s, r=False):\n",
    "    \"\"\"Bottleneck\n",
    "    This function defines a basic bottleneck structure.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        r: Boolean, Whether to use the residuals.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    tchannel = K.int_shape(inputs)[channel_axis] * t\n",
    "\n",
    "    x = _conv_block(inputs, tchannel, (1, 1), (1, 1))\n",
    "\n",
    "    x = DepthwiseConv2D(kernel, strides=(s, s), depth_multiplier=1, padding='same', kernel_initializer='glorot_normal')(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = PReLU(cval)(x)\n",
    "#     x = Activation(relu)(x)\n",
    "\n",
    "    x = Conv2D(filters, (1, 1), strides=(1, 1), padding='same', kernel_initializer='glorot_normal',kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = BatchNormalization(axis=channel_axis,**batch_norm_params)(x)\n",
    "\n",
    "    if r:\n",
    "        x = add([x, inputs])\n",
    "    return x\n",
    "\n",
    "\n",
    "def _inverted_residual_block(inputs, filters, kernel, t, strides, n):\n",
    "    \"\"\"Inverted Residual Block\n",
    "    This function defines a sequence of 1 or more identical layers.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        n: Integer, layer repeat times.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    x = _bottleneck(inputs, filters, kernel, t, strides)\n",
    "\n",
    "    for i in range(1, n):\n",
    "        x = _bottleneck(x, filters, kernel, t, 1, True)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class ArcFace(Layer):\n",
    "    \"\"\"改进的softmax，得出的结果再与真是结果之间求交叉熵\"\"\"\n",
    "    def __init__(self, n_classes=10, s=64.0, m=0.50, regularizer=None, **kwargs):\n",
    "        super(ArcFace, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcFace, self).build(input_shape[0])\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                shape=(input_shape[0][-1], self.n_classes),\n",
    "                                initializer='glorot_uniform',\n",
    "                                trainable=True,\n",
    "                                regularizer=self.regularizer)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x, y = inputs # x为embeddings，y为labels\n",
    "#         c = K.shape(x)[-1]  # 特征维度\n",
    "#         # 1. normalize feature\n",
    "#         x = tf.nn.l2_normalize(x, axis=1)\n",
    "#         # 2. normalize weights\n",
    "#         W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "#         # dot product\n",
    "#         # 全连接层，x的结构为（None，128）w的结构为（128，n_classes）。logits的结构为(None,n_classes)\n",
    "#         # (np.random.randn(5,128) @ np.random.randn(128,10)).shape # (5, 10)\n",
    "#         # 3. 计算xW得到预测向量y\n",
    "#         logits = x @ W\n",
    "#         # add margin\n",
    "#         # clip logits to prevent zero division when backward\n",
    "#         theta = tf.acos(K.clip(logits, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n",
    "#         target_logits = tf.cos(theta + self.m)\n",
    "#         # sin = tf.sqrt(1 - logits**2)\n",
    "#         # cos_m = tf.cos(logits)\n",
    "#         # sin_m = tf.sin(logits)\n",
    "#         # target_logits = logits * cos_m - sin * sin_m\n",
    "#         logits = logits * (1 - y) + target_logits * y\n",
    "#         # feature re-scale\n",
    "#         # 9. 对所有值乘上固定值s\n",
    "#         logits *= self.s\n",
    "#         out = tf.nn.softmax(logits)\n",
    "#         print(out)\n",
    "#         return out\n",
    "\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        embedding, labels = inputs\n",
    "        labels = tf.reshape(labels,shape=(-1,))\n",
    "        print('labels:',labels)\n",
    "        \n",
    "        out_num = self.n_classes\n",
    "        w_init=None\n",
    "        s=64.\n",
    "        m=0.5\n",
    "        \n",
    "        cos_m = tf.cos(m)\n",
    "        sin_m = tf.sin(m)\n",
    "        mm = sin_m * m  # issue 1\n",
    "        threshold = tf.cos(math.pi - m)\n",
    "        with tf.variable_scope('arcface_loss'):\n",
    "            # inputs and weights norm\n",
    "            embedding_norm = tf.norm(embedding, axis=1, keepdims=True)\n",
    "            embedding = tf.div(embedding, embedding_norm, name='norm_embedding')\n",
    "#             weights = tf.get_variable(name='embedding_weights', shape=(embedding.get_shape().as_list()[-1], out_num),\n",
    "#                                       initializer=w_init, dtype=tf.float32)\n",
    "            weights = self.W\n",
    "            weights_norm = tf.norm(weights, axis=0, keepdims=True)\n",
    "            weights = tf.div(weights, weights_norm, name='norm_weights')\n",
    "            # cos(theta+m)\n",
    "            cos_t = tf.matmul(embedding, weights, name='cos_t')\n",
    "            cos_t2 = tf.square(cos_t, name='cos_2')\n",
    "            sin_t2 = tf.subtract(1., cos_t2, name='sin_2')\n",
    "            sin_t = tf.sqrt(sin_t2, name='sin_t')\n",
    "            cos_mt = s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')\n",
    "\n",
    "            # this condition controls the theta+m should in range [0, pi]\n",
    "            #      0<=theta+m<=pi\n",
    "            #     -m<=theta<=pi-m\n",
    "            cond_v = cos_t - threshold\n",
    "            cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)\n",
    "\n",
    "            keep_val = s*(cos_t - mm)\n",
    "            cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n",
    "            print('labels:',labels,'out_num',out_num)\n",
    "            mask = tf.one_hot(labels, depth=out_num, name='one_hot_mask')\n",
    "#             mask = tf.reshape(mask,(-1,))\n",
    "#             mask = labels\n",
    "            print(mask)\n",
    "            # mask = tf.squeeze(mask, 1)\n",
    "            inv_mask = tf.subtract(1., mask, name='inverse_mask')\n",
    "            print(inv_mask)\n",
    "            s_cos_t = tf.multiply(s, cos_t, name='scalar_cos_t')\n",
    "            print(s_cos_t)\n",
    "            logit = tf.add(tf.multiply(s_cos_t, inv_mask), tf.multiply(cos_mt_temp, mask), name='arcface_loss_output')\n",
    "            print(logit)\n",
    "#             inference_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=labels))\n",
    "            inference_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=labels)\n",
    "            print(inference_loss)\n",
    "#             inference_loss = tf.nn.softmax(logit)\n",
    "            print(inference_loss)\n",
    "        return inference_loss\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.n_classes)\n",
    "\n",
    "def MobileFaceNets(input_shape=(112,112,3), n_classes=10, k=128):\n",
    "    \"\"\"MobileFaceNets\"\"\"\n",
    "    inputs = Input(shape=input_shape) #112x112，(img-127.5)/255\n",
    "    y      = Input(shape=(1,), dtype=tf.int32)\n",
    "#     y      = Input(shape=(n_classes,))\n",
    "    x = _conv_block(inputs, 64, (3, 3), strides=(2, 2))\n",
    "    \n",
    "    # depthwise conv3x3\n",
    "    x = DepthwiseConv2D(3, strides=(1, 1), depth_multiplier=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = PReLU(cval)(x)\n",
    "#     x = Activation(relu)(x)\n",
    "    \n",
    "    # 5层bottleneck\n",
    "    x = _inverted_residual_block(x, 64, (3, 3), t=2, strides=2, n=5)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=4, strides=2, n=1)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=2, strides=1, n=6)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=4, strides=2, n=1)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=2, strides=1, n=2)\n",
    "    \n",
    "    # conv1x1\n",
    "    x = _conv_block(x, 512, (1, 1), strides=(1, 1))\n",
    "    \n",
    "    # linear GDConv7x7\n",
    "    x = DepthwiseConv2D(7, strides=(1, 1), depth_multiplier=1, padding='valid')(x)\n",
    "#     x = Dropout(0.3, name='Dropout')(x)\n",
    "    \n",
    "    x = Conv2D(k, (1, 1), padding='same',kernel_initializer='glorot_normal',kernel_regularizer=l2(1e-10))(x)\n",
    "#     x = Activation(keras.activations.re)\n",
    "    x = Reshape((k,))(x)\n",
    "#     x = tf.nn.l2_normalize(x, 1, 1e-10, name='embeddings')\n",
    "    \n",
    "    # x 为embeddings， y为embeddings对应的类别标签，output为\n",
    "    output = ArcFace(n_classes=n_classes, regularizer=None)([x, y])\n",
    "    \n",
    "    model = Model([inputs, y], output)\n",
    "#     plot_model(model, to_file='images/MobileNetv2.png', show_shapes=True)\n",
    "    print(model.input,model.output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred)\n",
    "\n",
    "model = MobileFaceNets(n_classes=NUM_CLASSES)\n",
    "model.compile(optimizer=keras.optimizers.Adam(0.1,beta_1=0.9, beta_2=0.999, epsilon=0.1),\n",
    "      loss=my_loss,\n",
    "      metrics=['accuracy'])\n",
    "# model.compile(optimizer=keras.optimizers.sgd(0.1),loss=my_loss,metrics=['accuracy'])\n",
    "\n",
    "val_model = keras.models.Model(inputs=model.inputs[0], outputs=model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(53.444035, 0.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = my_generator_wrapper()\n",
    "images_train, labels_train = g.__next__()\n",
    "loss, accuracy = model.train_on_batch(images_train, labels_train)\n",
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    # 当监测值不再改善时，该回调函数将中止训练\n",
    "    # 如发现loss相比上一个epoch训练没有下降），则经过patience个epoch后停止训练。\n",
    "    EarlyStopping(monitor='loss', patience=10, verbose=1),\n",
    "    # 该回调函数将日志信息写入TensorBorad\n",
    "#     TensorBoard(log_dir='./models/logs',histogram_freq=1),\n",
    "    # 当评价指标不在提升时，减少学习率\n",
    "    # min_lr：学习率的下限\n",
    "    ReduceLROnPlateau(monitor='loss',factor=0.2,patience=3,min_lr=0.0001),\n",
    "    # 该回调函数将在每个epoch后保存模型到filepath\n",
    "    ModelCheckpoint(filepath='models/weights-autoencoder-{epoch:02d}-{loss:.2f}.h5',save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresholds max: 3.99 <=> min: 0.79\n"
     ]
    }
   ],
   "source": [
    "tpr, fpr, accuracy, val, val_std, far = evaluate(emb_array, issame_list, nrof_folds=eval_nrof_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,  lr:(0, 0.1)\n",
      "epoch:1,  lr:(1, 0.1)\n",
      "epoch:2,  lr:(2, 0.1)\n",
      "epoch:3,  lr:(3, 0.1)\n",
      "epoch:4,  lr:(4, 0.1)\n",
      "epoch:5,  lr:(5, 0.01)\n",
      "epoch:6,  lr:(6, 0.01)\n",
      "epoch:7,  lr:(7, 0.01)\n",
      "epoch:8,  lr:(8, 0.001)\n",
      "epoch:9,  lr:(9, 0.001)\n",
      "epoch:10,  lr:(10, 1e-04)\n",
      "epoch:11,  lr:(11, 1e-04)\n",
      "epoch:12,  lr:(12, 1e-05)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1e-05"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning_rate_schedule\n",
    "\n",
    "lr_schedule = [4, 7, 9, 11]\n",
    "values=[0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "def learning_rate_schedule(epoch, lr, boundaries, values):\n",
    "    \"\"\"\n",
    "    # Argument:\n",
    "        epoch: now epoch\n",
    "        lr: learning rate to schedule\n",
    "        boundaries: Number of epochs for learning rate piecewise.\n",
    "        values: target value of learning rate\n",
    "    \"\"\"\n",
    "    if epoch <= boundaries[0]:\n",
    "        t = values[0]\n",
    "    for low, high, v in zip(boundaries[:-1],boundaries[1:],values[1:]):\n",
    "        if low < epoch <= high:\n",
    "            t = v\n",
    "    if epoch > boundaries[-1]:\n",
    "        t = values[-1]\n",
    "        \n",
    "    K.get_session().run(lr.assign(t))\n",
    "    return epoch,K.get_session().run(lr)\n",
    "\n",
    "lr = 0\n",
    "\n",
    "for i in range(13):\n",
    "    lr = learning_rate_schedule(i,model.optimizer.lr,lr_schedule,values)\n",
    "    print('epoch:{},  lr:{}'.format(i,lr))\n",
    "\n",
    "# K.get_session().run(model.optimizer.lr.assign(0.001))\n",
    "K.get_session().run(model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, lr:0.1\n",
      "End of epoch 0\n",
      "epoch:1, lr:0.1\n",
      "End of epoch 1\n",
      "epoch:2, lr:0.1\n",
      "End of epoch 2\n",
      "epoch:3, lr:0.1\n"
     ]
    }
   ],
   "source": [
    "# test_iterator\n",
    "iterator, next_element, g_sess = my_generator()\n",
    "\n",
    "# epoch = -1\n",
    "count = 0\n",
    "total_accuracy = {}\n",
    "for i in range(max_epoch):\n",
    "    g_sess.run(iterator.initializer)\n",
    "    _, lr = learning_rate_schedule(i,model.optimizer.lr,lr_schedule,values)\n",
    "    print('epoch:{}, lr:{}'.format(i, lr))\n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            images_train, labels_train = g_sess.run(next_element)\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of epoch %d\" % i)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 90\n",
    "test_batch_size = 100\n",
    "eval_datasets = ['lfw']\n",
    "eval_db_path = '/workspace/dataset/faces_webface_112x112/'\n",
    "eval_nrof_folds = 10\n",
    "tfrecords_file_path = '/workspace/dataset/faces_webface_112x112/tfrecords/'\n",
    "summary_path = '/workspace/output/summary'\n",
    "ckpt_path = '/workspace/output/ckpt'\n",
    "pretrained_model = False\n",
    "log_file_path = '/workspace/output/logs'\n",
    "ckpt_best_path = '/workspace/output/ckpt_best'\n",
    "saver_maxkeep = 50\n",
    "summary_interval = 400\n",
    "ckpt_interval = 200\n",
    "validate_interval = 500\n",
    "show_info_interval = 50\n",
    "log_device_mapping = False\n",
    "log_histograms = False\n",
    "prelogits_norm_loss_factor = 2e-5\n",
    "prelogits_norm_p = 1.0\n",
    "max_epoch = 12\n",
    "image_size = [112, 112]\n",
    "embedding_size = 128\n",
    "\n",
    "lr_schedule = [4, 7, 9, 11]\n",
    "values=[0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "# prepare validate datasets\n",
    "ver_list = []\n",
    "ver_name_list = []\n",
    "for db in eval_datasets:\n",
    "    print('begin db %s convert.' % db)\n",
    "    data_set = load_data(db, image_size, eval_db_path)\n",
    "    ver_list.append(data_set)\n",
    "    ver_name_list.append(db)\n",
    "\n",
    "# output file path\n",
    "if not os.path.exists(log_file_path):\n",
    "    os.makedirs(log_file_path)\n",
    "if not os.path.exists(ckpt_best_path):\n",
    "    os.makedirs(ckpt_best_path)\n",
    "if not os.path.exists(ckpt_path):\n",
    "    os.makedirs(ckpt_path)\n",
    "# create log dir\n",
    "subdir = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "log_dir = os.path.join(os.path.expanduser(log_file_path), subdir)\n",
    "if not os.path.isdir(log_dir):  # Create the log directory if it doesn't exist\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "# g = my_generator_wrapper()\n",
    "iterator, next_element, g_sess = my_generator()\n",
    "\n",
    "# epoch = -1\n",
    "count = 0\n",
    "total_accuracy = {}\n",
    "for i in range(max_epoch):\n",
    "    # 调整学习率\n",
    "    _, lr = learning_rate_schedule(i,model.optimizer.lr,lr_schedule,values)\n",
    "    print('epoch:{}, lr:{}'.format(i, lr))\n",
    "    # 初始化迭代器\n",
    "    g_sess.run(iterator.initializer)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            x, y = g_sess.run(next_element)\n",
    "            \n",
    "            images_train,labels_train = ([x,y],y)\n",
    "\n",
    "            start = time.time()\n",
    "#             _, total_loss_val, inference_loss_val, reg_loss_val, _, acc_val = \\\n",
    "#             sess.run([train_op, total_loss, inference_loss, regularization_losses, inc_global_step_op, Accuracy_Op],\n",
    "#                      feed_dict=feed_dict)\n",
    "            loss, accuracy = model.train_on_batch(images_train, labels_train)\n",
    "    \n",
    "            end = time.time()\n",
    "            pre_sec = train_batch_size/(end - start)\n",
    "\n",
    "            count += 1\n",
    "            # print training information\n",
    "            if count > 0 and count % show_info_interval == 0:\n",
    "#                 print('epoch %d, total_step %d, total loss is %.2f , inference loss is %.2f, reg_loss is %.2f, training accuracy is %.6f, time %.3f samples/sec' %\n",
    "#                       (i, count, total_loss_val, inference_loss_val, np.sum(reg_loss_val), acc_val, pre_sec))\n",
    "                print('epoch %d, total_step %d, loss is %.6f, training accuracy is %.6f, time %.3f samples/sec' %\n",
    "                      (i, count, loss, accuracy, pre_sec))\n",
    "\n",
    "            # save summary\n",
    "#             if count > 0 and count % summary_interval == 0:\n",
    "#                 feed_dict = {inputs: images_train, labels: labels_train, phase_train_placeholder: True}\n",
    "#                 summary_op_val = sess.run(summary_op, feed_dict=feed_dict)\n",
    "#                 summary.add_summary(summary_op_val, count)\n",
    "\n",
    "            # save ckpt files\n",
    "            if count > 0 and count % ckpt_interval == 0:\n",
    "#                 filename = 'MobileFaceNet_iter_{:d}'.format(count) + '.ckpt'\n",
    "                filename = 'MobileFaceNet_iter_{:d}'.format(count) + '.h5'\n",
    "                filename = os.path.join(ckpt_path, filename)\n",
    "                val_model.save(filename)\n",
    "\n",
    "            # validate\n",
    "            if count > 0 and count % validate_interval == 0:\n",
    "                print('\\nIteration', count, 'testing...')\n",
    "                for db_index in range(len(ver_list)):\n",
    "                    start_time = time.time()\n",
    "                    data_sets, issame_list = ver_list[db_index]\n",
    "                    emb_array = np.zeros((data_sets.shape[0], embedding_size))\n",
    "                    nrof_batches = data_sets.shape[0] // test_batch_size\n",
    "                    for index in range(nrof_batches): # actual is same multiply 2, test data total\n",
    "                        start_index = index * test_batch_size\n",
    "                        end_index = min((index + 1) * test_batch_size, data_sets.shape[0])\n",
    "\n",
    "#                         feed_dict = {inputs: data_sets[start_index:end_index, ...], phase_train_placeholder: False}\n",
    "#                         emb_array[start_index:end_index, :] = sess.run(embeddings, feed_dict=feed_dict)\n",
    "                        emb_array[start_index:end_index, :] = val_model.predict(data_sets[start_index:end_index, ...])\n",
    "\n",
    "                    tpr, fpr, accuracy, val, val_std, far = evaluate(emb_array, issame_list, nrof_folds=eval_nrof_folds)\n",
    "                    duration = time.time() - start_time\n",
    "\n",
    "                    print(\"total time %.3fs to evaluate %d images of %s\" % (duration, data_sets.shape[0], ver_name_list[db_index]))\n",
    "                    print('Accuracy: %1.3f+-%1.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "                    print('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far))\n",
    "                    print('fpr and tpr: %1.3f %1.3f' % (np.mean(fpr, 0), np.mean(tpr, 0)))\n",
    "\n",
    "                    auc = metrics.auc(fpr, tpr)\n",
    "                    print('Area Under Curve (AUC): %1.3f' % auc)\n",
    "#                     eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\n",
    "#                     print('Equal Error Rate (EER): %1.3f\\n' % eer)\n",
    "\n",
    "                    with open(os.path.join(log_dir, '{}_result.txt'.format(ver_name_list[db_index])), 'at') as f:\n",
    "                        f.write('%d\\t%.5f\\t%.5f\\n' % (count, np.mean(accuracy), val))\n",
    "\n",
    "                    if ver_name_list == 'lfw' and np.mean(accuracy) > 0.992:\n",
    "                        print('best accuracy is %.5f' % np.mean(accuracy))\n",
    "                        filename = 'MobileFaceNet_iter_best_{:d}'.format(count) + '.ckpt'\n",
    "                        filename = os.path.join(ckpt_best_path, filename)\n",
    "                        saver.save(sess, filename)\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of epoch %d\" % i)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 68543 images belonging to 1005 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((90, 112, 112, 3), 68543, 1005)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(img):\n",
    "    \"\"\"图片预处理，the image is substracted 127.5 and multiplied 1/128.\"\"\"\n",
    "    return (img-127.5)*0.0078125\n",
    "\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess,)\n",
    "\n",
    "flow = datagen.flow_from_directory('/workspace/dataset/face_ms1m1/',target_size=(112,112),batch_size=90,)\n",
    "\n",
    "flow.next()[0].shape,flow.n,flow.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_from_directory(path='/workspace/dataset/face_ms1m1/'):\n",
    "    \"\"\"从path中加载图片的标签和路径\"\"\"\n",
    "    p = Path(path)\n",
    "    # 结构为图片的标签和图片的绝对路径\n",
    "    rets = []\n",
    "    \n",
    "    # 图片父目录文件夹名对应的id\n",
    "    label_dict = {}\n",
    "    \n",
    "    for i,file in enumerate(p.glob('*')):\n",
    "        file_name=str(file.name)\n",
    "        label_dict[file_name]=i\n",
    "    \n",
    "    for i,file in enumerate(p.glob('*/*')):\n",
    "        # 获得图片的父目录名字\n",
    "        parent = str(file.parent.name)\n",
    "        # 图片绝对路径\n",
    "        file_path = str(file)\n",
    "        # 重新设置标签\n",
    "        index = label_dict[parent]\n",
    "        rets.append([index,file_path])\n",
    "    return rets, label_dict\n",
    "\n",
    "def my_generator(images,labels,batch_size=32,preprocess_function=lambda x:(x-127.5)*0.0078125,shuffle=False,target_size=(112,112)):\n",
    "    \"\"\"图片和标签生成器，images为路径，label为标签\"\"\"\n",
    "    \n",
    "    def load_img(imgs_path):\n",
    "        imgs = []\n",
    "        for img_path in imgs_path:\n",
    "            img = tf.image.decode_image(img_path)\n",
    "            img = tf.image.resize(img, target_size)\n",
    "            img = preprocess_function(img)\n",
    "            imgs.append(img)\n",
    "        imgs = np.asarray(imgs)    \n",
    "        return imgs\n",
    "        \n",
    "    \n",
    "    num_imgs = len(images)\n",
    "    # 52//32=1\n",
    "    num_of_steps = num_imgs//batch_size\n",
    "#     tf.image.flip_left_right()\n",
    "    while True:\n",
    "        for step in range(num_of_steps):\n",
    "            left = step*batch_size\n",
    "            right = (step+1)*batch_size\n",
    "            batch_imgs = images[left:right]\n",
    "            Image.open(batch_imgs[1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs,label_dict= load_img_from_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1005"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/workspace/dataset/face_ms1m/'\n",
    "path2 = '/workspace/dataset/face_ms1m1/'\n",
    "di = os.listdir('/workspace/dataset/face_ms1m/')\n",
    "di1 = [path1 + x for x in di]\n",
    "di2 = [path2 + x for x in di]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    !cp -r {di1[i]} {di2[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006\n"
     ]
    }
   ],
   "source": [
    "!ls -l {path1} | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=model.inputs, outputs=model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.advanced_activations.PReLU at 0x7f1e482cd438>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1e4829f048>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1e48187438>,\n",
       " <keras.layers.merge.Add at 0x7f1e481870b8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1e4813dac8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1e480bd2e8>,\n",
       " <keras.layers.advanced_activations.PReLU at 0x7f1e48093748>,\n",
       " <keras.layers.convolutional.DepthwiseConv2D at 0x7f1e187ef978>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1e186bca90>,\n",
       " <keras.layers.core.Reshape at 0x7f1e186f1a20>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_model.layers[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.normalization.BatchNormalization at 0x7f1e48187438>,\n",
       " <keras.layers.merge.Add at 0x7f1e481870b8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1e4813dac8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1e480bd2e8>,\n",
       " <keras.layers.advanced_activations.PReLU at 0x7f1e48093748>,\n",
       " <keras.layers.convolutional.DepthwiseConv2D at 0x7f1e187ef978>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1e186bca90>,\n",
       " <keras.layers.core.Reshape at 0x7f1e186f1a20>,\n",
       " <keras.engine.input_layer.InputLayer at 0x7f20303f95c0>,\n",
       " <__main__.ArcFace at 0x7f1e186f1940>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_model.save(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-137.44328  ,  -68.79495  ,  -99.532005 ,  -46.580643 ,\n",
       "           2.5137744,   78.63011  ,  -18.590235 ,  181.65863  ,\n",
       "          74.97012  , -137.03073  ,   65.568085 ,  133.66187  ,\n",
       "          34.358456 , -227.65933  ,  -43.28893  ,    1.3398043,\n",
       "          98.42542  , -101.04429  ,  -89.81973  ,  -81.57272  ,\n",
       "          74.3758   ,  -15.765728 , -206.82236  ,  -95.986336 ,\n",
       "          14.37602  ,  130.16223  ,   30.449846 ,    5.3226433,\n",
       "          -5.843759 ,   34.294674 ,   23.412924 ,   47.444656 ,\n",
       "          39.06975  ,   26.283422 ,  -40.548523 ,   43.333717 ,\n",
       "         -53.868473 , -108.09902  ,    5.3926153,   38.239613 ,\n",
       "         -15.745132 ,   28.899343 ,  -56.287743 , -103.94787  ,\n",
       "        -226.5171   ,   83.46842  ,  -36.740948 ,   27.888771 ,\n",
       "         -95.54252  ,  -80.14416  ,    5.209347 , -152.81812  ,\n",
       "        -110.79656  ,  -56.1902   , -107.680275 ,   -4.8217063,\n",
       "         282.08752  ,   86.118225 ,  123.744965 ,   65.71629  ,\n",
       "          48.013123 ,  143.44035  ,   -1.4938791, -184.6916   ,\n",
       "          90.81892  , -155.06958  ,   -2.0931423,   11.598673 ,\n",
       "         -22.557701 , -172.91805  ,    3.9984648,  119.32204  ,\n",
       "         -16.418644 ,   80.53693  ,  -13.518033 , -102.97508  ,\n",
       "         -68.46148  ,  -54.49891  , -143.71501  ,   64.983574 ,\n",
       "           7.842596 ,   73.23991  ,  -63.87937  ,   -4.6569705,\n",
       "         -36.75754  ,   94.70842  ,   11.974182 ,   88.395584 ,\n",
       "          92.862785 ,  -32.141064 ,   90.76171  ,  -13.786361 ,\n",
       "         111.55715  ,  -25.115503 ,   81.64404  ,  -31.554367 ,\n",
       "          42.028538 ,    8.5887165,  -38.33784  ,   75.48781  ,\n",
       "          83.95754  ,  -51.657146 ,   34.420403 , -202.87009  ,\n",
       "          82.92646  ,   63.894516 , -109.30933  ,   31.69018  ,\n",
       "          18.570717 ,  -92.30046  ,   14.733731 ,  122.53722  ,\n",
       "         -60.532135 ,   -7.0567365,   15.200605 , -208.47559  ,\n",
       "        -181.27036  ,   65.33774  ,  -23.168371 ,  -19.711184 ,\n",
       "          22.128931 ,   91.465324 ,  -50.3111   ,  -24.76745  ,\n",
       "         -14.56008  ,    7.2333074,  -27.807365 ,   59.709545 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_model = keras.models.load_model(MODEL_FILE)\n",
    "img = cv2.imread('/home/cmf/workspace/Anthony_Hopkins_0001.jpg')\n",
    "img = cv2.resize(img,(112,112))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = np.asarray(img)\n",
    "img = np.reshape(img,(1,112,112,3))\n",
    "img = (img-127.5)/128\n",
    "val_model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:96: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /home/cmf/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 325 variables.\n",
      "INFO:tensorflow:Converted 325 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15008916"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(MODEL_FILE)\n",
    "tflite_model = converter.convert()\n",
    "open(LITE_FILE, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS: \n",
      "[{'name': 'input_1', 'index': 160, 'shape': array([  1, 112, 112,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\n",
      "OUTPUTS: \n",
      "[{'name': 'reshape_1/Reshape', 'index': 227, 'shape': array([  1, 128], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\n",
      "output:\n",
      "[[-143.00523    -71.5679    -103.5633     -48.46702      2.5932927\n",
      "    81.819016   -19.29133    189.0453      77.99276   -142.63434\n",
      "    68.24936    139.07986     35.810715  -236.90675    -44.981247\n",
      "     1.4322075  102.37677   -105.10348    -93.47523    -84.86958\n",
      "    77.39422    -16.39632   -215.19426    -99.938286    14.969105\n",
      "   135.36948     31.761492     5.5816884   -6.1123066   35.70189\n",
      "    24.37168     49.37809     40.72366     27.295727   -42.169006\n",
      "    45.130142   -56.03184   -112.45094      5.656587    39.828728\n",
      "   -16.38633     30.055883   -58.549942  -108.20052   -235.7259\n",
      "    86.849174   -38.29927     29.017857   -99.39468    -83.396286\n",
      "     5.374714  -159.05252   -115.30416    -58.519997  -112.0396\n",
      "    -5.042204   293.50354     89.582886   128.70555     68.37992\n",
      "    50.005108   149.2383      -1.5712103 -192.17958     94.48231\n",
      "  -161.32617     -2.2077186   12.073165   -23.50953   -179.89766\n",
      "     4.103298   124.169815   -17.079073    83.8483     -14.068096\n",
      "  -107.1346     -71.21887    -56.68051   -149.53279     67.64148\n",
      "     8.156588    76.18202    -66.43498     -4.829883   -38.26009\n",
      "    98.50786     12.439293    91.985596    96.649956   -33.43307\n",
      "    94.45184    -14.34378    116.05835    -26.1284      84.93896\n",
      "   -32.84381     43.78844      8.982206   -39.8835      78.55227\n",
      "    87.37408    -53.78064     35.83522   -211.1332      86.28941\n",
      "    66.53856   -113.72543     32.993637    19.320072   -96.04619\n",
      "    15.270219   127.531815   -62.969807    -7.360306    15.82165\n",
      "  -216.94237   -188.63184     68.03283    -24.12871    -20.477304\n",
      "    23.069393    95.21394    -52.367573   -25.76344    -15.14509\n",
      "     7.5493784  -28.952171    62.122265 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=LITE_FILE)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print('INPUTS: ')\n",
    "print(input_details)\n",
    "print('OUTPUTS: ')\n",
    "print(output_details)\n",
    "\n",
    "# Test model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=input_details[0]['dtype'])\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "print('output:')\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
